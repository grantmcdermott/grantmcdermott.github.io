[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Unless otherwise noted, all courses were taught at the University of Oregon’s Department of Economics between 2017 and 2022.\n\nWorkshops\n(Pretty) big data wrangling with DuckDB and Polars, May 2024. (Website. GitHub.)\n\n\nGraduate\nData Science for Economists, EC 607. (Syllabus. Lectures.)\nBig Data in Economics, EC 510. (Syllabus. Lectures.)\n\n\nUndergraduate\nEnvironmental Economics, EC 434. (Syllabus.)\nIntermediate Microeconomic Theory, EC 311. (Syllabus.)"
  },
  {
    "objectID": "posts/rstudio-server-compute-engine/index.html",
    "href": "posts/rstudio-server-compute-engine/index.html",
    "title": "RStudio Server on Google Compute Engine",
    "section": "",
    "text": "Update: I’ve converted this blog post to a more detailed, two-part lecture in my Data Science for Economists](https://github.com/uo-ec607/lectures?tab=readme-ov-file#lectures) course. Please click on the link for this updated version."
  },
  {
    "objectID": "posts/even-more-reshape/index.html",
    "href": "posts/even-more-reshape/index.html",
    "title": "Even more reshape benchmarks",
    "section": "",
    "text": "Various people have asked me to add some additional benchmarks to my data reshaping post from earlier this week. I’ve been hesitant to add these as an update, since I didn’t want to distract from the major point I was trying to make in that previous post.1 However, I’m happy to put these additional benchmarks in a new blog post here.\nThe additional benchmarks that we’ll be considering today are:\nThese will go alongside the core routines from my original post…\nI’ll divide the results into two sections."
  },
  {
    "objectID": "posts/even-more-reshape/index.html#smallish-data",
    "href": "posts/even-more-reshape/index.html#smallish-data",
    "title": "Even more reshape benchmarks",
    "section": "Small(ish) data",
    "text": "Small(ish) data\nOur first task will be to reshape the same (sparse) 1,000 by 1,002 dataset from wide to long. Here are the results and I’ll remind you that the x-axis has been log-transformed to handle scaling.\n\nOnce more, we see that data.table rules the roost. However, the newly-added DataFrames (Julia) and pandas (Python) implementations certainly put in a good shout, coming in second and third, respectively. Interestingly enough, my two tidyr benchmarks seemed to have shuffled slightly this time around, but that’s only to be expected for very quick operations like this. (We’ll test again in a moment on a larger dataset.) Adding options to gtools yields a fairly modest if noticeable difference, while the base R reshape() command doesn’t totally discrace itself. Certainly much faster than the Stata equivalent."
  },
  {
    "objectID": "posts/even-more-reshape/index.html#largeish-data",
    "href": "posts/even-more-reshape/index.html#largeish-data",
    "title": "Even more reshape benchmarks",
    "section": "Large(ish) data",
    "text": "Large(ish) data\nAnother thing to ponder is whether the results are sensitive to the relatively small size of the test data. The long-form dataset is “only” 1 million rows deep and the fastest methods complete in only a few milliseconds. So, for this next set of benchmarks, I’ve scaled up to the data by two orders of magnitude: Now we want to reshape a 100,000 by 1,002 dataset from wide to long. In other words, the resulting long-form dataset is 100 million rows deep.2\nWithout further ado, here are the results. Note that I’m dropping the slowest methods (because I’m not a masochist) and this also means that I won’t need to log-transform the x-axis anymore.\n\nReassuringly, everything stays pretty much the same from a rankings perspective. The ratios between the different methods are very close to the small data benchmarks. The most notable thing is that gtools manages to claw back time (suggesting some initial overhead penalty), although it still lags the other methods. For reference, the default data.table melt() method completes in just over a second on my laptop, which is just crazy fast. All of the methods here are impressively quick, to be honest.\nSummarizing, here is each language represented by its fastest method."
  },
  {
    "objectID": "posts/even-more-reshape/index.html#code",
    "href": "posts/even-more-reshape/index.html#code",
    "title": "Even more reshape benchmarks",
    "section": "Code",
    "text": "Code\nSee my previous post for the data generation and plotting code. (Remember to set n = 1e8 for the large data benchmark.) For the sake of brevity, here is quick recap of the main reshaping functions that I use across the different languages and how I record timing.\n\nR\n# Libraries ---------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(microbenchmark)\n\n# Data --------------------------------------------------------------------\n\nd = fread('sparse-wide.csv')\n\n# Base --------------------------------------------------------------------\n\nbase_reshape = function() reshape(d, direction='long', varying=3:1002, sep=\"\")\n\n# tidyverse ---------------------------------------------------------------\n\n## Default\ntidy_pivot = function() pivot_longer(d, -c(id, grp))\n## Default with na.rm argument\ntidy_pivot_narm = function() pivot_longer(d, -c(id, grp), values_drop_na = TRUE)\n\n# data.table --------------------------------------------------------------\n\nDT = as.data.table(d)\n## Default\ndt_melt = function() melt(DT, id.vars = c('id', 'grp'))\n## Default with na.rm argument\ndt_melt_narm = function() melt(DT, id.vars = c('id', 'grp'), na.rm = TRUE)\n\n# Benchmark ---------------------------------------------------------------\n\nb = microbenchmark(base_reshape(),\n                   tidy_pivot(), tidy_pivot_narm(),\n                   dt_melt(), dt_melt_narm(),  \n                   times = 5)\n\n\nStata\nclear\nclear matrix\ntimer clear\nset more off\n\ncd \"Z:\\home\\grant\\Documents\\Projects\\reshape-benchmarks\"\n\nimport delimited \"sparse-wide.csv\"\n\n// Vanilla Stata\npreserve\ntimer on 1\nreshape long x, i(id grp) j(variable) \ntimer off 1\nrestore\n\n// sreshape\npreserve\ntimer on 2\nsreshape long x, i(id grp) j(variable) missing(drop all)\ntimer off 2\nrestore\n\n// gtools\npreserve\ntimer on 3\ngreshape long x, by(id grp) key(variable)\ntimer off 3\nrestore\n\n// gtools (dropmiss)\npreserve\ntimer on 4\ngreshape long x, by(id grp) key(variable) dropmiss\ntimer off 4\nrestore\n\n// gtools (nochecks)\npreserve\ntimer on 5\ngreshape long x, by(id grp) key(variable) dropmiss nochecks\ntimer off 5\nrestore\n\ntimer list\n\ndrop _all\ngen result = .\nset obs 5\ntimer list\nforval j = 1/5{\n    replace result = r(t`j') if _n == `j'\n}\noutsheet using \"reshape-results-stata.csv\", replace\n\n\nPython\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('sparse-wide.csv')\nresult = %timeit -o df.melt(id_vars=['id', 'grp'])\nresult_df = pd.DataFrame({'result':[np.median(result.timings)]})\nresult_df.to_csv('reshape-results-python.csv')\n\n\nJulia\nusing CSV, DataFrames, BenchmarkTools\n\nd = DataFrame(CSV.File(\"sparse-wide.csv\"))\njl_stack = @benchmark stack(d, Not([:id, :grp])) evals=5\nCSV.write(\"reshape-results-julia.csv\", DataFrame(result = median(jl_stack)))"
  },
  {
    "objectID": "posts/even-more-reshape/index.html#footnotes",
    "href": "posts/even-more-reshape/index.html#footnotes",
    "title": "Even more reshape benchmarks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNamely: A manual split-apply-combine reshaping approach doesn’t yield the same kind of benefits in R as it does in Stata. You’re much better off sticking to the already-optimised defaults.↩︎\nLet the record show that I tried running one additional order of magnitude (i.e. a billion rows), but data.table was the only method that reliably completed its benchmark runs without completely swamping my memory (32 GB) and crashing everything. As I said last time, it truly is a marvel for big data work.↩︎"
  },
  {
    "objectID": "posts/reshape-benchmarks/index.html",
    "href": "posts/reshape-benchmarks/index.html",
    "title": "Reshape benchmarks",
    "section": "",
    "text": "Over on Twitter, I was reply-tagged in a tweet thread by Ryan Hill. Ryan shows how he overcomes a problem that arises when reshaping a sparse (i.e. unbalanced) dataset in Stata. Namely, how can you cut down on the computation time that Stata wastes with all the missing values, especially when reshaping really wide data? Ryan’s clever solution is very much in the split-apply-combine mould. Manually split the data into like groups (i.e. sharing the same columns), drop any missing observations, and then reshape on those before recombining everything at the end. It turns out that this is a lot faster than Stata’s default reshape command… and there is even a package (sreshape) that implements this for you.\nSo far so good. But I was asked what the R equivalent of this approach would be. It’s pretty easy to implement — more on that in a moment — but I expressed scepticism that it would yield the same kind of benefits as the Stata case. There are various reasons for my scepticism, including the fact that R’s reshaping libraries are already highly optimised for this kind of thing and R generally does a better job of handling missing values.1\nSounds like we need a good reshape horserace up in here!\nInsert obligatory joke about time spent reading reshape help files."
  },
  {
    "objectID": "posts/reshape-benchmarks/index.html#motivation",
    "href": "posts/reshape-benchmarks/index.html#motivation",
    "title": "Reshape benchmarks",
    "section": "",
    "text": "Over on Twitter, I was reply-tagged in a tweet thread by Ryan Hill. Ryan shows how he overcomes a problem that arises when reshaping a sparse (i.e. unbalanced) dataset in Stata. Namely, how can you cut down on the computation time that Stata wastes with all the missing values, especially when reshaping really wide data? Ryan’s clever solution is very much in the split-apply-combine mould. Manually split the data into like groups (i.e. sharing the same columns), drop any missing observations, and then reshape on those before recombining everything at the end. It turns out that this is a lot faster than Stata’s default reshape command… and there is even a package (sreshape) that implements this for you.\nSo far so good. But I was asked what the R equivalent of this approach would be. It’s pretty easy to implement — more on that in a moment — but I expressed scepticism that it would yield the same kind of benefits as the Stata case. There are various reasons for my scepticism, including the fact that R’s reshaping libraries are already highly optimised for this kind of thing and R generally does a better job of handling missing values.1\nSounds like we need a good reshape horserace up in here!\nInsert obligatory joke about time spent reading reshape help files."
  },
  {
    "objectID": "posts/reshape-benchmarks/index.html#benchmarks",
    "href": "posts/reshape-benchmarks/index.html#benchmarks",
    "title": "Reshape benchmarks",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nDefaults\nSimilar to Ryan, our task will be to reshape wide data (1000 non-index columns) with a lot of missing observations. I’ll leave my scripts at the bottom of this post, but first a comparison of the “default” reshaping methods. For Stata, that includes the vanilla reshape command and the aforementioned sreshape command, as well as greshape from gtools. For R, we’ll use pivot_longer() from the tidyverse (i.e. tidyr) and melt() from data.table. Note the log scale and the fact that I’ve rebased everything relative to the fastest option.\n\nUnsuprisingly, data.table::melt() is easily the fastest method. However, tidyr::pivot_longer() gives a very decent account of itself and is about three times as fast as gtools’ greshape. The base Stata reshape option is hopelessly slow for this task, demonstrating (among other things) the difficulty it has with missing values.\n\n\nManual implementation\nDefaults out of the way, let’s implement the manual split-apply-combine approach in R. Again, I’ll leave my scripts at the bottom of the post for you to look at, but I’m essentially just following (variants of) the approach that Ryan adroitly lays out. Note that both tidyr::pivot_longer() and data.table::melt() provide options to drop missing values, so I’m going to try those out too.\n\nAs expected, the manual split-apply-combine approach(es) don’t yield any benefits in the R case. In fact, quite the opposite, with it resulting in a rather sizeable performance loss. (Yes, I know that I could try running things in parallel but I can already tell you that the extra overhead won’t be worth it for this particular example.)"
  },
  {
    "objectID": "posts/reshape-benchmarks/index.html#bottom-line",
    "href": "posts/reshape-benchmarks/index.html#bottom-line",
    "title": "Reshape benchmarks",
    "section": "Bottom line",
    "text": "Bottom line\nFor reshaping sparse data, you can’t really do much better than sticking with the defaults in R. data.table remains a speed marvel, although tidyr gives very good account of itself too. Stata users should definitely switch to gtools if they aren’t using it already.\nUpdate: Follow-up post here with additional benchmarks, including other SW languages and a larger dataset."
  },
  {
    "objectID": "posts/reshape-benchmarks/index.html#code",
    "href": "posts/reshape-benchmarks/index.html#code",
    "title": "Reshape benchmarks",
    "section": "Code",
    "text": "Code\nAs promised, here is the code. Please let me know if you spot any errors.\nFirst, generate the dataset (in R).\n\n# Libraries ---------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(data.table)\n\n# Data prep ---------------------------------------------------------------\n\nset.seed(10)\n\nn = 1e6\nn_col=1e3\n\nd = matrix(sample(LETTERS, n, replace=TRUE), ncol=n_col)\n\n## Randomly replace columns with NA values\nfor(i in 1:nrow(d)) {\n    j = sample(2:n_col, 1)\n    d[i, j:n_col] = NA_character_\n  }\nrm(i, j)\n## Ensure at least one row has obs for all columns\nd[1, ] = sample(LETTERS, n_col, replace = TRUE)\n\n## Get non-missing obs group (only really needed for the manual split-apply-combine approaches)\ngrp = apply(d, 1, function(x) sum(!is.na(x)))\n\n## Convert to data frame and name columns\nd = as.data.frame(d)\ncolnames(d) = paste0(\"x\", seq_along(d))\nd$grp = grp\nd$id = row.names(d)\nd = d %&gt;% select(id, grp, everything())\n\n# Export -----------------------------------------------------------------\nfwrite(d, '~/sparse-wide.csv')\n\nNext, run the Stata benchmarks.\n\nclear\nclear matrix\ntimer clear\nset more off\n\nimport delimited \"~/sparse-wide.csv\"\n\n// Vanilla Stata\npreserve\ntimer on 1\nreshape long x, i(id grp) j(variable) \ntimer off 1\nrestore\n\n// sreshape\n// net install dm0090\npreserve\ntimer on 2\nsreshape long x, i(id grp) j(variable) missing(drop all)\ntimer off 2\nrestore\n\n// gtools\n// ssc install gtools\npreserve\ntimer on 3\ngreshape long x, by(id grp) key(variable)\ntimer off 3\nrestore\n\ntimer list\n\ndrop _all\ngen result = .\nset obs 3\ntimer list\nforval j = 1/3{\n    replace result = r(t`j') if _n == `j'\n}\noutsheet using \"~/sparse-reshape-stata.csv\", replace\n\nFinally, let’s run the R benchmarks and compare.\n\n# Libraries ---------------------------------------------------------------\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(microbenchmark)\nlibrary(hrbrthemes)\ntheme_set(theme_ipsum())\n\n# tidyverse ---------------------------------------------------------------\n\n## Default\ntidy_pivot = function() pivot_longer(d, -c(id, grp))\n## Default with na.rm argument\ntidy_pivot_narm = function() pivot_longer(d, -c(id, grp), values_drop_na = TRUE)\n## Manual split-apply-combine approach\ntidy_split = function() map_dfr(unique(d$grp), function(i) pivot_longer(filter(d, grp==i)[1:(i+2)], -c(id, grp)))\n## Version of manual split-apply-combine approach that uses nesting\ntidy_nest = function() {\n  d %&gt;%\n    group_nest(grp) %&gt;%\n    mutate(data = map2(data, grp, ~ select(.x, 1:(.y+1)))) %&gt;%\n    mutate(data = map(data, ~ pivot_longer(.x, -id))) %&gt;%\n    unnest(cols = data)\n}\n\n# data.table --------------------------------------------------------------\n\nDT = as.data.table(d)\n## Default\ndt_melt = function() melt(DT, id.vars = c('id', 'grp'))\n## Default with na.rm argument\ndt_melt_narm = function() melt(DT, id.vars = c('id', 'grp'), na.rm = TRUE)\n## Manual split-apply-combine approach\ndt_split = function() rbindlist(lapply(unique(DT$grp), function(i) melt(DT[grp==i, 1:(i+2)], id.vars=c('id','grp'))))\n\n# Benchmark ---------------------------------------------------------------\n\nb = microbenchmark(tidy_pivot(), tidy_pivot_narm(), tidy_split(), tidy_nest(), \n                   dt_melt(), dt_melt_narm(), dt_split(), \n                   times = 5)\nb\nautoplot(b)\n\n# Comparison with Stata results -------------------------------------------\n\nstata = fread('~/sparse-reshape-stata.csv')\nstata$method = c('reshape', 'sreshape', 'gtools')\nstata$sw = 'Stata'\n\nr = data.table(result = print(b, 's')$median, ## just take the median time\n               method = gsub('\\\\(\\\\)', '', print(b)$expr),\n               sw = 'R'\n               )\n\nres = rbind(r, stata)\nres[, rel_speed := result/min(result)]\n\ncapn = paste0('Task: Wide to long reshaping of an unbalanced (sparse) ', nrow(d),\n              ' × ', ncol(d), ' data frame with two ID variables.')\n\n## Defaults only\nggplot(res[method %chin% c('dt_melt', 'tidy_pivot', 'gtools', 'sreshape', 'reshape')], \n       aes(x = rel_speed, y = fct_reorder(method, rel_speed), col = sw, fill = sw)) +\n  geom_col() +\n  scale_x_log10() +\n  scale_color_brewer(palette = 'Set1') + scale_fill_brewer(palette = 'Set1') +\n  labs(x = 'Time (relative to fastest method)', y = 'Method', title = 'Reshape benchmark', \n       subtitle = 'Default methods only',\n       caption = capn)\n\n## All\nggplot(res, aes(x = rel_speed, y = fct_reorder(method, rel_speed), col = sw, fill = sw)) +\n  geom_col() +\n  scale_x_log10() +\n  labs(x = 'Time (relative to fastest method)', y = 'Method', title = 'Reshape benchmark', \n       caption = capn) +\n  scale_color_brewer(palette = 'Set1') + scale_fill_brewer(palette = 'Set1')"
  },
  {
    "objectID": "posts/reshape-benchmarks/index.html#footnotes",
    "href": "posts/reshape-benchmarks/index.html#footnotes",
    "title": "Reshape benchmarks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs good as Stata is at handling rectangular data, it’s somewhat notorious for how it handles missing observations. But that’s a subject for another day.↩︎"
  },
  {
    "objectID": "posts/better-way-adjust-ses/index.html",
    "href": "posts/better-way-adjust-ses/index.html",
    "title": "A better way to adjust your standard errors",
    "section": "",
    "text": "Consider the following scenario:\nA researcher has to adjust the standard errors (SEs) for a regression model that she has already run. Maybe this is to appease a journal referee. Or, maybe it’s because she is busy iterating through the early stages of a project. She’s still getting to grips with her data and wants to understand how sensitive her results are to different modeling assumptions.\nDoes that sound familiar? I believe it should, because something like that has happened to me on every single one of my empirical projects. I end up estimating multiple versions of the same underlying regression model — even putting them side-by-side in a regression table, where the only difference across columns is slight tweaks to the way that the SEs were calculated.\nConfronted by this task, I’m willing to bet that most people do the following:\n\nRun their model under one SE specification (e.g. iid).\nRe-run their model a second time under another SE specification (e.g. HC robust).\nRe-run their model a third time under yet another SE specification (e.g. clustered).\nEtc.\n\nWhile this is fine as far as it goes, I’m here to tell you that there’s a better way. Rather than re-running your model multiple times, I’m going to advocate that you run your model only once and then adjust SEs on the backend as needed. This approach — what I’ll call “on-the-fly” SE adjustment — is not only safer, it’s much faster too.\nLet’s see some examples."
  },
  {
    "objectID": "posts/better-way-adjust-ses/index.html#motivation",
    "href": "posts/better-way-adjust-ses/index.html#motivation",
    "title": "A better way to adjust your standard errors",
    "section": "",
    "text": "Consider the following scenario:\nA researcher has to adjust the standard errors (SEs) for a regression model that she has already run. Maybe this is to appease a journal referee. Or, maybe it’s because she is busy iterating through the early stages of a project. She’s still getting to grips with her data and wants to understand how sensitive her results are to different modeling assumptions.\nDoes that sound familiar? I believe it should, because something like that has happened to me on every single one of my empirical projects. I end up estimating multiple versions of the same underlying regression model — even putting them side-by-side in a regression table, where the only difference across columns is slight tweaks to the way that the SEs were calculated.\nConfronted by this task, I’m willing to bet that most people do the following:\n\nRun their model under one SE specification (e.g. iid).\nRe-run their model a second time under another SE specification (e.g. HC robust).\nRe-run their model a third time under yet another SE specification (e.g. clustered).\nEtc.\n\nWhile this is fine as far as it goes, I’m here to tell you that there’s a better way. Rather than re-running your model multiple times, I’m going to advocate that you run your model only once and then adjust SEs on the backend as needed. This approach — what I’ll call “on-the-fly” SE adjustment — is not only safer, it’s much faster too.\nLet’s see some examples."
  },
  {
    "objectID": "posts/better-way-adjust-ses/index.html#example-1-sandwich",
    "href": "posts/better-way-adjust-ses/index.html#example-1-sandwich",
    "title": "A better way to adjust your standard errors",
    "section": "Example 1: sandwich",
    "text": "Example 1: sandwich\nTo the best of my knowledge, on-the-fly SE adjustment was introduced to R by the sandwich package (@Achim Zeilles et al.) This package has been around for well over a decade and is incredibly versatile, providing an object-orientated framework for recomputing variance-covariance (VCOV) matrix estimators — and thus SEs — for a wide array of model objects and classes. At the same time, sandwich just recently got its own website to coincide with some cool new features. So it’s worth exploring what that means for a modern empirical workflow. In the code that follows, I’m going to borrow liberally from the introductory vignette. But I’ll also tack on some additional tips and tricks that I use in my own workflow. (UPDATE (2020-08-23): The vignette has now been updated to include some of the suggestions from this post. Thanks Achim!)\nLet’s start by running a simple linear regression on some sample data; namely, the “PetersenCL” dataset that comes bundled with the package.\n\nlibrary(sandwich)\ndata('PetersenCL')\n\nm = lm(y ~ x, data = PetersenCL)\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x, data = PetersenCL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7611 -1.3680 -0.0166  1.3387  8.6779 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.02968    0.02836   1.047    0.295    \nx            1.03483    0.02858  36.204   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.005 on 4998 degrees of freedom\nMultiple R-squared:  0.2078,    Adjusted R-squared:  0.2076 \nF-statistic:  1311 on 1 and 4998 DF,  p-value: &lt; 2.2e-16\n\n\nOur simple model above assumes that the errors are iid. But we can adjust these SEs by calling one of the many alternate VCOV estimators provided by sandwich. For example, to get a robust, or heteroscedasticity-consistent (“HC3”), VCOV matrix we’d use:\n\nvcovHC(m)\n\n              (Intercept)             x\n(Intercept)  8.046458e-04 -1.155095e-05\nx           -1.155095e-05  8.072475e-04\n\n\nTo actually substitute the robust VCOV into our original model — so that we can print it in a nice regression table and perform statistical inference — we pair sandwich with its companion package, lmtest. The workhorse function here is lmtest::coeftest and, as we can see, this yields an object that is similar to a standard model summary in R.\n\nlibrary(lmtest)\n\ncoeftest(m, vcov = vcovHC)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.029680   0.028366  1.0463   0.2955    \nx           1.034833   0.028412 36.4223   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTo recap: We ran our base m model just the once and then adjusted for robust SEs on the backend using sandwich/coeftest.\nNow, I’ll admit that the benefits of this workflow aren’t super clear from my simple example yet. Though, we did cut down on copying-and-pasting of duplicate code and this automatically helps to minimize user error. (Remember: DRY!) But we can easily scale things up to get a better sense of its power. For instance, we could imagine calling a whole host of alternate VCOVs to our base model.\n\n## Calculate the VCOV (SEs) under a range of different assumptions\nvc = list(\n  \"Standard\"              = vcov(m),\n  \"Sandwich (basic)\"      = sandwich(m),\n  \"Clustered\"             = vcovCL(m, cluster = ~ firm),\n  \"Clustered (two-way)\"   = vcovCL(m, cluster = ~ firm + year),\n  \"HC3\"                   = vcovHC(m),\n  \"Andrews' kernel HAC\"   = kernHAC(m),\n  \"Newey-West\"            = NeweyWest(m),\n  \"Bootstrap\"             = vcovBS(m),\n  \"Bootstrap (clustered)\" = vcovBS(m, cluster = ~ firm)\n  )\n\nYou could, of course, print the vc list to screen now if you so wanted. But I want to go one small step further by showing you how easy it is to create a regression table that encapsulates all of these different models. In the next code chunk, I’m going to create a list of models by passing vc to an lapply() call.1 I’m then going to generate a regression table using msummary() from the excellent modelsummary package (@Vincent Arel-Bundock).\n\nlibrary(modelsummary) ## For great-looking regression tables (among other things)\n\n## Adjust our model SEs on-the-fly\nlm_mods = lapply(vc, function(x) coeftest(m, vcov = x))\n\n## Print the regression table\nmsummary(lm_mods)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Standard\n                Sandwich (basic)\n                Clustered\n                Clustered (two-way)\n                HC3\n                Andrews' kernel HAC\n                Newey-West\n                Bootstrap\n                Bootstrap (clustered)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.030  \n                  0.030  \n                  0.030  \n                  0.030  \n                  0.030  \n                  0.030  \n                  0.030  \n                  0.030  \n                  0.030  \n                \n                \n                             \n                  (0.028)\n                  (0.028)\n                  (0.067)\n                  (0.065)\n                  (0.028)\n                  (0.044)\n                  (0.066)\n                  (0.031)\n                  (0.075)\n                \n                \n                  x          \n                  1.035  \n                  1.035  \n                  1.035  \n                  1.035  \n                  1.035  \n                  1.035  \n                  1.035  \n                  1.035  \n                  1.035  \n                \n                \n                             \n                  (0.029)\n                  (0.028)\n                  (0.051)\n                  (0.054)\n                  (0.028)\n                  (0.035)\n                  (0.048)\n                  (0.029)\n                  (0.056)\n                \n                \n                  Num.Obs.   \n                  5000   \n                  5000   \n                  5000   \n                  5000   \n                  5000   \n                  5000   \n                  5000   \n                  5000   \n                  5000   \n                \n                \n                  AIC        \n                  31141.2\n                  31141.2\n                  31141.2\n                  31141.2\n                  31141.2\n                  31141.2\n                  31141.2\n                  31141.2\n                  31141.2\n                \n                \n                  BIC        \n                  63714.1\n                  63714.1\n                  63714.1\n                  63714.1\n                  63714.1\n                  63714.1\n                  63714.1\n                  63714.1\n                  63714.1\n                \n        \n      \n    \n\n\n\nIf you’re the type of person — like me — that prefers visual representation, then printing a coefficient plot is equally easy with modelsummary::modelplot(). This creates a ggplot2 object that can be further manipulated as needed. In the code chunk below, I’ll demonstrate this fairly simply by flipping the plot orientation.\n\nlibrary(ggplot2)\n\nmodelplot(lm_mods, coef_omit = 'Interc') +\n  coord_flip()\n\n\n\n\n\n\n\n\nAnd there you have it: An intuitive and helpful comparison across a host of specifications, even though we only “ran” the underlying model once. Simple!\n\n\n\n\n\n\nNote\n\n\n\nUPDATE (June 2021): You can now automate all of the steps that I show above in newer versions of modelsummary, thanks to the incredibly flexible vcov argument. See the documentation here.\n\nmsummary(m, vcov = vc)\nmodelplot(m, vcov = vc, coef_omit = 'Interc') +\n    coord_flip()\n# etc."
  },
  {
    "objectID": "posts/better-way-adjust-ses/index.html#example-2-fixest",
    "href": "posts/better-way-adjust-ses/index.html#example-2-fixest",
    "title": "A better way to adjust your standard errors",
    "section": "Example 2: fixest",
    "text": "Example 2: fixest\nWhile sandwich covers a wide range of model classes in R, it’s important to know that a number of libraries provide their own specialised methods for on-the-fly SE adjustment. The one that I want to show you for this second example is the fixest package (@Laurent Bergé).\nIf you follow me on Twitter or have read my lecture notes, you already know that I am a huge fan of this package. It’s very elegantly designed and provides an insanely fast way to estimate high-dimensional fixed effects models. More importantly for today’s post, fixest offers automatic support for on-the-fly SE adjustment. We only need to run our model once and can then adjust the SEs on the backend via a call to summary(..., vcov = 'vcov_type').2\nTo demonstrate, I’m going to run some regressions on a subsample of the well-known RITA air traffic data. I’ve already downloaded the dataset from Revolution Analytics and prepped it for the narrow purposes of this blog post. (See the data appendix below for code.) All told we’re looking at 9 variables extending over approximately 1.8 million rows. So, not “big” data by any stretch of the imagination, but my regressions should take at least a few seconds to run on most computers.\n\nlibrary(data.table)\n\nair = fread('~/air.csv')\nair\n\n          year month day_of_week tail_num origin_airport_id dest_airport_id\n         &lt;int&gt; &lt;int&gt;       &lt;int&gt;   &lt;char&gt;             &lt;int&gt;           &lt;int&gt;\n      1:  2012     1           3   N320AA             12478           12892\n      2:  2012     1           5   N327AA             12478           12892\n      3:  2012     1           4   N329AA             12892           12478\n      4:  2012     1           5   N336AA             12478           12892\n      5:  2012     1           7   N323AA             12478           12892\n     ---                                                                   \n1844063:  2011     1           1   N904FJ             14107           15376\n1844064:  2011     1           1   N7305V             14262           14107\n1844065:  2011     1           1   N922FJ             14683           11057\n1844066:  2011     1           1   N935LR             14698           14107\n1844067:  2011     1           1   N932LR             15376           14107\n         arr_delay dep_delay dep_tod\n             &lt;int&gt;     &lt;int&gt;  &lt;char&gt;\n      1:       -34         4     am2\n      2:         2        -2     am2\n      3:        -6        -5     am2\n      4:       -32        -9     am2\n      5:        -7        -6     am2\n     ---                            \n1844063:        -7        -1     pm2\n1844064:        -3         0     pm1\n1844065:       -10        -4     pm2\n1844066:        11         0     pm1\n1844067:        30        -5     am2\n\n\nThe actual regression that I’m going to run on these data is somewhat uninspired: Namely, how does arrival delay depend on departure delay, conditional on the time of day?3 I’ll throw in a bunch of fixed effects to make the computation a bit more interesting/intensive, but it’s fairly standard stuff. Note that I am running a linear fixed effect model by calling fixest::feols().\nBut, really, I don’t want you to get sidetrack by the regression details. The main thing I want to focus your attention on is the fact that I’m only going run the base model once, i.e. for mod1. Then, I’m going to adjust the SE for two more models, mod2 and mod3, on the fly via respective summary() calls.\n\nlibrary(fixest)\n\n## Start timer; we'll use for benchmarking later\npt = proc.time()\n\n## Run the model once and once only!\n## By default, fixest::feols will cluster the SEs by the first FE (here: month)\nmod1 = feols(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id, \n             data = air)\n\n## Adjust SEs on the fly: two-way cluster\nmod2 = summary(mod1, vcov = ~month + origin_airport_id)\n\n## Adjust SEs on the fly: three-way cluster. Note that we can even include\n## cluster vars (e.g. tail_num) that weren't present in the original regression\nmod3 = summary(mod1, vcov = ~month + origin_airport_id + tail_num)\n\n## Stop timer and save results\ntime_feols = (proc.time() - pt)[3]\n\nBefore I get to benchmarking, how about a quick coefficient plot? I’ll use modelsummary::modelplot() again, focusing only on the key “time of day × departure delay” interaction terms.\n\nfeols_mods = list(mod1, mod2, mod3)\n\nmodelplot(\n  feols_mods, \n  coef_map = c('dep_todam1:dep_delay' = 'Midnight × Dep. delay',\n               'dep_todam2:dep_delay' = 'Morning × Dep. delay',\n               'dep_todpm1:dep_delay' = 'Afternoon × Dep. delay',\n               'dep_todpm2:dep_delay' = 'Evening × Dep. delay')\n  ) +\n  labs(caption = 'Dependent variable: Arrival delay')\n\n\n\n\n\n\n\n\n\nBenchmarking\nGreat, it worked. But did it save time? To answer this question I’ve benchmarked against three other methods:\n\nfeols(), again from the fixest package, but this time with each of the three models run separately.\nfelm() from the lfe package.\nreghdfe from the reghdfe package (Stata).\n\nYou can find the benchmarking code for these other methods in the appendix. (Please let me know if you spot any errors.) In the interests of brevity, here are the results.\n\n\n\n\n\n\n\n\n\nThere are several takeaways from this exercise. For example, fixest::feols() is the fastest method even if you are (inefficiently) re-running the models separately. But — yet again and once more, dear friends — the key thing that I want to emphasise is the additional time savings brought on by adjusting the SEs on the fly. Indeed, we can see that the on-the-fly feols approach only takes a third of the time (approximately) that it does to run the models separately. This means that fixest is recomputing the SEs for models 2 and 3 pretty much instantaneously.\nTo add one last thing about benchmarking, the absolute difference in model run times was not that huge for this particular exercise. There’s maybe two minutes separating the fastest and slowest methods. (Then again, not trivial either…) But if you are like me and find yourself estimating models where each run takes many minutes or hours, or even days and weeks, then the time savings are literally exponential."
  },
  {
    "objectID": "posts/better-way-adjust-ses/index.html#conclusion",
    "href": "posts/better-way-adjust-ses/index.html#conclusion",
    "title": "A better way to adjust your standard errors",
    "section": "Conclusion",
    "text": "Conclusion\nThere comes a point in almost every empirical project where you have to estimate multiple versions of the same model. Which is to say, the only difference between these multiple versions is how the standard errors were calculated: robust, clustered, etc. Maybe you’re trying to satisfy a referee request before publication. Or, maybe you’re trying to understand how sensitive your results are to different modeling assumptions.\nThe goal of this blog post has been to show you that there is often a better approach than manually re-running multiple iterations of your model. Instead, I advocate that you run the model once and then adjust your standard errors on the backend, as needed. This “on-the-fly” approach will save you a ton of time if you are working with big datasets. Even if you aren’t working with big data, you will minimize copying and pasting of duplicate code. All of which will help to make your code more readable and cut down on potential errors.\nWhat’s not to like?\nP.S. There are a couple of other R libraries with support for on-the-fly SE adjustment, e.g. clubSandwich. Since I’ve used it as a counterfoil in the benchmark, I should add that lfe::felm() provides its own method for swapping out different SEs post-estimation; see here. Similarly, I’ve focused on R because it’s the software language that I use most often and — as far as I am aware — is the only one to provide methods for on-the-fly SE adjustment across a wide range of models. If anyone knows of equivalent methods or canned routines in other languages, please let me know in the comments.\nP.P.S. Look, I’m not saying it’s necessarily “wrong” to specify your SEs in the model call. Particularly if you’ve already settled on a single VCOV to use. Then, by all means, use the convenience of Stata’s , robust syntax or the R equivalent lm_robust() (via the estimatr package)."
  },
  {
    "objectID": "posts/better-way-adjust-ses/index.html#appendices",
    "href": "posts/better-way-adjust-ses/index.html#appendices",
    "title": "A better way to adjust your standard errors",
    "section": "Appendices",
    "text": "Appendices\n\nFlight data download and prep\n\nif (!file.exists(path.expand('~/air.csv'))) {\n  \n  ## Download and extract the data\n  URL = 'https://packages.revolutionanalytics.com/datasets/AirlineSubsetCsv.tar.gz'\n  dest_file = path.expand('~/AirlineSubsetCsv.tar.gz')\n  download.file(URL, dest_file, mode = \"wb\")\n  untar(dest_file, exdir = path.expand('~'))\n  \n  ## Bind together and do some data cleaning\n  library(data.table)\n  csvs = list.files(path.expand('~/AirlineSubsetCsv/'), full.names = TRUE)\n  air = rbindlist(lapply(csvs, fread))\n  names(air) = tolower(names(air))\n  int_vars = c('arr_delay', 'dep_delay', 'dep_time', 'arr_time')\n  air[, (int_vars) := lapply(.SD, as.integer), .SDcols = int_vars]\n  \n  ## Create a departure 'time of day' factor variable, dividing the day in four\n  ## quarters\n  air[, dep_tod := fcase(dep_time &lt;= 600, 'am1',\n                         dep_time &lt;= 1200, 'am2',\n                         dep_time &lt;= 1800, 'pm1',\n                         dep_time &gt; 1800, 'pm2')]\n  \n  ## Subset\n  air = air[!is.na(arr_delay), \n            .(year, month, day_of_week, tail_num, origin_airport_id, \n              dest_airport_id, arr_delay, dep_delay, dep_tod)]\n  \n  ## Write to disk\n  fwrite(air, path.expand('~/air.csv'))\n  \n  ## Clean up\n  file.remove(c(dest_file, csvs))\n  file.remove(path.expand('~/AirlineSubsetCsv/')) ## empty dir too\n}\n\n\n\nBenchmarking code for other methods\n\nfixest (separate models)\n\n# library(fixest) ## Already loaded\n\npt = proc.time()\nmod1a = feols(arr_delay ~ dep_tod / dep_delay | \n                month + year + day_of_week + origin_airport_id + dest_airport_id,    \n              data = air)\nmod2a = summary(\n  feols(arr_delay ~ dep_tod / dep_delay | \n          month + year + day_of_week + origin_airport_id + dest_airport_id,          \n        data = air), \n  cluster = c('month', 'origin_airport_id')\n  )\nmod3a = summary(\n  feols(arr_delay ~ dep_tod / dep_delay | \n          month + year + day_of_week + origin_airport_id + dest_airport_id,          \n        data = air), \n  cluster = c('month', 'origin_airport_id', 'tail_num')\n  )\ntime_feols_sep = (proc.time() - pt)[3]\n\n\n\nlfe\n\nlibrary(lfe)\n\npt = proc.time()\nest1 = felm(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id |\n             0 |\n             month, \n            data = air)\nest2 = felm(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id |\n             0 |\n             month + origin_airport_id, \n            data = air)\nest3 = felm(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id |\n             0 |\n             month + origin_airport_id + tail_num, \n            data = air)\ntime_felm = (proc.time() - pt)[3]\n\n\n\nreghdfe\n\nclear\nclear matrix\ntimer clear\nset more off\n\ncd \"Z:\\home\\grant\"\n\nimport delimited air.csv\n\n// Encode strings as numeric factors (Stata struggles with the former)\nencode tail_num, generate(tail_num2)\nencode dep_tod, generate(dep_tod2)\n\n// Start timer and run regs\ntimer on 1\n\nqui reghdfe arr_delay i.dep_tod2 i.dep_tod2#c.dep_delay, ///\n  absorb(month year day_of_week origin_airport_id dest_airport_id) ///\n  cluster(month)\n\nqui reghdfe arr_delay i.dep_tod2 i.dep_tod2#c.dep_delay, ///\n  absorb(month year day_of_week origin_airport_id dest_airport_id) ///\n  cluster(month origin_airport_id)\n\nqui reghdfe arr_delay i.dep_tod2 i.dep_tod2#c.dep_delay, ///\n  absorb(month year day_of_week origin_airport_id dest_airport_id) ///\n  cluster(month origin_airport_id tail_num2)\n\ntimer off 1\n\n// Export time\ndrop _all\ngen elapsed = .\nset obs 1\nreplace elapsed = r(t1) if _n == 1\noutsheet using \"air-reghdfe.csv\", replace"
  },
  {
    "objectID": "posts/better-way-adjust-ses/index.html#footnotes",
    "href": "posts/better-way-adjust-ses/index.html#footnotes",
    "title": "A better way to adjust your standard errors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can substitute with a regular for loop or purrr::map() if you prefer.↩︎\nSee the package documentation for details about valid vcov arguments (e.g., “iid”, “hc1”, “twoway”, “threeway”, etc.)↩︎\nNote that I’m going to use a dep_tod / dep_delay expansion on the RHS to get the full marginal effect of the interaction terms. Don’t worry too much about this if you haven’t seen it before (click on the previous link if you want to learn more).↩︎"
  },
  {
    "objectID": "posts/aere-gfw-gee/index.html",
    "href": "posts/aere-gfw-gee/index.html",
    "title": "Intro to Global Fishing Watch and Google Earth Engine",
    "section": "",
    "text": "I’m busy getting settled into the new job. (Probably should have posted something about that…) The Oregon summer is as splendid as everyone said it would be and I’m excited to explore my new surrounds.\nSummer time is also conference time. Last month, I was invited to give a talk at the annual conference of the Association of Environmental and Resource Economists (AERE to her friends). More specifically, I presented at the pre-conference workshop, which was all about the role that remote sensing and satellite data can play in environmental and natural resource economics. I’ve been very fortunate to gain a relatively early exposure to this world though my involvement with the Global Fishing Watch project, as well as Google Earth Engine – the combined subjects of my talk. I’m strongly of the opinion that satellite data is underutilised by environmental economists at present, but will become a mainstay of the field in years to come.\nThe slides for my talk can be viewed here.\n(Hit F11 to go fullscreen and “p” to see my speaker notes.)"
  },
  {
    "objectID": "posts/duckdb-polars-workshop/index.html",
    "href": "posts/duckdb-polars-workshop/index.html",
    "title": "DuckDB + Polars workshop",
    "section": "",
    "text": "On May 22, I’ll be be giving an online workshop entitled (Pretty) big data wrangling with DuckDB and Polars.\nHere is the description:\n\nThis workshop will introduce you to DuckDB and Polars, two data wrangling libraries at the frontier of high-performance computation. (See benchmarks.) In addition to being extremely fast and portable, both DuckDB and Polars provide user-friendly implementations across multiple languages. This makes them very well suited to production and applied research settings, without the overhead of tools like Spark. We will provide a variety of real-life examples in both R and Python, with the aim of getting participants up and running as quickly as possible. We will learn how wrangle datasets extending over several hundred million observations in a matter of seconds or less, using only our laptops. And we will learn how to scale to even larger contexts where the data exceeds our computers’ RAM capacity. Finally, we will also discuss some complementary tools and how these can be integrated for an efficient end-to-end workflow (data I/O -&gt; wrangling -&gt; analysis).\n\nThe attendance fee is 20 EUR/USD and all proceeds will be going towards aid organizations in Ukraine. Please see the WFU website for details.\nI’m pretty psyched about these tools and use them all the time in my own work. Please consider joining or sponsoring someone else. I promise to make it worthwile and you’ll be contributing to a good cause at the same time.\nP.S. Workshop materials will be made available here:\n\nWebsite: https://grantmcdermott.com/duckdb-polars\nGitHub: https://github.com/grantmcdermott/duckdb-polars"
  },
  {
    "objectID": "posts/recent-radio-interviews/index.html",
    "href": "posts/recent-radio-interviews/index.html",
    "title": "Some recent radio interviews",
    "section": "",
    "text": "Here are two radio interviews that I did recently, covering the blue paradox and related topics. It’s like reading the actual paper, except quicker and with an accent.\n\nNPR interview on OPB’s “Think Out Loud” with Dave Miller.\n\nListen to “The Blue Paradox” on Spreaker.\n\n\nJefferson Radio’s “Curious: Research meets radio” with Geoffrey Riley."
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html",
    "href": "posts/efficient-simulations-r/index.html",
    "title": "Efficient simulations in R",
    "section": "",
    "text": "Being able to code up efficient simulations is one of the most useful skills that you can develop as a social (data) scientist. Unfortunately, it’s also something that’s rarely taught in universities or textbooks.1 This post will cover some general principles that I’ve adopted for writing fast simulation code in R.\nI should clarify that the type of simulations that I, personally, am most interested in are related to econometrics. For example, Monte Carlo experiments to better understand when a particular estimator or regression specification does well (or poorly). The guidelines here should be considered accordingly and might not map well on to other domains (e.g. agent-based models or numerical computation). {: .notice–info}"
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#motivation",
    "href": "posts/efficient-simulations-r/index.html#motivation",
    "title": "Efficient simulations in R",
    "section": "",
    "text": "Being able to code up efficient simulations is one of the most useful skills that you can develop as a social (data) scientist. Unfortunately, it’s also something that’s rarely taught in universities or textbooks.1 This post will cover some general principles that I’ve adopted for writing fast simulation code in R.\nI should clarify that the type of simulations that I, personally, am most interested in are related to econometrics. For example, Monte Carlo experiments to better understand when a particular estimator or regression specification does well (or poorly). The guidelines here should be considered accordingly and might not map well on to other domains (e.g. agent-based models or numerical computation). {: .notice–info}"
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#our-example-interaction-effects-in-panel-models",
    "href": "posts/efficient-simulations-r/index.html#our-example-interaction-effects-in-panel-models",
    "title": "Efficient simulations in R",
    "section": "Our example: Interaction effects in panel models",
    "text": "Our example: Interaction effects in panel models\nI’m going to illustrate by replicating a simulation result in a paper that I really like: “Interaction effects in econometrics” by Balli & Sørensen (2013) (hereafter, BS13).\nBS13 does various things, but one result in particular has had a big impact on my own research. They show that empirical researchers working with panel data are well advised to demean any (continuous) variables that are going to be interacted in a regression. That is, rather than estimating the model in “level” terms…\n\\[Y_{it} = \\mu_i + \\beta_1X1_{it} + \\beta_2X2_{it} + \\beta_3X1_{it} \\cdot X2_{it} + \\epsilon_{it}\\]\n… you should estimate the “demeaned” version instead2\n\\[Y_{it} = \\beta_0 + \\beta_1 (X1_{it} - \\overline{X1}_{i.}) + \\beta_2 (X2_{it} - \\overline{X2}_{i.}) + \\beta_3(X1_{it} - \\overline{X1}_{i.}) \\cdot (X2_{it} - \\overline{X2}_{i.}) + \\epsilon_{it}\\]\nHere, \\(\\overline{X1}_{i.}\\) refers to mean value of variable \\(X1\\) (e.g. GDP over time) for unit \\(i\\) (e.g. country).\nWe’ll get to the simulations in a second, but BS13 describe the reasons for their recommendation in very intuitive terms. The super short version — again, you really should read the paper — is that the level model can pick up spurious trends in the case of varying slopes. The implications of this insight are fairly profound… if for no other reason that so many applied econometrics papers employ interaction terms in a panel setting.3\nOkay, so a potentially big deal. But let’s see a simulation and thereby get the ball rolling for this post. I’m going to run a simulation experiment that exactly mimics one in BS13 (see Table 3). We’ll create a fake dataset where the true interaction is ZERO. However, the slope coefficient of one of the parent terms varies by unit (here: country). If BS13 is right, then including an interaction term in our model could accidentally result in a spurious, non-zero coefficient on this interaction term. The exact model is\n\\[y_{it} = \\alpha + x_{1,it} + 1.5x_{2,it} + \\epsilon_{it}\\]\n\nData generating function\nIt will prove convenient for me to create a function that generates an instance of the experimental dataset — i.e. corresponding to one simulation run — which is what you see in the code below. The exact details are not especially important. (I’m going to coerce the return object into a data.table instead of standard data frame, but I’ll get back to that later.) For now, just remember that the coefficient on any interaction term should be zero by design. I’ll preview the resulting dataset at the end of the code.\n\nlibrary(data.table)\n\n## Convenience function for generating our experimental panel data. Takes a \n## single argument: `sims` (i.e. how many simulation runs to do we want; defaults \n## to 1).\ngen_data = function(sims=1) {\n  \n  ## Total time periods in the the panel = 500\n  tt = 500\n  \n  sim = rep(rep(1:sims, each = 10), times = 2) ## Repeat twice b/c we have two countries\n  \n  ## x1 covariates\n  x1_A = 1 + rnorm(tt*sims, 0, 1)\n  x1_B = 1/4 + rnorm(tt*sims, 0, 1)\n  \n  ## Add second, nested x2 covariates for each country\n  x2_A = 1 + x1_A + rnorm(tt*sims, 0, 1)\n  x2_B = 1 + x1_B + rnorm(tt*sims, 0, 1)\n  \n  ## Outcomes (notice different slope coefs for x2_A and x2_B)\n  y_A = x1_A + 1*x2_A + rnorm(tt*sims, 0, 1)\n  y_B = x1_B + 2*x2_B + rnorm(tt*sims, 0, 1)\n  \n  ## Combine in a data table (basically just an enhanced data frame)\n  dat = \n    data.table(\n      sim,\n      id = as.factor(c(rep('A', length(x1_A)), rep('B', length(x1_B)))),\n      x1 = c(x1_A, x1_B),\n      x2 = c(x2_A, x2_B),\n      y = c(y_A, y_B)\n      )\n  \n  ## Demeaned covariates (grouped by country and simulation)\n  dat[, \n      `:=` (x1_dmean = x1 - mean(x1),\n            x2_dmean = x2 - mean(x2)),\n      by = .(sim, id)][]\n  \n  ## Optional set order i.t.o sims\n  setorder(dat, sim)\n  \n  return(dat)\n}\n## Generate an instance of the data (using the default arguments)\nset.seed(123)\nd = gen_data()\nd\n\n        sim     id            x1         x2          y    x1_dmean    x2_dmean\n      &lt;int&gt; &lt;fctr&gt;         &lt;num&gt;      &lt;num&gt;      &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n   1:     1      A  0.4395243534  0.4437256  0.3716463 -0.59506609 -1.61706557\n   2:     1      A  0.7698225105  0.7298675  1.7366279 -0.26476794 -1.33092373\n   3:     1      A  2.5587083141  3.5407281  5.5578472  1.52411787  1.47993688\n   4:     1      A  1.0705083914  1.9383333  4.2280693  0.03591794 -0.12245794\n   5:     1      A  1.1292877352 -0.4200550  0.8833686  0.09469729 -2.48084624\n  ---                                                                         \n 996:     1      B  0.1600248030  1.2366685  3.6943280 -0.08764048 -0.06972657\n 997:     1      B  1.3205160368  2.5756808  6.0263720  1.07285075  1.26928576\n 998:     1      B -1.1011003857  0.1763464 -1.1775877 -1.34876567 -1.13004861\n 999:     1      B -0.2726166972  1.2642393  3.4448738 -0.52028198 -0.04215572\n1000:     1      B  0.0008093222  0.5403238  1.9157509 -0.24685596 -0.76607128\n\n\nLet’s run some regressions on one simulated draw of our dataset. Since this is a panel model, I’ll use the (incredible) fixest package to control for country (“id”) fixed-effects.\n\nlibrary(fixest)\n\nmod_level = feols(y ~ x1 * x2 | id, d)\nmod_dmean = feols(y ~ x1_dmean * x2_dmean | id, d)\netable(mod_level, mod_dmean, se  = 'standard')\n\n                              mod_level          mod_dmean\nDependent Var.:                       y                  y\n                                                          \nx1                    1.195*** (0.0650)                   \nx2                    1.638*** (0.0394)                   \nx1 x x2             -0.1373*** (0.0187)                   \nx1_dmean                                0.9544*** (0.0577)\nx2_dmean                                 1.556*** (0.0388)\nx1_dmean x x2_dmean                        0.0199 (0.0213)\nFixed-Effects:      ------------------- ------------------\nid                                  Yes                Yes\n___________________ ___________________ __________________\nS.E. type                           IID                IID\nObservations                      1,000              1,000\nR2                              0.86768            0.86062\nWithin R2                       0.86761            0.86055\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWell, there you have it. The “level” model spuriously yields a statistically significant coefficient on the interaction term. In comparison, the “demeaned” version avoids this trap and also appears to have better estimated the parent term coefficients.\nCool. But to really be sure, we should repeat our simulation many times. (BS13 do it 20,000 times…) And, so, we now move on to the main purpose of this post: How do we write simulation code that efficiently completes tens of thousands of runs? Here follow some key principles that I try to keep in mind."
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#principle-1-trim-the-fat",
    "href": "posts/efficient-simulations-r/index.html#principle-1-trim-the-fat",
    "title": "Efficient simulations in R",
    "section": "Principle 1: Trim the fat",
    "text": "Principle 1: Trim the fat\nSubtitle: lm.fit() is your friend\nThe first key principle for writing efficient simulation code is to trim the fat as much as possible. Even small differences start to add up once you’re repeating operations tens of thousands of times. For example, does it really make sense to use fixest::feols() for this example data? As much as I am a huge fixest stan, in this case I have to say… no. The package is optimised for high-dimensional fixed-effects, clustered errors, etc. Our toy dataset contains just one fixed-effect (comprising two levels) and we are ultimately only interested in extracting a single coefficient for our simulation. We don’t even need to save the standard errors. Most of fixest’s extra features are essentially wasted. We could probably do better just by using a simple lm() call and specifying the country fixed-effect (“id”) as a factor.\nHowever, lm() objects still contain quite a lot of information (and invoke extra steps) that we don’t need. We can simplify things even further by directly using the fitting function that lm calls underneath the hood. Specifically, the lm.fit() function. This requires a slightly different way of writing our regression model — closer to matrix form — but yields considerable speed gains. Here’s a benchmark to demonstrate.\n\nlibrary(microbenchmark)\n\nmicrobenchmark(\n  feols  = feols(y ~ x1_dmean * x2_dmean | id, d),\n  lm     = lm(y ~ x1_dmean * x2_dmean + id, d),\n  lm.fit = lm.fit(cbind(1, d$x1_dmean, d$x2_dmean, d$x1_dmean*d$x2_dmean, d$id), d$y),\n  times  = 2\n  )\n\nUnit: microseconds\n   expr      min       lq     mean   median       uq      max neval cld\n  feols 4779.250 4779.250 4944.217 4944.217 5109.184 5109.184     2  a \n     lm  896.933  896.933 1628.575 1628.575 2360.217 2360.217     2   b\n lm.fit   93.579   93.579  115.151  115.151  136.723  136.723     2   b\n\n\nFor this small dataset example, a regular lm() call is about five times faster than feols()… and lm.fit() is a further ten times faster still. Now, we’re talking microseconds here and the difference is not something you’d notice running a single regression. But… once you start running 20,000 of them, then those microseconds start to add up.4 Final thing, just to prove that we’re getting the same coefficients:\n\ncoef(lm.fit(cbind(1, d$x1_dmean, d$x2_dmean, d$x1_dmean*d$x2_dmean, d$id), d$y))\n\n         x1          x2          x3          x4          x5 \n 3.16116857  0.95435281  1.55596182  0.01993248 -0.14977947 \n\n\nThe output is less visually appealing a regular regression summary, but we can see the interaction term coefficient of 0.01993247 in the order in which it appeared (i.e. “x4”). FWIW, you can also name the coefficients in the design matrix if you wanted to make it easier to reference a coefficient by name. This is what I’ll be doing in the full simulation right at the end.\n\ncoef(lm.fit(cbind('intercept' = 1, 'x1' = d$x1_dmean, 'x2' = d$x2_dmean, 'x1:x2' = d$x1_dmean*d$x2_dmean, 'id' = d$id), d$y))\n\n  intercept          x1          x2       x1:x2          id \n 3.16116857  0.95435281  1.55596182  0.01993248 -0.14977947 \n\n\n\n\n\n\n\n\nTip\n\n\n\nThere is an even faster .lm.fit function (note the leading .). The same ideas carry over as with “regular” lm.fit(). See follow up post here."
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#principle-2-generate-your-data-once",
    "href": "posts/efficient-simulations-r/index.html#principle-2-generate-your-data-once",
    "title": "Efficient simulations in R",
    "section": "Principle 2: Generate your data once",
    "text": "Principle 2: Generate your data once\nSubtitle: It’s much quicker to generate one large dataset than many small ones\nOne common bottleneck I see in a lot of simulation code is generating a small dataset for each new run of a simulation. This is much less efficient that generating a single large dataset that you can either sample from during each iteration, or subset by a dedicated simulation ID. We’ll get to iteration next, but this second principle really stems from the same core idea: vectorisation in R is much faster than iteration. Here’s a simple benchmark to illustrate, where we generate data for a 100 simulation runs. Note that the relative difference would keep growing as we added more simulations.\n\nmicrobenchmark(\n  many_small = lapply(1:100, gen_data),\n  one_big = gen_data(100),\n  times = 2\n)\n\nUnit: milliseconds\n       expr       min        lq       mean     median         uq        max\n many_small 1473.3100 1473.3100 1751.12735 1751.12735 2028.94469 2028.94469\n    one_big   23.1516   23.1516   23.16008   23.16008   23.16856   23.16856\n neval cld\n     2  a \n     2   b"
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#principle-3-go-parallel-or-nest",
    "href": "posts/efficient-simulations-r/index.html#principle-3-go-parallel-or-nest",
    "title": "Efficient simulations in R",
    "section": "Principle 3: Go parallel or nest",
    "text": "Principle 3: Go parallel or nest\nSubtitle: Let data.table and co. handle the heavy lifting\nThe standard approach to coding up a simulation is to run everything as an iteration, either using a for() loop or an lapply() call. Experienced R programmers are probably reading this section right now and thinking, “Even better; run everything in parallel.” And it’s true. A Monte Carlo experiment like the one we’re doing here is ideally suited to parallel implementation, because each individual simulation run is independent. It’s a key reason why Monte Carlo experiments are such popular tools for teaching parallel programming concepts. (Guilty as charged.)\nBut any type of explicit iteration — whether it is a for() loop or an lapply() call, or whether it is run sequentially or in parallel — runs up against the same problem as we saw in Principle 2. Specifically, it is slower than vectorisation. So how can we run our simulations in vectorised fashion? Well, it turns out there is a pretty simple way that directly leverages Principle 2’s idea of generating one large dataset: We nest our simulations directly in our large data.table or tibble.\nHadley and Garret’s R for Data Science book has a nice chapter on model nesting with tibbles, and then Vincent has a cool blog post replicating the same workflow with data.table. But, really, the core idea is pretty simple: We can use the advanced data structure and functionality of tibbles or data.tables to run our simulations as grouped operations (i.e. by simulation ID). In other words, just like we can group a data frame and then collapse down to (say) mean values, we can also group a data frame and then run a regression on each subgroup.\nWhy might this be faster than explicit parallel iteration? Well, basically it boils down to the fact that data.tables and tibbles provide an enhanced structure for returning complex objects (including list columns) and their grouped operations are highly optimised to run in (implicit) parallel at the C++ level.5 The internal code of data.table, in particular, is just so insanely optimised that trying to beat it with some explicit parallel loop can be a fool’s errand.\nOkay, so let’s see a benchmark. I’m going to compare three options for simulating 100 draws: 1) sequential iteration with lapply(), 2) explicit parallel iteration with parallel::mclapply, and 3) nested (implicit parallel) iteration. For the latter, I’m simply grouping my dataset by simulation ID and then leveraging data.table’s powerful .SD syntax.6 Note further than I’m going to run regular lm() calls rather than lm.fit() — see Principle 1 — because I want to keep things simple and familiar for the moment.\n\nlibrary(parallel) ## For parallel::mclapply\n\n## Generate dataset with 1000 simulation draws\nset.seed(123)\nd = gen_data(100)\n\nmicrobenchmark(\n    sequential = lapply(1:max(d$sim), \n                        function(i) coef(lm(y ~ x1 * x2 + id, d[sim==i]))['x1:x2']\n                        ),\n    \n    parallel = mclapply(1:max(d$sim), \n                        function(i) coef(lm(y ~ x1 * x2 + id, d[sim==i]))['x1:x2'], \n                        mc.cores = detectCores()\n                        ),\n    \n    nested = d[, coef(lm(y ~ x1 * x2 + id, .SD))['x1:x2'], by = sim],\n    \n    times = 2\n    )\n\nUnit: milliseconds\n       expr       min        lq     mean   median       uq      max neval cld\n sequential 142.18928 142.18928 161.5120 161.5120 180.8346 180.8346     2   a\n   parallel 126.91207 126.91207 128.9292 128.9292 130.9463 130.9463     2   a\n     nested  99.18945  99.18945 101.9551 101.9551 104.7208 104.7208     2   a\n\n\nOkay, not a huge difference between the three options for this small benchmark. But — trust me — the difference will grow for the full simulation where we’re comparing the level vs demeaned regressions with lm.fit(). UPDATE: Upon reflection, I’m not being quite fair to mclapply() here, because it is being penalised for overhead on a small example. But I definitely stand by my next point. There are also some other reasons why relying on data.table will help us here. For example, parallel::mclapply() relies on forking, which is only available on Linux or Mac. Sure, you could use a different package like future.apply to provide a parallel backend (PSOCK) for Windows, but that’s going to be slower. Really, the bottom line is that we can outsource all of that parallel overhead to data.table and it will automatically handle everything at the C(++) level. Winning."
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#principle-4-use-matrices-for-an-extra-edge",
    "href": "posts/efficient-simulations-r/index.html#principle-4-use-matrices-for-an-extra-edge",
    "title": "Efficient simulations in R",
    "section": "Principle 4: Use matrices for an extra edge",
    "text": "Principle 4: Use matrices for an extra edge\nSubtitle: Save your simulation from having to do extra conversion work\nThe primary array format of empirical work is the data frame. It’s what we all use, really, so there’s no point expanding on that. (TL;DR data frames are just very convenient for humans to work with and reason about.) However, regressions are run on matrices. Which is to say that when you run a regression in R — and most other languages for that matter — behind the scenes your input data frame is first converted to an equivalent matrix before any computation gets done. Matrices have several features that make them “faster” to compute on than data frames. For example, every element must be of the same type (say, numeric). But let’s just agree that converting a data frame to a matrix requires at least some computational effort. Consider then what happens when we feed our lm.fit() function a pre-created design matrix, instead asking it to convert a bunch of data frame columns on the fly.\n\nset.seed(123)\nd = gen_data()\n\nX = cbind(intercept = 1, x1 = d$x1_dmean, x2 = d$x2_dmean, 'x1:x2' = d$x1_dmean*d$x2_dmean, id = d$id)\nY = d$y\n\nmicrobenchmark(\n  lm.fit = lm.fit(cbind(1, d$x1_dmean, d$x2_dmean, d$x1_dmean*d$x2_dmean, d$id), d$y),\n  lm.fit_mat = lm.fit(X, Y),\n  times  = 5\n  )\n\nUnit: microseconds\n       expr    min     lq     mean median     uq     max neval cld\n     lm.fit 75.345 78.277 108.4792 81.008 84.877 222.889     5   a\n lm.fit_mat 57.491 59.186  60.5674 59.268 61.745  65.147     5   a\n\n\nWe’re splitting hairs at this point. I mean, what’s 20 microseconds between friends? And, yet, these 20 microseconds translate to a roughly 40% improvement in relative terms. As I keep saying, even microseconds add up once you multiply them by a couple thousand.\n“Okay, Grant.” I can already you you saying. “You just told us to use data.tables and now you’re telling us to switch to matrices. Which is it, man?!” Well, remember what I said earlier about the enhanced structure that data.tables (and tibbles) offer us. We can easily create a list column of matrices inside a data.table (or tibble). We could have done this directly in the gen_data() function. But I’m going to leave that function as-is, and show you how simple it is to collapse columns of an existing data.table into a matrix list column. Once more we’ll use a standard grouped operation — where we are grouping by sim — to do the work:\n\nd = d[, \n      .(Y = list(y),\n        X_level = list(cbind(intercept = 1, x1 = x1, x2 = x2, 'x1:x2' = x1_dmean*x2_dmean, id = id)),\n        X_dmean = list(cbind(intercept = 1, x1 = x1_dmean, x2 = x2_dmean, 'x1:x2' = x1_dmean*x2_dmean, id = id))),\n      by = sim]\nd\n\n     sim                                                               Y\n   &lt;int&gt;                                                          &lt;list&gt;\n1:     1 0.3716463,1.7366279,5.5578472,4.2280693,0.8833686,6.8554351,...\n                                                                                                                                                                                                                                                                                                                                                                                                     X_level\n                                                                                                                                                                                                                                                                                                                                                                                                      &lt;list&gt;\n1:  1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000, 0.439524353, 0.769822511, 2.558708314, 1.070508391, 1.129287735, 2.715064987, 0.443725629, 0.729867467, 3.540728074, 1.938333259,-0.420055040, 4.755638443, 0.962260892, 0.352385931, 2.255598233,-0.004398437,-0.234929410, 4.528622182, 1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000,...\n                                                                                                                                                                                                                                                                                                                                                                                                     X_dmean\n                                                                                                                                                                                                                                                                                                                                                                                                      &lt;list&gt;\n1:  1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000,-0.595066094,-0.264767937, 1.524117867, 0.035917944, 0.094697288, 1.680474539,-1.617065570,-1.330923731, 1.479936875,-0.122457940,-2.480846238, 2.694847244, 0.962260892, 0.352385931, 2.255598233,-0.004398437,-0.234929410, 4.528622182, 1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000, 1.000000000,...\n\n\nI know the printed output looks a little different, but the key thing to know is that each simulation is now represented by a single row. In this case, we only have one simulation, so our whole data table consists of just one row. Moreover, those fancy list columns contain all of the 500 panel observations — in matrix form — that we need run our regressions. To access whatever is inside one of the list columns, we “unnest” very simply by extracting the first element with brackets, i.e. [[1]]. For example, to extract the Y column of our single simulation dataset, we could do:\n\nhead(d$Y[[1]]) ## Just show the first few rows\n\n[1] 0.3716463 1.7366279 5.5578472 4.2280693 0.8833686 6.8554351\n\n\nIf you’d like to know more about this approach, than I highly recommend Vincent’s aforementioned blog post on the topic. The very last thing I’m going to show you here (since we’ll soon be adapting it to run our full simulation), is how easily everything carries over to operations inside a nested data table. In short, we just use the magic of .SD again:\n\nd[, head(.SD$Y[[1]]), by = sim]\n\n     sim        V1\n   &lt;int&gt;     &lt;num&gt;\n1:     1 0.3716463\n2:     1 1.7366279\n3:     1 5.5578472\n4:     1 4.2280693\n5:     1 0.8833686\n6:     1 6.8554351"
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#putting-it-all-together",
    "href": "posts/efficient-simulations-r/index.html#putting-it-all-together",
    "title": "Efficient simulations in R",
    "section": "Putting it all together",
    "text": "Putting it all together\nTime to put everything together and run this thing. Like BS13, I’m going to simulate 20,000 runs. I’ll print the time it takes to complete the full simulation at the bottom.\n\nset.seed(123)\n\n## Generate our large dataset of 20k simulations\nd = gen_data(2e4)\n\n## Optional: Set key for better collapse performance\nsetkey(d, sim)\n\n## Collapse into a nested data.table (1 row per simulation), with matrix list columns\nd = d[, \n      .(Y = list(y),\n        X_level = list(cbind(intercept = 1, x1 = x1, x2 = x2, 'x1:x2' = x1*x2, id = id)),\n        X_dmean = list(cbind(intercept = 1, x1 = x1_dmean, x2 = x2_dmean, 'x1:x2' = x1_dmean*x2_dmean, id = id))),\n      by = sim]\n\n## Run our simulation\ntic = Sys.time()\nsims = d[, \n         .(level = coef(lm.fit(.SD$X_level[[1]], .SD$Y[[1]]))['x1:x2'],\n           dmean = coef(lm.fit(.SD$X_dmean[[1]], .SD$Y[[1]]))['x1:x2']), \n         by = sim]\nSys.time() - tic\n\nTime difference of 2.860526 secs\n\n\nAnd look at that. Just over 2 seconds to run the full 20k simulations! (Can you beat that? Let me know in the comments… UPDATE: Turns out you can thanks to the even faster .lm.fit() function. See follow-up post here.)\nAll that hard work deserves a nice plot, don’t you think?\n\npar(family = 'HersheySans') ## Optional: Nice font for (base) plotting\n\nhist(sims$level, col = scales::alpha('skyblue', .7), border=FALSE,\n     main = 'Simulating interaction effects in panel data',\n     xlim = c(-0.3, 0.2), \n     xlab = 'Coefficient values',\n     sub = '(True value is zero)')\nhist(sims$dmean, add=TRUE, col = scales::alpha('red', .5), border=FALSE)\nabline(v = 0, lty = 2)\nlegend(\"topright\", col = c(scales::alpha(c('skyblue', 'red'), .5)), lwd = 10,\n       legend = c(\"Level\", \"Demeaned\"))\n\n\n\n\n\n\n\n\nHere we have replicated the key result in BS13, Table 3. Moral of the story: If you have an interaction effect in a panel setting (e.g. DiD!), it’s always worth demeaning your terms and double-checking that your results don’t change."
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#conclusion",
    "href": "posts/efficient-simulations-r/index.html#conclusion",
    "title": "Efficient simulations in R",
    "section": "Conclusion",
    "text": "Conclusion\nBeing able to write efficient simulation code is a very valuable skill. In this post we have replicated an actual published result, incorporating several principles that have served me well:\n\nTrim the fat (Subtitle: lm.fit() is your friend.)\nGenerate your data once (Subtitle: It’s much quicker to generate one large dataset than many small ones)\nGo parallel or nest (Subtitle: Let data.table and co. handle the heavy lifting)\nUse matrices for an extra edge (Subtitle: Save your simulation from having to do extra conversion work)\n\nYou certainly don’t have to adopt all of these principles to write your own efficient simulation code in R. There may even be cases where it’s more efficient to do something else. But I’m confident that incorporating at least one or two of them will generally make your simulations much faster.\nP.S. If you made it this far and still need convincing that simulations are awesome, watch John Rauser’s incredible talk, “Statistics Without The Agonizing Pain”."
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#references",
    "href": "posts/efficient-simulations-r/index.html#references",
    "title": "Efficient simulations in R",
    "section": "References",
    "text": "References\nBalli, Hatice Ozer, and Bent E. Sørensen. “Interaction effects in econometrics.” Empirical Economics 45, no. 1 (2013): 583-603. Link"
  },
  {
    "objectID": "posts/efficient-simulations-r/index.html#footnotes",
    "href": "posts/efficient-simulations-r/index.html#footnotes",
    "title": "Efficient simulations in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEd Rubin and I are writing a book that will attempt to fill this gap, among other things. Stay tuned!↩︎\nIn their notation, BS13 only demean the interacted terms on \\(\\beta_3\\). But demeaning the parent terms on \\(\\beta_1\\) and \\(\\beta_2\\) is functionally equivalent and, as we shall see later, more convenient when writing the code since we can use R’s * expansion operator to concisely specify all of the terms.↩︎\nGot a difference-in-differences model that uses twoway fixed-effects? Ya, that’s just an interaction term in a panel setting. In fact, the demeaning point that BS13 are making here — and actually draw an explicit comparison to later in the paper — is equivalent to the argument that we should control for unit-specific time trends in DiD models. The paper includes additional simulations demonstrating this equivalence, but I don’t want to get sidetracked by that here.↩︎\nAnother thing is that lm.fit() produces a much more limited, but leaner return object. We’ll be taxing our computer’s memory less as a result.↩︎\nThat’s basically all that vectorisation is; i.e. a loop implemented at the C(++) level.↩︎\nThis will closely mimic a related example in the data.table vignettes, which you should read if you’re interested to learn more.↩︎"
  },
  {
    "objectID": "posts/hello-quarto/index.html",
    "href": "posts/hello-quarto/index.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "If you’re reading this, it means that I successfully migrated my website over to Quarto. The previous Jekyll-based framework had served me well for almost a decade, but the wheels were starting to come off (or, at least seize up). Bizantine Ruby version requirements and incompatible security patches? Time to move on.\nI’ve been using Quarto for many of my ad hoc projects and it’s a pleasure to use. More importantly, it seems to have enough momentum behind that it’s going to be around for a long time yet. I can only muster the energy to change website frameworks once a decade. Speaking of which…\nMigrating was more painful than I’d anticipated. Not terrible, but not seamless either. The main paint points involved manually converting my old blog posts into the requisite Quarto posts/&lt;slug&gt;/index.qmd format (plus re-running code and fixing hyperlinks), and then getting the GitHub Actions CI deployment to run properly. RE: the former, I’ve decided to only pull in the more recent blog posts. (“Recent” being a relative concept, since I’ve hardly any new posts from the last few years; partly due to Jekyll headaches.) Perhaps I’ll find the strength to convert those early posts too, although I doubt that anyone really needs to hear the ramblings of an early 2010s graduate student at this point. OTOH I’m reliably informed that blogging is making a comeback, so who knows? RE: the latter, the main issue appears to have been an existing gh-pages cache. My problem was similar to this one, although I finally fixed it by deleting my gh-pages branch and old deployment point, and starting afresh. When all else fails, burn it down and built back up."
  },
  {
    "objectID": "posts/quackchat/index.html",
    "href": "posts/quackchat/index.html",
    "title": "QuackChat slides",
    "section": "",
    "text": "The University of Oregon runs a bi-monthly series of pub talks, or “QuackChats” (geddit?), where professors get to engage the public on their research.\nI gave such a talk on Tuesday, covering my research on “Big Data and Its Impact on Global Fisheries”. Given the informal setting, I tried to keep it somewhat irreverent and, I hope, entertaining. (The beers helped.) Thanks to everyone who came out!\nYou can view my slides here.\n(Hit F11 to go fullscreen and “p” to see my speaker notes.)"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Here is a collection of software tools that I have written and serve as primary maintainer for.\n\nR packages\n\nduckreg Very fast out-of-memory regressions with DuckDB.\netwfe Convenience functions for Extended Two-Way Fixed Effect regressions.\nggfixest Dedicated ggplot2 methods for fixest objects.\nlfe2fixest Convert lfe calls into their fixest equivalents.\nparttree Visualize 2-D decision tree partitions.\nritest Fast randomization inference on R model objects.\ntinyplot Lightweight extension of the base R graphics system.\n\nIn addition to the above, I am a fairly active contributor to other R packages for which I am not the primary maintainer. You can take a look at the activity log on my GitHub page for more. Finally, I have also posted a number of gists over the years, usually after being nerd-sniped by some prompt on social media. Examples include a (fast) Bayesian bootstrap, stacked regression, etc.\n\n\nQuarto / R Markdown\n\nrevealjs-clean A minimal and elegant presentation theme for Quarto reveal.js.\nlecturenotes A personalised R Markdown template for writing lecture notes and academic papers. (2023 update: I recommend switching to Quarto instead of R Markdown.)\n\n\n\nWebsites\nI am a (co-)maintainer of various websites and webpages that collectively aim to improve the accessibility of data science software. These include:\n\nCRAN Econometrics Task View\nDiD (Difference-in-Differences)\nLOST (Library of Statistical Techniques)\nStata2R\n\n\n\nMiscellaneous\nA random selection of other guides, scripts, and tools:\n\narch-tips A fairly detailed changelog and collection of customization tips for Arch Linux.\nCausal Inference: The Mixtape (Fast Forward ed.) A lean ’n mean reworking of the code accompanying The Mixtape book, which focuses on performance and concision (both the code itself and package dependencies).\ncodespaces-r2u A minimal(ish) Codespaces environment for R. Provides a fully-functioning R environment up in the cloud (running on GitHub servers) at the click of a button.\ncontainerit-demo Simple demo for automating a Docker image build from your R environment, using the neat containerit package.\nhidpi.sh Shell script for fixing HiDPI scaling issues on Linux. Good for automating an otherwise laborious process following system or library updates.\nopen-access-fishery An interactive Shiny app for exploring open-access fishery dynamics. Good for an introductory resource economics class.\nrenv-rspm An example repo that demonstrates my recommonded approach to creating reproducible environments in R. Includes a video link. (Update: This approach should be enabled automatically with the latest release of )\ntikz-examples Some examples of how to draw TikZ and PGFPlots figures in LaTeX, with a focus on environmental economics topics (e.g. a negative externality with deadweight loss)."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Hello, Quarto\n\n\n\n\n\n\nmedia\n\n\n\nFinally migrating from Jekyll\n\n\n\n\n\nJan 6, 2025\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nDuckDB + Polars workshop\n\n\n\n\n\n\nR\n\n\nPython\n\n\nSQL\n\n\ndata science\n\n\nworkshops\n\n\n\nBig data backends for R and Python\n\n\n\n\n\nApr 10, 2024\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nFast geospatial tasks with data.table, geos & co.\n\n\n\n\n\n\nR\n\n\ngeospatial\n\n\ndata science\n\n\n\nUse all your favourites\n\n\n\n\n\nJan 14, 2022\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nSimulations remix: Turn up the base\n\n\n\n\n\n\nR\n\n\neconometrics\n\n\ndata science\n\n\n\nFaster still, with base R.\n\n\n\n\n\nJul 8, 2021\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient simulations in R\n\n\n\n\n\n\nR\n\n\neconometrics\n\n\ndata science\n\n\n\nBetter, faster, stronger.\n\n\n\n\n\nJun 24, 2021\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nA better way to adjust your standard errors\n\n\n\n\n\n\nR\n\n\nStata\n\n\neconometrics\n\n\n\nOn the fly, for the win.\n\n\n\n\n\nOct 19, 2020\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nEven more reshape benchmarks\n\n\n\n\n\n\ndata science\n\n\nR\n\n\nStata\n\n\nJulia\n\n\nPython\n\n\n\nGiving the people what they want\n\n\n\n\n\nJul 2, 2020\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nReshape benchmarks\n\n\n\n\n\n\nR\n\n\nStata\n\n\ndata science\n\n\n\nGoing wide to long with sparse data\n\n\n\n\n\nJun 30, 2020\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nMarginal effects and interaction terms\n\n\n\n\n\n\neconometrics\n\n\nR\n\n\n\nQuickly get the full marginal effect of interaction terms in R (and other software)\n\n\n\n\n\nDec 16, 2019\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nQuackChat slides\n\n\n\n\n\n\nfisheries\n\n\ntalks\n\n\n\nIf it talks like a duck…\n\n\n\n\n\nSep 13, 2018\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nSome recent radio interviews\n\n\n\n\n\n\nmedia\n\n\nfisheries\n\n\n\nYes I have an accent\n\n\n\n\n\nSep 10, 2018\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nThe Blue Paradox published in PNAS\n\n\n\n\n\n\nresearch\n\n\nfisheries\n\n\nreproducibility\n\n\n\nFishers go all in while the going’s good\n\n\n\n\n\nAug 29, 2018\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nBycatch paper published in Science\n\n\n\n\n\n\nresearch\n\n\nfisheries\n\n\nR\n\n\nreproducibility\n\n\n\nReforming global fisheries has important collateral benefits\n\n\n\n\n\nMar 16, 2018\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to Global Fishing Watch and Google Earth Engine\n\n\n\n\n\n\nworkshops\n\n\ncloud\n\n\ngeospatial\n\n\nfisheries\n\n\nclimate\n\n\n\nMy slides from AERE2017\n\n\n\n\n\nJul 6, 2017\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Server on Google Compute Engine\n\n\n\n\n\n\ndata science\n\n\nR\n\n\ncloud\n\n\n\nSay hello to cloud computing\n\n\n\n\n\nMay 30, 2017\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nQuick thoughts on Brexit\n\n\n\n\n\n\neconomics\n\n\nmedia\n\n\ntrade\n\n\ncurrencies\n\n\n\nHow relevant is the Norwegian experience?\n\n\n\n\n\nJun 24, 2016\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nDo weak currencies lead to trade deficits?\n\n\n\n\n\n\neconomics\n\n\nenergy\n\n\nmedia\n\n\ntrade\n\n\n\nSpoiler: Correlation != causation\n\n\n\n\n\nMay 1, 2016\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nCentralised versus decentralised\n\n\n\n\n\n\nenergy\n\n\nmedia\n\n\n\nWhat is the energy future of sub-Saharan Africa?\n\n\n\n\n\nJun 17, 2015\n\n\nGrant McDermott\n\n\n\n\n\n\n\n\n\n\n\n\nClimate capers at Cato\n\n\n\n\n\n\nclimate\n\n\n\nDodgy statistics at the Cato Institute\n\n\n\n\n\nDec 30, 2014\n\n\nGrant McDermott\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "“Dangerous Waters: The Economic Toll of Piracy on Maritime Shipping” With Renato Molina, Juan Carlos Villaseñor-Derbez, and Gavin G. McDonald. CESifo Working Paper No. 11077. Latest draft available here.\n“Labor reallocation and remote work during COVID-19: Real-time evidence from GitHub.” With B. Hansen. NBER Working Paper no. w29598.\n“Hydro power. Market might.” Paper.\n\n\n“The value of mass information.” With Christopher J. Costello and Michael B. Ward."
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "",
    "text": "“Dangerous Waters: The Economic Toll of Piracy on Maritime Shipping” With Renato Molina, Juan Carlos Villaseñor-Derbez, and Gavin G. McDonald. CESifo Working Paper No. 11077. Latest draft available here.\n“Labor reallocation and remote work during COVID-19: Real-time evidence from GitHub.” With B. Hansen. NBER Working Paper no. w29598.\n“Hydro power. Market might.” Paper.\n\n\n“The value of mass information.” With Christopher J. Costello and Michael B. Ward."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n“Sceptic priors and climate consensus.” Published in Climatic Change, 2021 (166:7). Paper. Pre-print. Code. Radio interview..\n“Alternate explanations for the blue paradox do not withstand statistical scrutiny.” With Kyle C. Meng, Gavin G. McDonald and Christopher J. Costello. Published in Proceedings of the National Academy of Sciences, 115 (52), E12124–E12125, 2018. Paper. Ungated. Code.\n“The Blue Paradox: Preemptive Overfishing in Marine Reserves.” With Kyle C. Meng, Gavin G. McDonald and Christopher J. Costello. Published in Proceedings of the National Academy of Sciences, 116 (12), 5319–5325, 2019. Paper. Ungated. Code. Blog post and media coverage.\n“Protecting marine mammals, turtles and birds by rebuilding global fisheries.” With Matthew G. Burgess and others. Published in Science, 359 (6381) 1255–31258, 2018. Paper. Ungated. Code. Blog post and media coverage.\n“Five rules for pragmatic blue growth.” With Matthew G. Burgess and others. Published in Marine Policy, 87 (2018) 331–339, 2018. Paper. Ungated.\n“Resolving disputes over ocean calamities.” With Matthew G. Burgess. Published in BioScience, 65(12), 1115-1116, 2015. Paper. Ungated.\n“Electricity prices, river temperatures and cooling water scarcity.” With Øivind A. Nilsen. Published in Land Economics, 90(1), 131-148, 2014. Paper. Ungated. Blog post and media coverage."
  },
  {
    "objectID": "research.html#report-chapters",
    "href": "research.html#report-chapters",
    "title": "Research",
    "section": "Report chapters",
    "text": "Report chapters\n“South Africa Compliance Analysis.” With James Morrissey and Davina Mendelsohn, in “Governing Global Climate Change: St Petersburg Compliance Report for the ‘G8 Plus Five’ Countries”, Maria Banda and Joanna Langille (eds.), G8 Final Compliance Report 2007, Oxford: G8 Research Group Oxford, 1 June 2007. xiii + 190 pp. Report.\n“The Current Status of the EPWP (Infrastructure) in the Western Cape.” With Anna McCord, Kim Adonis and Lisa van Dongen. Prepared for the Western Cape Provincial Treasury & Department of Transport And Public Works CAPE. Public Works Research Project, SALDRU, UCT, 24 March 2006. Report."
  },
  {
    "objectID": "posts/efficient-simulations-r-base-remix/index.html",
    "href": "posts/efficient-simulations-r-base-remix/index.html",
    "title": "Simulations remix: Turn up the base",
    "section": "",
    "text": "I apologise for the title of this post and will show myself the door shortly.\nI wanted to quickly follow up on my last post about efficient simulations in R. If you recall, in that post we used data.table and some other tricks to run 40,000 regressions (i.e. 20k simulations with 2 regressions each) in just over 2 seconds. The question before us today is: Can we go even faster using only base R? And it turns out that the answer is, yes, we can.\nMy motivation for a follow-up is partially the result of this very nice post by Benjamin Elbers, who replicates my simulation using Julia. In so doing, he demonstrates some of Julia’s killer features; most notably the fact that we don’t need to think about vectorisation — e.g. when creating the data — since Julia’s compiler will take care of that for us automatically.1 But Ben also does another interesting thing, which is to show the speed gains that come from defining our own (super lean) regression function. He uses Cholesky decomposition and it’s fairly straightforward to do the same thing in R. (Here is a nice tutorial by Issac Lee.)\nI was halfway on my way to doing this myself when I stumbled on a totally different post by R Core member, Martin Maechler. Therein he introduces the .lm.fit() function (note the leading dot), which incurs even less overhead than the lm.fit() function I mentioned in my last post. I’m slightly embarrassed to say I had never heard about it until now2, but a quick bit of testing “confirms” Martin’s more rigorous benchmarks: .lm.fit yields a consistent 30-40% improvement over even lm.fit.\nNow, it would be trivial to amend my previous simulation script to slot in .lm.fit() and re-run the benchmarks. But I thought I’d make this a bit more interesting by redoing the whole thing using only base R. (I’ll load the parallel package, but that comes bundled with the base distribution so hardly counts as cheating.) Here’s the full script with benchmarks for both sequential and parallel implementations at the bottom.\n# Data generation ---------------------------------------------------------\n\ngen_data = function(sims=1) {\n  \n  ## Total time periods in the the panel = 500\n  tt = 500\n  \n  ## x1 covariates\n  x1_A = 1 + rnorm(tt*sims, 0, 1)\n  x1_B = 1/4 + rnorm(tt*sims, 0, 1)\n  x1 = c(x1_A, x1_B)\n  \n  ## Add second, nested x2 covariates for each country\n  x2_A = 1 + x1_A + rnorm(tt*sims, 0, 1)\n  x2_B = 1 + x1_B + rnorm(tt*sims, 0, 1)\n  x2 = c(x2_A, x2_B)\n  \n  ## Outcomes (notice different slope coefs for x2_A and x2_B)\n  y_A = x1_A + 1*x2_A + rnorm(tt*sims, 0, 1)\n  y_B = x1_B + 2*x2_B + rnorm(tt*sims, 0, 1)\n  y = c(y_A, y_B)\n  \n  ## Group variables (id and sim)\n  id = as.factor(c(rep('A', length(x1_A)), rep('B', length(x1_B))))\n  sim = rep(rep(1L:sims, each = tt), times = length(levels(id)))\n  \n  ## Demeaned covariates\n  x1_dmean = x1 - ave(x1, list(sim, id), FUN = mean)\n  x2_dmean = x2 - ave(x2, list(sim, id), FUN = mean)\n  \n  ## Bind in a matrix\n  mat = cbind('sim' = sim, \n              'id' = id,\n              'y' = y,\n              'intercept' = 1, \n              'x1' = x1, \n              'x2' = x2, \n              'x1:x2' = x1*x2, \n              'x1_dmean:x2_dmean' = x1_dmean * x2_dmean)\n  \n  ## Set order i.t.o simulations\n  mat = mat[order(mat[, 'sim']), ]\n  \n  return(mat)\n}\n\n## How many simulations do we want?\nn_sims = 2e4\n\n## Generate them all as one big matrix\nd = gen_data(n_sims)\n\n## Create index list (for efficient subsetting of the large data matrix)\n## Note that each simulation is (2*500=)1000 rows long.\nii = lapply(1L:n_sims-1, function(i) 1L:1e3L + rep(1e3L*(i), each=1e3L))\n\n# Benchmarks --------------------------------------------------------------\n\nlibrary(microbenchmark) ## For high-precision timing\nlibrary(parallel)\nn_cores = detectCores()\n\n\n## Convenience function for running the two regressions and extracting the \n## interaction coefficients of interest (saves having to retype everything).\n## The key bit is the .lm.fit() function.\nget_coefs = function(dat) {\n  level = coef(.lm.fit(dat[, c('intercept', 'x1', 'x2', 'x1:x2', 'id')], \n                       dat[, 'y']))[4]\n  dmean = coef(.lm.fit(dat[, c('intercept', 'x1', 'x2', 'x1_dmean:x2_dmean', 'id')], \n                       dat[, 'y']))[4]\n  return(cbind(level, dmean))\n}\n\n## Run the benchmarks for both sequential and parallel versions\nmicrobenchmark(\n  sims_sequential = lapply(1:n_sims, \n                           function(i) {\n                             index = ii[[i]]\n                             get_coefs(d[index, ])\n                             }),\n  sims_parallel = mclapply(1:n_sims, \n                           function(i) {\n                             index = ii[[i]]\n                             get_coefs(d[index, ])\n                             }, \n                           mc.cores = n_cores\n                           ),\n  times = 1\n)\n\nUnit: milliseconds\n            expr       min        lq      mean    median        uq       max\n sims_sequential 2749.2756 2749.2756 2749.2756 2749.2756 2749.2756 2749.2756\n   sims_parallel  988.4186  988.4186  988.4186  988.4186  988.4186  988.4186\n neval\n     1\n     1\nThere you have it. Down to less than a second for a simulation involving 40,000 regressions using only base R.3 On a laptop, no less. Just incredibly impressive."
  },
  {
    "objectID": "posts/efficient-simulations-r-base-remix/index.html#footnotes",
    "href": "posts/efficient-simulations-r-base-remix/index.html#footnotes",
    "title": "Simulations remix: Turn up the base",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this house, we stan both R and Julia.↩︎\nI think Dirk Eddelbuettel had mentioned it to me, but I hadn’t grokked the difference.↩︎\nThe timings from this knitted Quarto document are actually about 20% slower than when I run the script directly in my R console. But we’re really splitting hairs now.↩︎"
  },
  {
    "objectID": "posts/blue-paradox-pnas/index.html",
    "href": "posts/blue-paradox-pnas/index.html",
    "title": "The Blue Paradox published in PNAS",
    "section": "",
    "text": "This blog posted is jointly written with Kyle Meng and has been cross-posted in a couple other places.\nCan you actually make a problem worse by promising to solve it?\nThat’s a conundrum that policymakers face — often unwittingly — on a variety of issues. A famous, if controversial, example comes from the gun control debate in the United States, where calls for tougher legislation in the wake of the 2012 Sandy Hook school massacre were followed by a surge in firearm sales. The reason? Gun enthusiasts tried to stockpile firearms before it became harder to purchase them.\nIn a new paper published in PNAS, we ask whether the same thing can happen with environmental conservation.\nThe short answer is “yes”. Using data from Global Fishing Watch (GFW), we show that efforts to ban fishing in a large, ecologically sensitive, part of the ocean paradoxically led to more fishing before the ban could be enforced.\nWe focus on the Phoenix Islands Protected Area (PIPA), a California-sized swathe of ocean in the central Pacific, known for its remarkable and diverse marine ecosystem. Fishing in PIPA has been banned since January 1, 2015, when it was established as one of the world’s largest marine reserves. The success in enforcing this ban has been widely celebrated by conservationists and scientists alike. Indeed, demonstrating this conclusively helped to launch GFW in the first place.\nHowever, it turns out that the story is more complicated than that. We show that there was a dramatic spike in fishing effort in the period leading up to the ban, as fishermen preemptively clamored to harvest resources while they still could. Here’s the key figure from the paper:\n\n\n\nFig. 3\n\n\nFocus on the red and blue lines in the top panel. The red line shows fishing effort in PIPA. The blue line shows fishing effort over a control region that serves as our counterfactual (i.e. it is very similar to PIPA but no ban was ever implemented there). The dashed vertical line shows the date when the fishing ban was enforced, on January 1, 2015. The earlier solid vertical line shows the earliest mention of an eventual PIPA ban that we could find in the news media, on September 1, 2013.\nNotice that fishing effort in the two regions are almost identical to that first news coverage, which is reassuring in terms of the validity of our control region. But then notice the dramatic increase in fishing over PIPA from September 1, 2003 to January 1, 2015, relative to the control group. You can see that difference (and the statistical significance of that difference) more clearly in the bottom panel. The area under that purple line is equivalent in terms of extra fishing to 1.5 years of avoided fishing after the ban.\nIn summary, anticipation of the fishing ban perversely led to more fishing, undermining the very conservation goal that was being sought and likely placing PIPA in a relatively impoverished state before the policy could be enforced. We call this phenomenon the “blue paradox”.\nAlongside our headline finding, there are several other things that we think are noteworthy about the paper:\n\nOur data are rich enough to precisely quantify magnitudes, which is hard to do in other settings. The fact that we can say the surge in fishing effort requires 1.5 years of banned fishing effort just to break even, is a big step forward compared to previous studies.\nSimilarly, previous studies of preemptive resource extraction have all been land-based and focus on the role of secure property rights. We now show that a preemptive response can happen even in a “commons” like the ocean, where property rights are nominally weak to non-existent. (This is an area for future research, although we speculate about some possible mechanisms in the paper.)\nThe blue paradox can help to explain a puzzle in the scientific literature: Why aren’t marine protected areas (MPAs) as effective as we thought they would be?\n\nThere’s more that we could say, much of which you can find in the paper itself. Please note that our intention is not to denigrate MPAs as a potentially valuable conservation tool, much less claim that PIPA was not worth it. (Far from it!) Rather, our goal is to spark a wider conversation about the tradeoffs involved in designing environmental policies, and the role that new data sources can play in informing those tradeoffs. As we conclude in the paper:\n\nWe end on a hopeful note, recognizing that the evidence presented herein would have been impossible only a few years ago due to data limitations. Thanks to the advent of incredibly rich satellite data provided by the likes of GFW, we now have the means to address previously unanswered questions and improve management of our natural resources accordingly.\n\nNote: “The blue paradox: Preemptive overfishing in marine reserves” (PNAS, 2018) is joint work between ourselves, Gavin McDonald and Chris Costello. All of the code and data used in the paper are available at https://github.com/grantmcdermott/blueparadox.\nPS — Some nice media coverage of our paper in The Atlantic, Oceana, Phys/UO, Science Daily/UCSB, and the PNAS blog. In addition, here are some radio interviews that I’ve done on the paper."
  },
  {
    "objectID": "posts/interaction-effects/index.html",
    "href": "posts/interaction-effects/index.html",
    "title": "Marginal effects and interaction terms",
    "section": "",
    "text": "I recently tweeted one of my favourite R tricks for getting the full marginal effect(s) of interaction terms. The short version is that, instead of writing your model as lm(y ~ f1 * x2), you write it as lm(y ~ f1 / x2). Here’s an example using everyone’s favourite mtcars dataset.\nFirst, partial marginal effects with the standard f1 * x2 interaction syntax.\n\nsummary(lm(mpg ~ factor(am) * wt, mtcars))\n\n\nCall:\nlm(formula = mpg ~ factor(am) * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     31.4161     3.0201  10.402 4.00e-11 ***\nfactor(am)1     14.8784     4.2640   3.489  0.00162 ** \nwt              -3.7859     0.7856  -4.819 4.55e-05 ***\nfactor(am)1:wt  -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nSecond, full marginal effects with the trick f1 / x2 interaction syntax.\n\nsummary(lm(mpg ~ factor(am) / wt, mtcars))\n\n\nCall:\nlm(formula = mpg ~ factor(am)/wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     31.4161     3.0201  10.402 4.00e-11 ***\nfactor(am)1     14.8784     4.2640   3.489  0.00162 ** \nfactor(am)0:wt  -3.7859     0.7856  -4.819 4.55e-05 ***\nfactor(am)1:wt  -9.0843     1.2124  -7.493 3.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nTo get the full marginal effect of factor(am)1:wt in the first case, I have to manually sum up the coefficients on the constituent parts (i.e. factor(am)1=14.8784 + factor(am)1:wt=-5.2984). In the second case, I get the full marginal effect of −9.0843 immediately in the model summary. Not only that, but the correct standard errors, p-values, etc. are also automatically calculated for me. (If you don’t remember, manually calculating SEs for multiplicative interaction terms is a huge pain. And that’s before we even get to complications like standard error clustering.)\nNote that the lm(y ~ f1 / x2) syntax is actually shorthand for the more verbose lm(y ~ f1 + f1:x2). I’ll get back to this point further below, but I wanted to flag the expanded syntax as important because it demonstrates why this trick “works”. The key idea is to drop the continuous variable parent term (here: x2) from the regression. This forces all of the remaining child terms relative to the same base. It’s also why this trick can easily be adapted to, say, Julia or Stata (see here).\nSo far, so good. It’s a great trick that has saved me a bunch of time (say nothing of likely user-error) that I recommend to everyone. Yet, I was prompted to write a separate blog post after being asked whether this trick a) works for higher-order interactions, and b) other non-linear models like logit? The answer in both cases is a happy “Yes!”."
  },
  {
    "objectID": "posts/interaction-effects/index.html#the-trick",
    "href": "posts/interaction-effects/index.html#the-trick",
    "title": "Marginal effects and interaction terms",
    "section": "",
    "text": "I recently tweeted one of my favourite R tricks for getting the full marginal effect(s) of interaction terms. The short version is that, instead of writing your model as lm(y ~ f1 * x2), you write it as lm(y ~ f1 / x2). Here’s an example using everyone’s favourite mtcars dataset.\nFirst, partial marginal effects with the standard f1 * x2 interaction syntax.\n\nsummary(lm(mpg ~ factor(am) * wt, mtcars))\n\n\nCall:\nlm(formula = mpg ~ factor(am) * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     31.4161     3.0201  10.402 4.00e-11 ***\nfactor(am)1     14.8784     4.2640   3.489  0.00162 ** \nwt              -3.7859     0.7856  -4.819 4.55e-05 ***\nfactor(am)1:wt  -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nSecond, full marginal effects with the trick f1 / x2 interaction syntax.\n\nsummary(lm(mpg ~ factor(am) / wt, mtcars))\n\n\nCall:\nlm(formula = mpg ~ factor(am)/wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     31.4161     3.0201  10.402 4.00e-11 ***\nfactor(am)1     14.8784     4.2640   3.489  0.00162 ** \nfactor(am)0:wt  -3.7859     0.7856  -4.819 4.55e-05 ***\nfactor(am)1:wt  -9.0843     1.2124  -7.493 3.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833, Adjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n\n\nTo get the full marginal effect of factor(am)1:wt in the first case, I have to manually sum up the coefficients on the constituent parts (i.e. factor(am)1=14.8784 + factor(am)1:wt=-5.2984). In the second case, I get the full marginal effect of −9.0843 immediately in the model summary. Not only that, but the correct standard errors, p-values, etc. are also automatically calculated for me. (If you don’t remember, manually calculating SEs for multiplicative interaction terms is a huge pain. And that’s before we even get to complications like standard error clustering.)\nNote that the lm(y ~ f1 / x2) syntax is actually shorthand for the more verbose lm(y ~ f1 + f1:x2). I’ll get back to this point further below, but I wanted to flag the expanded syntax as important because it demonstrates why this trick “works”. The key idea is to drop the continuous variable parent term (here: x2) from the regression. This forces all of the remaining child terms relative to the same base. It’s also why this trick can easily be adapted to, say, Julia or Stata (see here).\nSo far, so good. It’s a great trick that has saved me a bunch of time (say nothing of likely user-error) that I recommend to everyone. Yet, I was prompted to write a separate blog post after being asked whether this trick a) works for higher-order interactions, and b) other non-linear models like logit? The answer in both cases is a happy “Yes!”."
  },
  {
    "objectID": "posts/interaction-effects/index.html#dealing-with-higher-order-interactions",
    "href": "posts/interaction-effects/index.html#dealing-with-higher-order-interactions",
    "title": "Marginal effects and interaction terms",
    "section": "Dealing with higher-order interactions",
    "text": "Dealing with higher-order interactions\nLet’s consider a threeway interaction, since this will demonstrate the general principle for higher-order interactions. First, as a matter of convenience, I’ll create a new dataset so that I don’t have to keep specifying the factor variables.\n\nmtcars2 = transform(\n  mtcars,\n  vs = factor(vs),\n  am = factor(am)\n)\n\nNow, we run a threeway interaction and view the (naive, partial) marginal effects.\n\nfit1 = lm(mpg ~ am * vs * wt, mtcars2)\nsummary(fit1)\n\n\nCall:\nlm(formula = mpg ~ am * vs * wt, data = mtcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3055 -1.7152 -0.7279  1.3504  5.3624 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.0594     4.0397   6.203 2.07e-06 ***\nam1          17.3041     7.7041   2.246   0.0342 *  \nvs1           6.4677    10.1440   0.638   0.5298    \nwt           -2.4389     0.9689  -2.517   0.0189 *  \nam1:vs1      -4.7049    12.9763  -0.363   0.7201    \nam1:wt       -5.4749     2.4667  -2.220   0.0362 *  \nvs1:wt       -0.9372     3.0560  -0.307   0.7617    \nam1:vs1:wt    1.0833     4.4419   0.244   0.8094    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.469 on 24 degrees of freedom\nMultiple R-squared:  0.8701,    Adjusted R-squared:  0.8322 \nF-statistic: 22.96 on 7 and 24 DF,  p-value: 3.533e-09\n\n\nSay we are interested in the full marginal effect of the threeway interaction vs1:am1:wt. Even summing the correct parent coefficients is a potentially error-prone process of thinking through the underlying math (which terms are excluded from the partial derivative, etc.) And don’t even get me started on the standard errors…\nNow, it should be said that there are several existing tools for obtaining this number that don’t require us working through everything by hand. Here I’ll use my favourite such tool — the margins package — to save me the mental arithmetic.\n\nlibrary(margins)\n\nmargins_summary(\n  fit1,\n  variables = \"wt\",\n  at = list(vs = 1, am = 1),\n)[1,]\n\n factor     vs     am     AME     SE       z      p    lower   upper\n     wt 2.0000 2.0000 -7.7676 2.2903 -3.3916 0.0007 -12.2565 -3.2788\n\n\nWe now at least see that the full (average) marginal effect is −7.7676. Still, while this approach works well in the present example, we can also begin to see some downsides. It requires extra coding steps and comes with its own specialised syntax. Moreover, underneath the hood, margins relies on a numerical delta method that can dramatically increase computation time and memory use for even moderately sized real-world problems. (Is your dataset bigger than 1 GB? Good luck.) Another practical problem is that margins may not even support your model class. (See here.)\n\n\n\n\n\n\nmarginaleffects &gt;&gt; margins\n\n\n\nIn the period since I originally wrote this post, the margins package has come to be superseded by marginaleffects. The marginaleffects package is not only significantly faster, but also supports a much wider set of model classes and offers far richer functionality. So it’s essentially superior to margins in every way. Despite these improvements, I would still argue that the specific advantanges of the f1 / x2 trick carry over i.t.o. speed, convenience, etc. But for posterity here is the equivalent syntax for this newer package.\n\nlibrary(marginaleffects)\nslopes(\n  fit1,\n  variables = \"wt\",\n  newdata = datagrid(vs = 1, am = 1)\n)\n\nOkay, back to the original post…\n\n\nSo, what about the alternative? Does our little syntax trick work here too? The good news is that, yes, it’s just as simple as it was before.\n\nfit2 = lm(mpg ~ am / vs / wt, mtcars2)\nsummary(fit2)\n\n\nCall:\nlm(formula = mpg ~ am/vs/wt, data = mtcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3055 -1.7152 -0.7279  1.3504  5.3624 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.0594     4.0397   6.203 2.07e-06 ***\nam1          17.3041     7.7041   2.246  0.03417 *  \nam0:vs1       6.4677    10.1440   0.638  0.52978    \nam1:vs1       1.7629     8.0922   0.218  0.82939    \nam0:vs0:wt   -2.4389     0.9689  -2.517  0.01891 *  \nam1:vs0:wt   -7.9138     2.2685  -3.489  0.00190 ** \nam0:vs1:wt   -3.3761     2.8983  -1.165  0.25552    \nam1:vs1:wt   -7.7676     2.2903  -3.392  0.00241 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.469 on 24 degrees of freedom\nMultiple R-squared:  0.8701,    Adjusted R-squared:  0.8322 \nF-statistic: 22.96 on 7 and 24 DF,  p-value: 3.533e-09\n\n\nAgain, we get the full marginal effect of −7.7676 (and correct SE of 2.2903) directly in the model object. Much easier, isn’t it?\nWhere this approach really shines is in combination with plotting. Say, after extracting the coefficients with broom::tidy(), or simply plotting them directly with modelsummary::modelplot(). Model results are usually much easier to interpret visually, but this is precisely where we want to depict full marginal effects to our reader. Here I’ll use the modelsummary package to produce a nice coefficient plot of our threeway interaction terms.\n\nlibrary(modelsummary)\nlibrary(ggplot2)    ## for some extra ggplot2 layers\nlibrary(hrbrthemes) ## theme(s) I like\n\n## Optional: A dictionary of \"nice\" coefficient names for our plot\ndict = c('am0:vs0:wt' = 'Manual\\nStraight',\n         'am0:vs1:wt' = 'Manual\\nV-shaped',\n         'am1:vs0:wt' = 'Automatic\\nStraight',\n         'am1:vs1:wt' = 'Automatic\\nV-shaped')\n\nmodelplot(fit2, coef_map = dict) +\n  geom_vline(xintercept = 0, col = \"orange\") +\n  labs(\n    x = \"Marginal effect (Δ in MPG : Δ in '000 lbs)\",\n    title = \" Marginal effect of vehicle weight on MPG\", \n    subtitle = \"Conditional on transmission type and engine shape\"\n    ) +\n  theme_ipsum() \n\n\n\n\n\n\n\n\nThe above plot immediately makes clear how automatic transmission exacerbates the impact of vehicle weight on MPG. We also see that the conditional impact of engine shape is more ambiguous. In contrast, I invite you to produce an equivalent plot using our earlier fit1 object and see if you can easily make sense of it. I certainly can’t."
  },
  {
    "objectID": "posts/interaction-effects/index.html#aside-specifying-parent-terms-as-fixed-effects",
    "href": "posts/interaction-effects/index.html#aside-specifying-parent-terms-as-fixed-effects",
    "title": "Marginal effects and interaction terms",
    "section": "Aside: Specifying (parent) terms as fixed effects",
    "text": "Aside: Specifying (parent) terms as fixed effects\nOn the subject of speed, recall that the lm(y ~ f1 / x2) syntax is equivalent to the more verbose lm(y ~ f1 + f1:x2). This verbose syntax provides a clue for greatly reducing computation time for large models; particularly those with factor variables that contain many levels. We simply need specify the parent factor terms as fixed effects (using a specialised library like fixest). Going back to our introductory twoway interaction example, you would thus write the model as follows.\n\nlibrary(fixest)\n\n# Note: i(f1, x1) is like f1:x2 but with some special fixest features \n\n# feols(mpg ~ am:wt | am, mtcars2)\nfeols(mpg ~ i(am, wt) | am, mtcars2) \n\n(I’ll let you confirm for yourself that running the above models gives the correct −9.0843 figure from before.)\nIn case you’re wondering, the verbose equivalent for the f1 / f2 / x3 threeway interaction is f1 + f2 + f1:f2 + f1:f2:x3. So we can use the same FE approach for this more complicated case as follows.1\n\n## Option 1 using verbose base lm(). Not run.\n# summary(lm(mpg ~ am + vs + am:vs + am:vs:wt, mtcars2))\n\n## Option 2 using fixest::feols()\nfeols(mpg ~ am:vs:wt | am^vs, mtcars2)\n\nOLS estimation, Dep. Var.: mpg\nObservations: 32\nFixed-effects: am^vs: 4\nStandard-errors: Clustered (am^vs) \n           Estimate Std. Error       t value  Pr(&gt;|t|)    \nam0:vs0:wt -2.43889  2.330e-16 -1.048054e+16 &lt; 2.2e-16 ***\nam1:vs0:wt -7.91376  1.044e-15 -7.582149e+15 &lt; 2.2e-16 ***\nam0:vs1:wt -3.37612  1.514e-15 -2.229254e+15 &lt; 2.2e-16 ***\nam1:vs1:wt -7.76765  2.955e-15 -2.628411e+15 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 2.1381     Adj. R2: 0.832197\n               Within R2: 0.566525\n\n\nThere’s our desired −7.7676 coefficient again. This time, however, we also get the added bonus of clustered standard errors (which are switched on by default in fixest::feols() models).\nCaveat: The above example implicitly presumes that you don’t care about doing inference on the parent term(s), since these are swept away by the underlying fixed-effect procedures. That is clearly not going to be desirable in every case. But, in practice, I often find that it is a perfectly acceptable trade-off for models that I am running. (For example, when I am trying to remove general calender artefacts like monthly effects.)"
  },
  {
    "objectID": "posts/interaction-effects/index.html#other-model-classes",
    "href": "posts/interaction-effects/index.html#other-model-classes",
    "title": "Marginal effects and interaction terms",
    "section": "Other model classes",
    "text": "Other model classes\nThe last thing I want to demonstrate quickly is that our little trick carries over neatly to other model classes to. Say, that old workhorse of non-linear stats hot! new! machine! learning! classifier: logit models. Again, I’ll let you run these to confirm for yourself:\n\n## Tired\nsummary(glm(am ~ vs * wt, family = binomial, mtcars2))\n## Wired\nsummary(glm(am ~ vs / wt, family = binomial, mtcars2))\n\nOkay, I confess: That last code chunk was a trick to see who was staying awake during statistics class. I mean, it will correctly sum the coefficient values. But we all know that the raw coefficient values on generalised linear models like logit cannot be interpreted as marginal effects, regardless of whether there are interactions or not. Instead, we need to convert them via an appropriate link function. In R, the mfx package will do this for us automatically. My real point, then, is to say that we can combine the link function (via mfx) and our syntax trick (in the case of interaction terms). This makes a surprisingly complicated problem much easier to handle.\n\nlibrary(mfx, quietly = TRUE)\n\n## Broke\nlogitmfx(am ~ vs * wt, mtcars2)\n\nCall:\nlogitmfx(formula = am ~ vs * wt, data = mtcars2)\n\nMarginal Effects:\n           dF/dx Std. Err.        z   P&gt;|z|    \nvs1    -0.994682  0.045074 -22.0680 &lt; 2e-16 ***\nwt     -1.268124  0.639812  -1.9820 0.04748 *  \nvs1:wt  0.494044  0.719052   0.6871 0.49203    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndF/dx is for discrete change for the following variables:\n\n[1] \"vs1\"\n\n## Woke\nlogitmfx(am ~ vs / wt, mtcars2)\n\nCall:\nlogitmfx(formula = am ~ vs/wt, data = mtcars2)\n\nMarginal Effects:\n           dF/dx Std. Err.        z   P&gt;|z|    \nvs1    -0.994682  0.045074 -22.0680 &lt; 2e-16 ***\nvs0:wt -1.268124  0.639812  -1.9820 0.04748 *  \nvs1:wt -0.774081  0.673267  -1.1497 0.25025    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndF/dx is for discrete change for the following variables:\n\n[1] \"vs1\""
  },
  {
    "objectID": "posts/interaction-effects/index.html#conclusion",
    "href": "posts/interaction-effects/index.html#conclusion",
    "title": "Marginal effects and interaction terms",
    "section": "Conclusion",
    "text": "Conclusion\nWe don’t always want the full marginal effect of an interaction term. Indeed, there are times where we are specifically interested in evaluating the partial marginal effect. (In a difference-in-differences model, for example.) But in many other cases, the full marginal effect of the interaction terms is exactly what we want. The lm(y ~ f1 / x2) syntax trick (and its equivalents) is a really useful shortcut to remember in these cases.\nPS. In case, I didn’t make it clear: This trick works best when your interaction contains at most one continuous variable. (This is the parent “x” term that gets left out in all of the above examples.) You can still use it when you have more than one continuous variable, but it will implicitly force one of them to zero. Factor variables, on the other hand, get forced relative to the same base (here: the intercept), which is what we want.\nUpdate. Subsequent to posting this, I was made aware of this nice SO answer by Heather Turner, which treads similar ground. I particularly like the definitional contrast between factors that are “crossed” versus those that are “nested”."
  },
  {
    "objectID": "posts/interaction-effects/index.html#footnotes",
    "href": "posts/interaction-effects/index.html#footnotes",
    "title": "Marginal effects and interaction terms",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the fixest::feols case, we don’t have to specify all of the parent terms in the fixed-effects slot — i.e. we just need | am^vs — because these fixed-effects terms are all swept out of the model simultaneously at estimation time.↩︎"
  },
  {
    "objectID": "posts/quick-thoughts-brexit/index.html",
    "href": "posts/quick-thoughts-brexit/index.html",
    "title": "Quick thoughts on Brexit",
    "section": "",
    "text": "It’s amusing to see how many in the Leave contingent are invoking Norway as a model for the way forward post-Brexit… apparently without having a clue as to the rules governing Norway’s relationship with the Eurozone.\nBriefly:\n\nNorway is a member of the Schengen area. Unfettered movement across borders, etc. Per capita migration rates are consequently higher in Norway than they are in the UK. (Never mind that roughly half of the UK’s migration originates from countries outside the EU. You know, ex-colonies and the like, but then what have immigrants from Australia, New Zealand, India and South Africa ever done for the British economy?)\nNorway, as a (paid-up) member of the EEA, currently abides with virtually all of the EU mandates and regulations, even though it does not vote on them. It simply has to in order to sustain business and service channels with its main trading partners. As one of my former economics professors put it: “In Norway, we get to abide by all the EU rules, without actually having a say on what those rules are.”\nSpeaking of trade, Norway alongside Switzerland, the other poster child for Leave (more on them in a minute), have the highest import tariffs in the Western world. I don’t see how ushering in protectionism jibes with a new era of economic liberalism, but then perhaps the Leave campaign weren’t being quite sincere on this point.\nMuch of what I written for Norway is true for Switzerland. They too have to enshrine their trading relationships in a multitude of bilateral agreements. However, these agreements are largely silent on the services sector, which forms the lifeblood of the British economy. I’m not saying these agreements cannot be extended to include services. I’m saying that an existing framework cannot easily be lifted off the shelf… unless, perhaps, you want to mimic the existing EU rules, but then we’ve already been through that.\nThe two most important lessons that Norway can teach us about becoming a successful, small(ish) nation are i) have inclusive institutions and ii) find a shitload of North Sea oil. The good news is that the UK did find a shitload of North Sea oil back in the 1960s. The bad news is that, unlike Norway, you have already spent most of it.\n\nFinal points: I’m not suggesting that Britain can’t do well post-Brexit. I think markets will stabilise after today’s shock. However, political contagion remains a big risk. (Another Scottish Independence referendum anyone? Growing independence grumbles in France, the Netherlands and elsewhere?) And just think about the incentives of the EC and the rest of the Brussels brains trust. If their entire raison d’etre is to preserve the EU project, what prevents them from punishing Britain with unfavourable bilateral terms so as to deter further defection? Most of all, be wary of glib comparisons to Norway. The Norwegian European experience is anathema to many of the promises that Leave used to sway the vote."
  },
  {
    "objectID": "posts/science-bycatch/index.html",
    "href": "posts/science-bycatch/index.html",
    "title": "Bycatch paper published in Science",
    "section": "",
    "text": "A paper that I co-led with my good friend and frequent collaborator, Matt Burgess, has just been published in Science:\n“Protecting marine mammals, turtles, and birds by rebuilding global fisheries.”\nThis is the culmination of a lot of work over the last 18+ months. It was also a true interdisciplinary effort, involving a great team from a variety of backgrounds (economists, ecologists, conservation scientists, etc.). I’ve already tried to summarise the main contribution of the paper on Twitter and highlighted some of my favourite aspects. But here’s a blog post for those of you aren’t masochists on social media.\n(A quick clarification on definitions first for anyone who isn’t familiar with this literature: “Target stocks” refers to economically valuable fish stocks, like tuna, that are typically caught for human consumption. “Bycatch”, on the other hand, refers to unintended fisheries catch, like turtles or dolphins getting caught in tuna nets.)\nThe paper is about an important collateral benefit of reforming global fisheries. Namely, how many threatened bycatch species would we save in the process?\nIt turns out that the answer to this question is “quite a lot!”\nWe estimate that approximately half of these threatened bycatch species could be saved by reforming target fisheries in a way that maximizes long-term profits. Below is the key summary figure from the paper. The left-hand panel shows the fraction of threatened bycatch populations recovering at different cost levels (defined in terms of foregone profit in the relevant target fisheries). The solid green line denotes our mean estimate and the shaded green region denotes a 95% probability interval. Again, we can see that approximately half of the threatened bycatch populations start recovering at minor (&lt;5%) loss to the maximum profits of target fisheries. The right-hand panel shows the same thing, expressed in terms of targeting improvement (e.g. how much better tuna vessels need to get at catching only tuna instead of turtles or dolphins).\n\n\n\nFig. 4\n\n\nAt its heart, this paper is really a classic environmental economics question about negative externalities and spillover effects. If we fix one problem (unsustainable fishing), can we solve another (threatened bycatch species)? And, while the paper would have been impossible to complete without pulling in experts from a variety of fields, that core idea is what appealed to me as an environmental economist when Matt first proposed it.\nThere are a couple of other fundamental economic ideas that I’m very pleased to have incorporated into our analysis. For example, we’ve taken pains to ensure that foregone profits are calculated according to principles of economic efficiency. If a bycatch species (say, the E. Pacific leatherback turtle) requires additional reductions in fishing – i.e. beyond those dictated by the goal of maximising long-term profitability – then we don’t just assume that those reductions will be uniformly distributed among its various target fisheries (say, a mix of tuna and other demersal fisheries). Instead, we ask: “Where is it cheapest to reduce the next unit of fishing activity?” By ordering things in terms of marginal fishing costs, we ensure that we achieve our end conservation goal at the lowest possible cost.\nI’ll highlight two final aspects of the paper before closing.\nThe first is the extent to which we’ve tried to account for and even embrace multiple forms of uncertainty; every estimate and parameter in the paper has an explicit distribution attached to it. There are a lot of unknowns about fisheries bycatch. We wish we knew more. However, the reassuring – and potentially surprising – thing is just how robust our central finding is to these various forms of uncertainty and a raft of sensitivity tests.\nSecond, I’m proud of the efforts that we’ve taken to scrupulously document our work and make it available to others. Matt and I, together with the rest of our coauthors, are big supporters of scientific transparency and reproducibility. All of our analysis, R code and data is available on GitHub. If you were so inclined, though I don’t recommend it, you can even dig through our long commit history to see how we problem-solved and arrived at the final product. For example, here’s me mildly berating Matt after I discovered some well-hidden for-loops that were causing frustrating timeout issues and other problems. (Remember kids, avoid for-loops in R!)\nThat about covers it. I hope you enjoy reading the paper. Please feel free to play around with the code too. And a final caveat: One person’s bycatch might be another person’s lunch. As I discovered in an Egyptian supermarket some years ago…\n\n\n\n\n\nPS – Some nice coverage of our paper in Science Daily, Around the O, the EDF blog, and European Scientist."
  },
  {
    "objectID": "posts/centralised-versus-decentralised/index.html",
    "href": "posts/centralised-versus-decentralised/index.html",
    "title": "Centralised versus decentralised",
    "section": "",
    "text": "I was quoted in a recent article about bringing power to sub-Saharan Africa: “How do you bring electricity to 620 million people?” The journalist, Tom Jackson, did a good job of summarising my position (although I am mildly annoyed that he didn’t send me a copy before publication; something I asked for). That being said, some additional context never hurts and so I thought I’d publish my full email response to his questions.\nTwo minor footnotes: First, this was framed as a “centralised versus decentralised” debate. There are of course many variations on the decentralisation theme. (Do you really mean distributed generation, rather than transmission? Does this include microgrids? Etc.) Given the way the questions were asked, I simply took it to mean the absence of a centralised electricity grid. Second, when I talk about first-best and second-best alternatives, I don’t quite mean in the strict economic sense of optimality conditions. Rather, I am trying to convey the idea that one solution is only really better when the other is unavailable due to outside factors.\nWhy are grids still vital? Why is a functioning electricity grid necessary for economic growth?\nThese two questions are more or less the same, so I’ll take them together. Large, centralised grids constitute the most efficient and cost-effective way of delivering (and consuming) electricity in modern economies. Not only are decentralised options substantially more expensive and (generally) less reliable, there’s no intrinsic reason to believe that they will be better at delivering a clean energy future.\nIs the centralised argument being lost in places like SA where the grid is so poor and not being improved?\nI wouldn’t say that South Africa’s present electricity woes are the result of grid failure. Rather, the problem is primarily one of generation capacity and government mismanagement. On that note, the grid is the one component of the electricity system that is best thought of as a “natural monopoly”. (The other components of the electricity value chain – i.e. generation and distribution – should then be left to competitive forces.) Your question highlights an irony. Eskom’s mismanagement on the generation side (huge overspends and delays on the Medupi and Kusile power stations, etc.) are undermining confidence in its ability to manage a centralised grid, the one aspect that government can legitimately claim needs to be operated as a regulated monopoly.\nThat all being said, Eskom is falling behind the required investment goals for maintaining an adequate grid infrastructure into the future. A deficient grid network has also constrained economic growth in many other developing countries, from Nigeria to India. And, yet, this is not to say that the decentralised alternative offers an intrinsically superior solution. A grid system remains the first-best option. Decentralised solutions are really a second-best option in the absence of the former. The distinction is crucial.\nWhat role for de-centralised solutions?\nI think that decentralised solutions will remain a second-best, niche alternative for the next few decades. There are several things that cause me to take this position, of which intermittency and local storage are probably the most pronounced. Now, there do happen to be a number of exciting developments on the storage issue, but nothing that I would expect to fundamentally change the equation. More to the point, I believe that the resilience of a decentralised generation system will fundamentally require a functioning grid. The increased intermittency and smaller scale of decentralised power production will necessitate excellent access to similar, small-scale generation in other regions. This can only be achieved through a robust grid network. (An example may help to make my point: Germany’s much-fêted Energiewende was supposed to involve a fundamental shift towards the decentralised paradigm. What we’ve seen in practice, however, is that the Germans are investing hugely in extending their inter-regional grid capacities to places like Norway, whose hydropower resources offer the most cost-effective means of accommodating the intermittency of wind and solar.) Similarly, the parallels that people inevitably draw between a dentralised electricity system and the communication sector (i.e. where fixed-line telephones were leap-frogged by cell phones) are misplaced. Beyond various other differences, cell phones networks are fundamentally centralised in nature: Cell phone towers are the grid equivalent of the modern-day communications sector.\nI should probably conclude by saying that I fully support experimentation with decentralised systems. I just wouldn’t want to put my own money on it."
  },
  {
    "objectID": "posts/weak-zar-trade-deficit/index.html",
    "href": "posts/weak-zar-trade-deficit/index.html",
    "title": "Do weak currencies lead to trade deficits?",
    "section": "",
    "text": "Below is a recent article that I wrote for BizNews. The tl;dr version is that – surprise! – correlation doesn’t imply causation. Unfortunately, this simple rule continues to be ignored by many economic and financial pundits in the popular press.\nImagine that you are interested in determining the effect that policing has on crime. Say, does deploying more police officers lead to less crime? You might be tempted to answer this question as follows: 1) Gather up all the data that you can find on police numbers and crime statistics from different parts of the country. 2) Do some formal test (calculate a correlation coefficient, run a regression, etc.) and see what comes out of it.\nUnfortunately, this simplistic approach would be unmistakably and irrevocably wrong. For one thing, I can virtually guarantee that it would yield a positive relationship. (The headline writes itself: “Police cause crime!”) The reason – and also the reason why our naive approach is fundamentally flawed – is straightforward: More police are deployed in areas with higher crime rates. Our headline has it backwards and we have made the classic error of mistaking correlation for causation.\nOnce you really start to think about it, this seemingly simple (and oft-repeated) admonition of correlation-vs-causation has powerful implications. Almost every empirical question becomes more complicated than it first appears. Another example: Does sending your kids to a good high school improve their grades and increase their chances of attending university? Probably, but how can we truly be sure when good high schools already tend to attract students that have higher IQs, strong work ethics and/or wealthy parents – all of which contribute significantly toward university attendance in of themselves? The question we are asking fundamentally requires us to think about a counterfactual world where smart, hard-working kids with wealthy parents are equally likely to enroll in impoverished high schools with a poor track records, as they are good ones. (Ditto for children with less fortunate backgrounds and all those in between.) Only then can we compare these counterfactual worlds to our own and attribute any residual difference to the “causal effect” of good schooling. Notice that I haven’t even touched on the issue of cost and whether it is actually worth shelling out all that extra cash on a posh boarding school…\nOvercoming these sorts of problems is largely what modern-day economics and econometrics is all about. Economists even have a fancy word for it: “identification”. (As in, identifying true causal effects.) If you were to attend a presentation or seminar at any economics department around the world, I can assure you that much of the discussion would be given over to identification strategies, plausible counterfactuals, and the like. It can be hard and tedious work addressing these concerns, but it is always necessary. Similarly, it’s easy to run regressions. The tricky part is identifying meaningful relationships.\nI was reminded of such issues as I read a recent article in this very publication: “Ugly secret – Weak Rand hit SA trade, economy by R150bn since 2010.” By now, you will hopefully see where I am going with this, but if not: The article (and intro from our magnanimous editor, Alec Hogg) undeniably mistake correlation for causation. For example, how can we be sure that causation doesn’t run the other way around? I.e. That the weak Rand is not a response – preemptive by the SARB or otherwise – to deteriorating economic conditions? Or, that the Rand and trade surplus aren’t both being driven by some other unobserved and unaccounted factor(s)? What the economist Don Cox memorably called “the dreaded third thing”.\nThe answer is we can’t. Certainly not with the information presented in the article alone. A priori, it would be equally plausible to claim that a strong Rand would have made the trade surplus even worse than it currently is. (Note that this latter argument at least appeals to the correct definition of a counterfactual.)\nTo end on a conciliatory note: The Maynard article does raise several thoughtful points. The fact that demand for exported South African goods may be more elastic than previously thought, for instance, constitutes a perfectly valid and insightful economic argument. I should also state for the record that I am in no way, shape or form arguing in favour of a weak Rand policy. Among other things, it imports inflation at a time when citizens and businesses are already struggling to keep up with sharply rising prices. (I suspect that corruption and mismanagement in the presidency are at least as much to blame as SARB policy, but that is a discussion for another day.)\nStill, this is not the first time that I have become embroiled in discussions about the specific effects of a weak Rand. Then, as now, my overriding point is that we should be cautious of headline-grabbing statements regarding the direction and magnitude of such effects. A reliable rule of thumb is that arguments resting on simple correlation coefficients, or a single figure depicting two economic time series against each other, should be consumed with a healthy serving of salt. They are best viewed as inspiration for exploring deeper correlations and (potential) causation with more sophisticated methods, but no more.\nWe all know that correlation doesn’t imply causation. The real trick is in realising just how often this rule applies to the world around us.\nP.S. - Fortunately, economists do have a number of empirical tools and tricks to tease out reliable causal effects. I won’t discuss these in detail here, but just in case you’re wondering: 1) Deploying extra police does reduce crime (link); 2) Going to a good school doesn’t necessarily increase your grades, although it does improve behavioural outcomes (link); 3) Currency devaluations can be either contractionary or expansionary depending on the circumstances (link)."
  },
  {
    "objectID": "posts/climate-capers-at-cato/index.html",
    "href": "posts/climate-capers-at-cato/index.html",
    "title": "Climate capers at Cato",
    "section": "",
    "text": "NOTE: The code and data used to produce all of the figures in this post can be found here.\nHaving forsworn blogging activity for several months in favour of actual dissertation work, I thought I’d mark a return in time for the holidays. Our topic for discussion today is a poster by Cato Institute researchers, Patrick Michaels and “Chip” Knappenberger.\nMichaels & Knappenberger (hereafter M&K) argue that climate models have predicted more warming than we have observed in the global temperature data. It should be noted in passing that this is not a particularly new claim and I may have more to say about the general subject in a future post. Back to the specific matter at hand, however, M&K go further by trying to quantify the mismatch within a formal regression framework. In so doing, they conclude that it is incumbent upon the scientific community to reject current climate models in favour of less “alarmist” ones. (Shots fired!)\nLet’s take closer look at their analysis, shall we?\nIn essence, M&K have implemented a simple linear regression of temperature on a time trend,\n\\[Temp_t = \\alpha_0 + \\beta_1 Trend + \\epsilon_t.\\]\nThis is done recursively, starting from 2014 and incrementing backwards one year at a time until the sample extends to the middle of the 20th century. The key figure in their study is shown below and compares the estimated trend coefficient, \\(\\hat{\\beta_1}\\), from a bunch of climate models (i.e. the CMIP5 ensemble) with that obtained from observed global temperatures (i.e. the UK Met Office’s HadCRUT4 dataset).\n\nSince the observed warming trend consistently falls below that predicted by the suite of climate models, M&K conclude:\n\n“[A]t the global scale, this suite of climate models has failed. Treating them as mathematical hypotheses, which they are, means that it is the duty of scientists to reject their predictions in lieu of those with a lower climate sensitivity.”\n\nThese are bold words. Unfortunately for M&K, however, not so bold on substance. As we’re about to see, their analysis is curiously incomplete and their claims begin to unravel under further scrutiny.\n\nProblem 1: Missing confidence intervals\nFor starters, M&K’s claim about “failed hypotheses” doesn’t make much sense because they haven’t done any actual hypothesis testing. Indeed, they have completely neglected to display confidence intervals. Either that, or (and I’m not sure which is worse) they have forgotten to calculate them in the first place.\nThe point about missing confidence intervals is really important. The “spread” that we see in the above graph is just a pseudo-interval generated by the full CMIP5 ensemble (i.e. the individual trend means across all 100-odd climate models). It is a measure of model uncertainty, not statistical uncertainty. In other words, we haven’t yet said anything about the confidence intervals attached to each trend estimate, \\(\\hat{\\beta_1}\\). And, as any first-year econometrics student will tell you, regression coefficients must come with standard errors and corresponding confidence intervals. You could even go so far as to say that accounting for these uncertainties is the very thing that defines hypothesis testing. Can we confidently rule out that a parameter of interest doesn’t overlap with some value or range? Absent these measures of statistical uncertainty, one simply cannot talk meaningfully about a “failed hypothesis”.\nWith this mind, I have reproduced the key figure from M&K’s poster, but now with 95% error bars attached to each of the “observed” HadCRUT4 trend estimates.  As we can see from the updated graph, the error bars comfortably overlap the model ensemble at every point in time. In fact, the true degree of overlap is even greater than what see in this new figure. That’s because the figure only depicts the error bars which are attached to the trend in observed temperatures. Recall that we are also running the same regression on all 106 climate model. Each of these regressions will produce a trend estimate with its own error bars. This will widen the ensemble range and further underscore the degree of overlap between the models and observations. (Again, with the computer models we have to account for both the spread between the models’ mean coefficient estimates and their associated individual standard errors. This is one reason why this type of regression exercise is fairly limited – it doubles up on uncertainty.) Whatever the case, it seems fair to say that M&K’s bold assertion that we need to reject the climate models as “failed hypotheses” simply does not hold water.\n\n\nProblem 2: Fixed starting date\nMy second problem with M&K’s approach is their choice of a recursive regression model and fixed starting point. The starting point for all of the individual regressions is 2014, which just happens to be a year where observed temperatures were unusually low compared to the model estimates; at least, relative to other years. In other words, M&K are anchoring their results in a way that distorts the relative trends along the remainder of the recursive series.\nNow, you may counter that it makes sense to use the most recent year as your starting point. However, the principle remains: Privileging observations from any particular year is going to give you misleading results in climate research, where the long-term is what really matters. This problem is exacerbated in the present case, because of the aforementioned absence of confidence intervals. The early discrepancies between the models and observations have to be discounted because that is where the confidence intervals are largest (due to the smaller sample size). And, yet, you would have no idea that this was a problem if you only looked at M&K’s original figure. Again, the missing confidence intervals gives the misleading impression that all points are “created equal”, when they simply aren’t from a statistical perspective.\nRather than a recursive regression, I would therefore argue that a rolling regression offers a much better way of investigating the performance of climate models. Another alternative is to stick with recursive regressions, but to vary the starting date. This is what I have done in the figures below, using 2005 and then 2000 as the new fixed points. (For the sake of comparison, I keep the maximum trend length the same, so each of these figures goes a little further back in time.) The effect on the relative trend slopes – and therefore the agreement between climate models and observations – is clear to see."
  },
  {
    "objectID": "posts/fast-geospatial-datatable-geos/index.html",
    "href": "posts/fast-geospatial-datatable-geos/index.html",
    "title": "Fast geospatial tasks with data.table, geos & co.",
    "section": "",
    "text": "Note\n\n\n\nI’ve updated this post to reflect the fact that we no longer need the development version of data.table."
  },
  {
    "objectID": "posts/fast-geospatial-datatable-geos/index.html#motivation",
    "href": "posts/fast-geospatial-datatable-geos/index.html#motivation",
    "title": "Fast geospatial tasks with data.table, geos & co.",
    "section": "Motivation",
    "text": "Motivation\nThis blog post pulls together various tips and suggestions that I’ve left around the place. My main goal is to show you some simple workflows that I use for high-performance geospatial work in R, leaning on the data.table, sf and geos packages.\nIf you’re the type of person who likes to load everything at once, here are the R libraries and theme settings that I’ll be using in this post. (Don’t worry if not: I’ll be loading them again in the relevant sections below to underscore why I’m calling a specific library.)\n\n## Data wrangling\nlibrary(dplyr)\nlibrary(data.table)\n\n## Geospatial\nlibrary(sf)\nlibrary(geos)\n\n## Plotting\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\n## Benchmarking\nlibrary(microbenchmark)"
  },
  {
    "objectID": "posts/fast-geospatial-datatable-geos/index.html#data.table-sf-workflow",
    "href": "posts/fast-geospatial-datatable-geos/index.html#data.table-sf-workflow",
    "title": "Fast geospatial tasks with data.table, geos & co.",
    "section": "data.table + sf workflow",
    "text": "data.table + sf workflow\nEveryone who does spatial work in R is familiar with the wonderful sf package. You know, this one:\n\nlibrary(sf)\nlibrary(ggplot2); theme_set(theme_minimal())\n\n## Grab the North Carolina shapefile that comes bundled with sf\nnc_shapefile = system.file(\"shape/nc.shp\", package = \"sf\")\nnc = st_read(nc_shapefile)\n\nReading layer `nc' from data source `/usr/lib/R/library/sf/shape/nc.shp' using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n## Quick plot\nggplot(nc) + geom_sf()\n\n\n\n\n\n\n\n\nThe revolutionary idea of sf was (is) that it allowed you to treat spatial objects as regular data frames, so you can do things like this:\n\nlibrary(dplyr)\n\nnc |&gt;\n  group_by(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low')) |&gt;\n  summarise(geometry = st_union(geometry)) |&gt;\n  ggplot(aes(fill=region)) + \n  geom_sf()\n\n\n\n\n\n\n\n\nIn the above code chunk, I’m using dplyr to do a grouped aggregation on our North Carolina data object. The aggregation itself is pretty silly—i.e. divide the state into two hemispheres—but the same idea extends to virtually all of dplyr’s capabilities. It makes for a very potent and flexible combination that has driven an awful lot of R-based spatial work in recent years.\nAt the same time, there’s another powerful data wrangling library in R: data.table. This post is not going to rehash the (mostly pointless) debates about which of dplyr or data.table is better.1 But I think it’s fair to say that the latter offers incredible performance that makes it a must-use library for a lot of people, including myself. Yet it seems to me that many data.table users aren’t aware that you can use it for spatial operations in exactly the same way.\n\n\n\n\n\n\ndata.table v1.15.0+\n\n\n\nIf you’re following along on your own computer, make sure that you have data.table v1.15.0 (released January 2024) or a newer version before continuing:\n\n\nOkay, let’s create a “data.table” version of our nc object and take a quick look at the first few rows and some columns.\n\nlibrary(data.table)\n\nnc_dt = as.data.table(nc)\nnc_dt[1:3, c('NAME', 'CNTY_ID', 'geometry')]\n\n        NAME CNTY_ID                       geometry\n      &lt;char&gt;   &lt;num&gt;             &lt;sfc_MULTIPOLYGON&gt;\n1:      Ashe    1825 MULTIPOLYGON (((-81.47276 3...\n2: Alleghany    1827 MULTIPOLYGON (((-81.23989 3...\n3:     Surry    1828 MULTIPOLYGON (((-80.45634 3...\n\n\nAt this point, I have to briefly back up to say that the reason I wanted you to grab a newer version of data.table is that it “pretty prints” the columns by default. This not only includes the columns types and keys (if you’ve set any), but also the special sfc_MULTIPLOYGON list columns which is where the sf magic is hiding. It’s a small cosmetic change that nonetheless underscores the integration between these two packages.2\nJust like we did with dplyr earlier, we can now do grouped spatial operations on this object using data.table’s concise syntax:\n\nbb = st_bbox(nc) # https://github.com/Rdatatable/data.table/issues/6707\n\nnc_dt[\n  ,\n  .(geometry = st_union(geometry)),\n  by = .(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low'))\n] |&gt;\n  ggplot(aes(geometry=geometry, fill=region)) + \n  geom_sf() +\n  coord_sf(xlim = bb[c(1,3)], ylim = bb[c(2,4)]) +\n  labs(caption = \"Now brought to you by data.table\")\n\n\n\n\n\n\n\n\nNow, I’ll admit that there are a few tweaks we need to make to the plot call. Unlike with the non-data.table workflow, this time we have to specify the geometry aesthetic with aes(geometry=geometry, ...). Otherwise, ggplot2 won’t know what do with this object. The other difference is that it doesn’t automatically recognise the CRS (i.e. “NAD27”), so the projection is a little off. Again, however, that information is contained with the geometry column of our nc_dt object. It just requires that we provide the CRS to our plot call explicitly.\n\n## Grab CRS from the geometry column\ncrs = st_crs(nc_dt$geometry)\n\n## Update our previous plot\nlast_plot() + \n  coord_sf(xlim = bb[c(1,3)], ylim = bb[c(2,4)], crs = crs) +\n  labs(caption = \"Now with the right projection too\")\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\n\n\n\n\n\n\n\n\nPlotting tweaks aside, I don’t want to lose sight of the main point of this post, namely: sf and data.table play perfectly well together. You can do (grouped) spatial operations and aggregations inside the latter, exactly how you would any other data wrangling task. So if you love data.table’s performance and syntax, then by all means continue using it for your spatial work too. Speaking of performance…"
  },
  {
    "objectID": "posts/fast-geospatial-datatable-geos/index.html#speeding-things-up-with-geos",
    "href": "posts/fast-geospatial-datatable-geos/index.html#speeding-things-up-with-geos",
    "title": "Fast geospatial tasks with data.table, geos & co.",
    "section": "Speeding things up with geos",
    "text": "Speeding things up with geos\nUpdate (2022-02-16): The benchmarks in this section are a bit unfair, since geos assumes planar (“flat”) geometries, whereas sf assumes spherical (“curved”) geometries by default. See the postscript at the bottom of this post, which corrects for this discrepancy.\nAs great as sf is, even its most ardent proponents will admit that it can drag a bit when it comes to big geospatial tasks. I don’t want to imply that that it’s “slow”. But I’ve found that it does lag behind geopandas, for example, when I’m doing heavy geospatial computation or working with really large spatial files. Luckily, there’s a new package in town that offers major performance gains and plays very well with the workflow I demonstrated above.\nDewey Dunnington and Edzer Pebesma’s geos package covers all of the same basic geospatial operations as sf. But it does so by directly wrapping the underlying GEOS API, which is written in C and is thus extremely performant. Here’s a simple example, where we calculate the centroid of each North Carolina county.\n\nlibrary(geos)           ## For geos operations  \nlibrary(microbenchmark) ## For benchmarking\n\n## Create a geos geometry object\nnc_geos = nc |&gt; as_geos_geometry()\n\n## Benchmark\nmicrobenchmark(\n  sf = nc$geometry |&gt; st_centroid(),\n  geos = nc_geos |&gt; geos_centroid(), \n  times = 2\n  )\n\nUnit: microseconds\n expr      min       lq      mean    median       uq      max neval cld\n   sf 6312.215 6312.215 6717.8645 6717.8645 7123.514 7123.514     2  a \n geos  132.241  132.241  166.0505  166.0505  199.860  199.860     2   b\n\n\nA couple of things worth noting. First, the geos centroid calculation completes orders of magnitude faster than the sf equivalent. Second, the executing functions are very similar (st_centroid() vs geos_centroid()). Third, we have to do an explicit as_geos_geometry() coercion before we can perform any geos operations on the resulting object.\nThat last point seems the most mundane. (Why aren’t you talking more about how crazy fast geos is?!) But it’s important since it underscores a key difference between the two packages and why the developers view them as complements. Unlike sf, which treats spatial objects as data frames, geos only preserves the geometry attributes. Take a look:\n\nhead(nc_geos)\n\n&lt;geos_geometry[6] with CRS=NAD27&gt;\n[1] &lt;MULTIPOLYGON [-81.741 36.234...-81.240 36.590]&gt;\n[2] &lt;MULTIPOLYGON [-81.348 36.365...-80.903 36.573]&gt;\n[3] &lt;MULTIPOLYGON [-80.966 36.234...-80.435 36.565]&gt;\n[4] &lt;MULTIPOLYGON [-76.330 36.073...-75.773 36.557]&gt;\n[5] &lt;MULTIPOLYGON [-77.901 36.163...-77.075 36.556]&gt;\n[6] &lt;MULTIPOLYGON [-77.218 36.230...-76.707 36.556]&gt;\n\n\nGone are all those extra columns containing information about county names, FIPS codes, population numbers, etc. etc. We’re just left with the necessary information to do high-performance spatial operations.\n\nQuick aside on plotting geos objects\nBecause we’ve dropped all of the sf / data frame attributes, we can’t use ggplot2 to plot anymore. But we can use the base R plotting method:\n\nplot(nc_geos, col = \"gray90\")\nplot(geos_centroid(nc_geos), pch = 21, col = 'red', bg = 'red', add = TRUE)\n\n\n\n\n\n\n\n\nActually, that’s not quite true, since an alternative is to convert it back into an sf object with st_as_sf() and then call ggplot2. This is particularly useful because you can hand off some heavy calculation to geos before bringing it back to sf for any additional functionality. Again, the developers of these packages designed them to act as complements.\n\nggplot() +\n  geom_sf(data = nc) +\n  geom_sf(data = nc_geos |&gt; geos_centroid() |&gt; st_as_sf(), \n          col = \"red\")\n\n\n\n\n\n\n\n\nOkay, back to the main post…\n\n\ndata.table + geos workflow\nFinally, we get to the pièce de résistance of today’s post. The fact that as_geos_geometry() creates a GEOS geometry object—rather than preserving all of the data frame attributes—is a good thing for our data.table workflow. Why? Well, because we can just include this geometry object as a list column inside our data.table.3 In turn, this means you can treat spatial operations as you would any other operation inside a data.table. You can aggregate by group, merge, compare, and generally combine the power of data.table and geos as you see fit.\n(The same is true for regular data frames and tibbles, but we’ll get to that.)\nLet’s prove that this idea works by creating a GEOS column in our data.table. I’ll creatively call this column geo, but really you could call it anything you want (including overwriting the existing geometry column).\n\nnc_dt[, geo := as_geos_geometry(geometry)]\nnc_dt[1:3, c('NAME', 'CNTY_ID', 'geo')] ## Print a few rows/columns\n\n        NAME CNTY_ID                                              geo\n      &lt;char&gt;   &lt;num&gt;                                  &lt;geos_geometry&gt;\n1:      Ashe    1825 &lt;MULTIPOLYGON [-81.741 36.234...-81.240 36.590]&gt;\n2: Alleghany    1827 &lt;MULTIPOLYGON [-81.348 36.365...-80.903 36.573]&gt;\n3:     Surry    1828 &lt;MULTIPOLYGON [-80.966 36.234...-80.435 36.565]&gt;\n\n\nGEOS column in hand, we can manipulate or plot it directly from within the data.table. For example, we can recreate our previous centroid plot.\n\nplot(nc_dt[, geo], col = \"gray90\")\nplot(nc_dt[, geos_centroid(geo)], pch = 21, col = 'red', bg = 'red', add = TRUE)\n\n\n\n\n\n\n\n\nAnd here’s how we could replicate our earlier “hemisphere” plot:\n\nnc_dt[\n  ,\n  .(geo = geo |&gt; geos_make_collection() |&gt; geos_unary_union()),\n  by = .(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low'))\n][, geo] |&gt;\n  plot()\n\n\n\n\n\n\n\n\nThis time around the translation from the equivalent sf code isn’t as direct. We have one step (st_union()) vs. two (geos_make_collection() |&gt; geos_unary_union()). The second geo_unary_union() step is clear enough. But it’s the first geos_make_collection()step that’s key for our aggregating task. We have to tell geos to treat everything within the same group (i.e. whatever is in by = ...) as a collective. This extra step becomes very natural after you’ve done it a few times and is a small price to pay for the resulting performance boost.\nSpeaking of which, it’s nearly time for some final benchmarks. The only extra thing I want to do first is, as promised, include a tibble/dplyr equivalent. The exact same concepts and benefits carry over here, for those of you that prefer the tidyverse syntax and workflow.4\n\nnc_tibble = tibble::as_tibble(nc) |&gt; \n  mutate(geo = as_geos_geometry(geometry))\n\n\n\nBenchmarks\nFor this final set of benchmarks, I’m going to horserace the same grouped aggregation that we’ve been using throughout.\n\nmicrobenchmark(\n  \n  sf_tidy = nc |&gt;\n    group_by(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low')) |&gt;\n    summarise(geometry = st_union(geometry)),\n  \n  sf_dt = nc_dt[\n    ,\n    .(geometry = st_union(geometry)),\n    by = .(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low'))\n  ],\n  \n  geos_tidy = nc_tibble |&gt;  \n    group_by(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low')) |&gt;\n    summarise(geo = geos_unary_union(geos_make_collection(geo))),\n  \n  geos_dt = nc_dt[\n    ,\n    .(geo = geos_unary_union(geos_make_collection(geo))),\n    by = .(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low'))\n  ],\n  \n  times = 2\n)\n\nUnit: milliseconds\n      expr      min       lq     mean   median       uq      max neval  cld\n   sf_tidy 58.05696 58.05696 58.76766 58.76766 59.47837 59.47837     2 a   \n     sf_dt 52.38609 52.38609 52.74219 52.74219 53.09829 53.09829     2  b  \n geos_tidy 13.34309 13.34309 13.64761 13.64761 13.95212 13.95212     2   c \n   geos_dt 10.02459 10.02459 10.02478 10.02478 10.02496 10.02496     2    d\n\n\nResult: A 6x speed-up. Nice! While the toy dataset that we’re using here is too small to make a meaningful difference in practice, those same performance benefits will carry over to big geospatial tasks too. Being able to reduce your computation time by a factor of 10 really makes a difference once you’re talking minutes or hours."
  },
  {
    "objectID": "posts/fast-geospatial-datatable-geos/index.html#conclusion",
    "href": "posts/fast-geospatial-datatable-geos/index.html#conclusion",
    "title": "Fast geospatial tasks with data.table, geos & co.",
    "section": "Conclusion",
    "text": "Conclusion\nMy takeaways:\n\nIt’s fine to treat sf objects as data.tables (or vice versa) if that’s your preferred workflow. Just remember to specify the geometry column.\nFor large (or small!) geospatial tasks, give the geos package a go. It integrates very well with both data.table and the tidyverse, and the high-performance benefits carry over to both ecosystems.\n\nBy the way, there are more exciting high-performance geospatial developments on the way in R (as well as other languages) like geoarrow. We’re lucky to have these tools at our disposal.\n\nPostscript: planar vs spherical\nNote: This section was added on 2021-01-16.\nAs Roger Bivand points out on Twitter, I’m not truly comparing apples with apples in the above benchmarks. geos assumes planar (“flat”) geometries, whereas sf does the more complicated task of calculating spherical (“curved”) geometries. More on that here if you are interested. Below I repeat these same benchmarks, but with sf switched to the same planar backend. The upshot is that geos is still faster, but the gap narrows considerably. A reminder that we’re also dealing with a very small dataset, so I recommend benchmarking on your own datasets to avoid the influence of misleading overhead. But I stand by my comment that these differences persist at scale, based on my own experiences and testing.\n\n## Turn off sf's spherical (\"S2\") backend\nsf_use_s2(FALSE)\n\n## Now redo our earlier benchmarks...\n\n## Centroid\nmicrobenchmark(\n  sf = nc$geometry |&gt; st_centroid(),\n  geos = nc_geos |&gt; geos_centroid(), \n  times = 2\n)\n\nUnit: microseconds\n expr      min       lq      mean    median       uq      max neval cld\n   sf 2163.516 2163.516 2451.2185 2451.2185 2738.921 2738.921     2  a \n geos  106.383  106.383  124.3305  124.3305  142.278  142.278     2   b\n\n## Hemisphere aggregation\nmicrobenchmark(\n  \n  sf_tidy = nc |&gt;\n    group_by(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low')) |&gt;\n    summarise(geometry = st_union(geometry)),\n  \n  sf_dt = nc_dt[\n    ,\n    .(geometry = st_union(geometry)),\n    by = .(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low'))\n  ],\n  \n  geos_tidy = nc_tibble |&gt;  \n    group_by(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low')) |&gt;\n    summarise(geo = geos_unary_union(geos_make_collection(geo))),\n  \n  geos_dt = nc_dt[\n    ,\n    .(geo = geos_unary_union(geos_make_collection(geo))),\n    by = .(region = ifelse(CNTY_ID&lt;=1980, 'high', 'low'))\n  ],\n  \n  times = 2\n  \n)\n\nUnit: milliseconds\n      expr      min       lq     mean   median       uq      max neval cld\n   sf_tidy 17.69324 17.69324 18.12985 18.12985 18.56646 18.56646     2 a  \n     sf_dt 11.91770 11.91770 12.02338 12.02338 12.12907 12.12907     2  b \n geos_tidy 13.27676 13.27676 13.31177 13.31177 13.34677 13.34677     2  b \n   geos_dt 10.02873 10.02873 10.14927 10.14927 10.26981 10.26981     2   c"
  },
  {
    "objectID": "posts/fast-geospatial-datatable-geos/index.html#footnotes",
    "href": "posts/fast-geospatial-datatable-geos/index.html#footnotes",
    "title": "Fast geospatial tasks with data.table, geos & co.",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUse what you want, people.↩︎\nNone of the actual functionality that I show here requires the dev version of data.table. But I recommend downloading it regardless, since v1.14.3 is set to introduce a bunch of other killer features. I might write up a list of my favourites once the new version hits CRAN. In the meantime, if any DT devs are reading this, please pretty please can we include these two PRs (1, 2) into the next release too.↩︎\nYes, yes. I know you can include a (list) column of data frames within a data.table. But just bear with me for the moment.↩︎\nThe important thing is that you explicitly convert it to a tibble. Leaving it as an sf object won’t yield the same speed benefits.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Grant R. McDermott",
    "section": "",
    "text": "I am a Principal Economist at Amazon, having recently returned to the private sector after an extended period in academia.\nMy professional interests are a mix of tech and environmental topics, with an emphasis on empirical tools and computation, especially in big data settings.\nMuch of my spare time is dedicated to various open-source software (OSS) projects. I am an advocate for open and reproducible science, and view OSS as the most effective and democratic means for delivering these outcomes."
  }
]