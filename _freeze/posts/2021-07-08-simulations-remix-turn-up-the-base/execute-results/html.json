{
  "hash": "2095c9f8f6d33bdf602ec774200083a4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Simulations remix: Turn up the base'\ndescription: 'Faster still, with base R'\ndate: '2021-07-08'\nslug: simulations-remix-turn-up-the-base\ntoc: false\nmathjax: true\ncategories:\n  - R\n  - econometrics\n  - data science\n---\n\n\n\n\n\n*I apologise for the title of this post and will show myself the door shortly.*\n\nI wanted to quickly follow up on my last post about [efficient simulations in R]({{ site.url }}/efficient-simulations-in-r). If you recall, in that post we used **data.table** and some other tricks to run 40,000 regressions (i.e. 20k simulations with 2 regressions each) in just over 2 seconds. The question before us today is: **Can we go even faster using only base R?** And it turns out that the answer is, yes, we can.\n\nMy motivation for a follow-up is partially the result of [this very nice post](https://elbersb.com/public/posts/interaction_simulation/) by Benjamin Elbers, who replicates my simulation using Julia. In so doing, he demonstrates some of Julia's killer features; most notably the fact that we don't need to think about vectorisation --- e.g. when creating the data --- since Julia's compiler will take care of that for us automatically.[^1] But Ben also does another interesting thing, which is to show the speed gains that come from defining our own (super lean) regression function. He uses Cholesky decomposition and it's fairly straightforward to do the same thing in R. ([Here](https://www.theissaclee.com/post/linearqrandchol/) is a nice tutorial by Issac Lee.) \n\nI was halfway on my way to doing this myself when I stumbled on a [totally different post](https://rpubs.com/maechler/fast_lm) by R Core member, Martin Maechler. Therein he introduces the **`.lm.fit()`** function (note the leading dot), which incurs even less overhead than the `lm.fit()` function I mentioned in my last post. I'm slightly embarrassed to say I had never heard about it until now[^2], but a quick bit of testing \"confirms\" Martin's more rigorous benchmarks: `.lm.fit` yields a consistent 30-40% improvement over even `lm.fit`.\n\nNow, it would be trivial to amend my previous simulation script to slot in `.lm.fit()` and re-run the benchmarks. But I thought I'd make this a bit more interesting by redoing the whole thing using only base R. (I'll load the **parallel** package, but that comes bundled with the [base distribution](https://stackoverflow.com/a/9705725/4115816) so hardly counts as cheating.) Here's the full script with benchmarks for both sequential and parallel implementations at the bottom.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data generation ---------------------------------------------------------\n\ngen_data = function(sims=1) {\n  \n  ## Total time periods in the the panel = 500\n  tt = 500\n  \n  ## x1 covariates\n  x1_A = 1 + rnorm(tt*sims, 0, 1)\n  x1_B = 1/4 + rnorm(tt*sims, 0, 1)\n  x1 = c(x1_A, x1_B)\n  \n  ## Add second, nested x2 covariates for each country\n  x2_A = 1 + x1_A + rnorm(tt*sims, 0, 1)\n  x2_B = 1 + x1_B + rnorm(tt*sims, 0, 1)\n  x2 = c(x2_A, x2_B)\n  \n  ## Outcomes (notice different slope coefs for x2_A and x2_B)\n  y_A = x1_A + 1*x2_A + rnorm(tt*sims, 0, 1)\n  y_B = x1_B + 2*x2_B + rnorm(tt*sims, 0, 1)\n  y = c(y_A, y_B)\n  \n  ## Group variables (id and sim)\n  id = as.factor(c(rep('A', length(x1_A)), rep('B', length(x1_B))))\n  sim = rep(rep(1L:sims, each = tt), times = length(levels(id)))\n  \n  ## Demeaned covariates\n  x1_dmean = x1 - ave(x1, list(sim, id), FUN = mean)\n  x2_dmean = x2 - ave(x2, list(sim, id), FUN = mean)\n  \n  ## Bind in a matrix\n  mat = cbind('sim' = sim, \n              'id' = id,\n              'y' = y,\n              'intercept' = 1, \n              'x1' = x1, \n              'x2' = x2, \n              'x1:x2' = x1*x2, \n              'x1_dmean:x2_dmean' = x1_dmean * x2_dmean)\n  \n  ## Set order i.t.o simulations\n  mat = mat[order(mat[, 'sim']), ]\n  \n  return(mat)\n}\n\n## How many simulations do we want?\nn_sims = 2e4\n\n## Generate them all as one big matrix\nd = gen_data(n_sims)\n\n## Create index list (for efficient subsetting of the large data matrix)\n## Note that each simulation is (2*500=)1000 rows long.\nii = lapply(1L:n_sims-1, function(i) 1L:1e3L + rep(1e3L*(i), each=1e3L))\n\n# Benchmarks --------------------------------------------------------------\n\nlibrary(microbenchmark) ## For high-precision timing\nlibrary(parallel)\nn_cores = detectCores()\n\n\n## Convenience function for running the two regressions and extracting the \n## interaction coefficients of interest (saves having to retype everything).\n## The key bit is the .lm.fit() function.\nget_coefs = function(dat) {\n  level = coef(.lm.fit(dat[, c('intercept', 'x1', 'x2', 'x1:x2', 'id')], \n                       dat[, 'y']))[4]\n  dmean = coef(.lm.fit(dat[, c('intercept', 'x1', 'x2', 'x1_dmean:x2_dmean', 'id')], \n                       dat[, 'y']))[4]\n  return(cbind(level, dmean))\n}\n\n## Run the benchmarks for both sequential and parallel versions\nmicrobenchmark(\n  sims_sequential = lapply(1:n_sims, \n                           function(i) {\n                             index = ii[[i]]\n                             get_coefs(d[index, ])\n                             }),\n  sims_parallel = mclapply(1:n_sims, \n                           function(i) {\n                             index = ii[[i]]\n                             get_coefs(d[index, ])\n                             }, \n                           mc.cores = n_cores\n                           ),\n  times = 1\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: milliseconds\n            expr       min        lq      mean    median        uq       max\n sims_sequential 2484.7792 2484.7792 2484.7792 2484.7792 2484.7792 2484.7792\n   sims_parallel  949.1027  949.1027  949.1027  949.1027  949.1027  949.1027\n neval\n     1\n     1\n```\n\n\n:::\n:::\n\n\n\nThere you have it. Down to _less than a second_ for a simulation involving 40,000 regressions using only base R.[^3] On a laptop, no less. Just incredibly impressive. \n\n### Conclusion\n\nNo grand conclusion today... except a sincere note of gratitude to the R Core team (and Julia devs and so many other OSS maintainers) for providing us with such an incredible base to build from. \n\nP.S. Achim Zeileis (who else?) has another great tip for speeding up simulations where the experimental design is fixed [here](https://twitter.com/AchimZeileis/status/1413407892556947465).\n\n[^1]: In this house, we stan both R and Julia.\n\n[^2]: I think Dirk Eddelbuettel had mentioned it to me, but I hadn't grokked the difference.\n\n[^3]: The timings from this knitted Quarto document are actually about 20% slower than when I run the script directly in my R console. But we're really splitting hairs now.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}