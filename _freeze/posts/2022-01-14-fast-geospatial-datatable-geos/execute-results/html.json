{
  "hash": "5d4bf5c9ee2178a14f0a8c6ab8e0f676",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Fast geospatial tasks with data.table, geos & co.\ndescription: 'Use all your favourites'\ndate: '2022-01-14'\nslug: fast-geospatial-datatable-geos\ntoc: true\ncategories:\n  - R\n  - geospatial\n  - data science\n---\n\n\n\n\n\n:::{.callout-note}\nI've updated this post to reflect the fact that we no longer need the\ndevelopment version of **data.table**.\n:::\n\n## Motivation\n\nThis blog post pulls together various tips and suggestions that I've left\naround the place. My main goal is to show you some simple workflows that I use \nfor high-performance geospatial work in R, leaning on the **data.table**, **sf** \nand **geos** packages.\n\nIf you're the type of person who likes to load everything at once, here are the\nR libraries and theme settings that I'll be using in this post. (Don't worry if\nnot: I'll be loading them again in the relevant sections below to underscore why\nI'm calling a specific library.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Data wrangling\nlibrary(dplyr)\nlibrary(data.table)\n\n## Geospatial\nlibrary(sf)\nlibrary(geos)\n\n## Plotting\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\n## Benchmarking\nlibrary(microbenchmark)\n```\n:::\n\n\n\n## data.table + sf workflow\n\nEveryone who does spatial work in R is familiar with the wonderful **sf**\npackage. You know, this one:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\nlibrary(ggplot2); theme_set(theme_minimal())\n\n## Grab the North Carolina shapefile that comes bundled with sf\nnc_shapefile = system.file(\"shape/nc.shp\", package = \"sf\")\nnc = st_read(nc_shapefile)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `nc' from data source `/usr/lib/R/library/sf/shape/nc.shp' using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n```\n\n\n:::\n\n```{.r .cell-code}\n## Quick plot\nggplot(nc) + geom_sf()\n```\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/nc-1.png){width=2400}\n:::\n:::\n\n\n\nThe revolutionary idea of **sf** was (is) that it allowed you to treat \nspatial objects as regular data frames, so you can do things like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nnc |>\n  group_by(region = ifelse(CNTY_ID<=1980, 'high', 'low')) |>\n  summarise(geometry = st_union(geometry)) |>\n  ggplot(aes(fill=region)) + \n  geom_sf()\n```\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/dplyr_union-1.png){width=2400}\n:::\n:::\n\n\n\nIn the above code chunk, I'm using **dplyr** to do a \ngrouped aggregation on our North Carolina data object. The aggregation itself is\npretty silly---i.e. divide the state into two hemispheres---but the same idea\nextends to virtually all of **dplyr's** capabilities. It makes for a very \npotent and flexible combination that has driven an awful lot of R-based spatial\nwork in recent years.\n\nAt the same time, there's another powerful data wrangling library in R: \n**data.table**. This post is not going to rehash the (mostly pointless) debates \nabout which of **dplyr** or **data.table** is better.[^1]\nBut I think it's fair to say that the latter offers incredible \nperformance that makes it a must-use library for a lot of people, including\nmyself. Yet it seems to me that many **data.table** users aren't aware that \nyou can use it for spatial operations in exactly the same way.\n\n:::{.callout-note}\n## data.table v1.15.0+\n\nIf you're following along on your own computer, make sure that you have\n**data.table** v1.15.0 (released January 2024) or a newer version before\ncontinuing:\n:::\n\nOkay, let's create a \"data.table\" version of our `nc` object and take a \nquick look at the first few rows and some columns.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\n\nnc_dt = as.data.table(nc)\nnc_dt[1:3, c('NAME', 'CNTY_ID', 'geometry')]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        NAME CNTY_ID                       geometry\n      <char>   <num>             <sfc_MULTIPOLYGON>\n1:      Ashe    1825 MULTIPOLYGON (((-81.47276 3...\n2: Alleghany    1827 MULTIPOLYGON (((-81.23989 3...\n3:     Surry    1828 MULTIPOLYGON (((-80.45634 3...\n```\n\n\n:::\n:::\n\n\n\nAt this point, I have to briefly back up to say that the reason I wanted you to\ngrab a newer version of **data.table** is that it \"pretty prints\" the\ncolumns by default. This not only includes the columns types and keys (if you've\nset any), but also the special `sfc_MULTIPLOYGON` list columns which is where\nthe **sf** magic is hiding. It's a small cosmetic change that nonetheless\nunderscores the integration between these two packages.[^2]\n\nJust like we did with **dplyr** earlier, we can now do grouped spatial \noperations on this object using **data.table**'s concise syntax:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbb = st_bbox(nc) # https://github.com/Rdatatable/data.table/issues/6707\n\nnc_dt[\n  ,\n  .(geometry = st_union(geometry)),\n  by = .(region = ifelse(CNTY_ID<=1980, 'high', 'low'))\n] |>\n  ggplot(aes(geometry=geometry, fill=region)) + \n  geom_sf() +\n  coord_sf(xlim = bb[c(1,3)], ylim = bb[c(2,4)]) +\n  labs(caption = \"Now brought to you by data.table\")\n```\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/dt_union-1.png){width=2400}\n:::\n:::\n\n\n\nNow, I'll admit that there are a few tweaks we need to make to the plot\ncall. Unlike with the non-**data.table** workflow, this time we have to specify\nthe geometry aesthetic with `aes(geometry=geometry, ...)`. Otherwise,\n**ggplot2** won't know what do with this object. The other difference is that it\ndoesn't automatically recognise the CRS (i.e. \"NAD27\"), so the projection is a\nlittle off. Again, however, that information is contained with the `geometry`\ncolumn of our `nc_dt` object. It just requires that we provide the CRS\nto our plot call explicitly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Grab CRS from the geometry column\ncrs = st_crs(nc_dt$geometry)\n\n## Update our previous plot\nlast_plot() + \n  coord_sf(xlim = bb[c(1,3)], ylim = bb[c(2,4)], crs = crs) +\n  labs(caption = \"Now with the right projection too\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/dt_union_crs-1.png){width=2400}\n:::\n:::\n\n\n\nPlotting tweaks aside, I don't want to lose sight of the main point of this\npost, namely: **sf** and **data.table** play perfectly well together. You\ncan do (grouped) spatial operations and aggregations inside the latter, exactly\nhow you would any other data wrangling task. So if you love **data.table's**\nperformance and syntax, then by all means continue using it for your spatial\nwork too. Speaking of performance...\n\n## Speeding things up with geos\n\n_**Update (2022-02-16):** The benchmarks in this section are a bit unfair, since \n**geos** assumes planar (\"flat\") geometries, whereas **sf**  assumes spherical \n(\"curved\") geometries by default. See the\n[postscript](#postscript-planar-vs-spherical) at the bottom of this post, which \ncorrects for this discrepancy._\n\nAs great as **sf** is, even its most ardent proponents will admit that it can\ndrag a bit when it comes to big geospatial tasks. I don't want to imply that\nthat it's \"slow\". But I've found that it does lag behind **geopandas**, for \nexample, when I'm doing heavy geospatial computation or working with really \nlarge spatial files. Luckily, there's a new package in town that offers major \nperformance gains and plays very well with the workflow I demonstrated above.\n\nDewey Dunnington and Edzer Pebesma's\n[**geos**](https://paleolimbot.github.io/geos/index.html) package covers all of\nthe same basic geospatial \n[operations](https://paleolimbot.github.io/geos/reference/index.html) as **sf**. \nBut it does so by directly wrapping the underlying `GEOS` API, which is written \nin C and is thus extremely performant. Here's a simple example, where we\ncalculate the centroid of each North Carolina county.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(geos)           ## For geos operations  \nlibrary(microbenchmark) ## For benchmarking\n\n## Create a geos geometry object\nnc_geos = nc |> as_geos_geometry()\n\n## Benchmark\nmicrobenchmark(\n  sf = nc$geometry |> st_centroid(),\n  geos = nc_geos |> geos_centroid(), \n  times = 2\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: microseconds\n expr      min       lq     mean   median       uq      max neval cld\n   sf 7260.992 7260.992 7674.841 7674.841 8088.690 8088.690     2  a \n geos  149.806  149.806  194.139  194.139  238.472  238.472     2   b\n```\n\n\n:::\n:::\n\n\n\nA couple of things worth noting. First, the **geos** centroid calculation\ncompletes orders of magnitude faster than the **sf** equivalent. Second, the\nexecuting functions are very similar (`st_centroid()` vs `geos_centroid()`).\nThird, we have to do an explicit `as_geos_geometry()` coercion before we can\nperform any **geos** operations on the resulting object.\n\nThat last point seems the most mundane. (_Why aren't you talking more about how\ncrazy fast **geos** is?!_) But it's important since it underscores a key \ndifference between the two packages and why the developers view them as \ncomplements. Unlike **sf**, which treats spatial objects as data frames, \n**geos** only preserves the geometry attributes. Take a look:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(nc_geos)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<geos_geometry[6] with CRS=NAD27>\n[1] <MULTIPOLYGON [-81.741 36.234...-81.240 36.590]>\n[2] <MULTIPOLYGON [-81.348 36.365...-80.903 36.573]>\n[3] <MULTIPOLYGON [-80.966 36.234...-80.435 36.565]>\n[4] <MULTIPOLYGON [-76.330 36.073...-75.773 36.557]>\n[5] <MULTIPOLYGON [-77.901 36.163...-77.075 36.556]>\n[6] <MULTIPOLYGON [-77.218 36.230...-76.707 36.556]>\n```\n\n\n:::\n:::\n\n\n\nGone are all those extra columns containing information about county names,\nFIPS codes, population numbers, etc. etc. We're just left with the necessary \ninformation to do high-performance spatial operations.\n\n### Quick aside on plotting geos objects\n\nBecause we've dropped all of the **sf** / data frame attributes, we can't use \n**ggplot2** to plot anymore. But we can use the base R plotting method:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(nc_geos, col = \"gray90\")\nplot(geos_centroid(nc_geos), pch = 21, col = 'red', bg = 'red', add = TRUE)\n```\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/nc_geos_baseplot-1.png){width=2400}\n:::\n:::\n\n\n\nActually, that's not quite true, since an alternative is to convert it back into an\n**sf** object with `st_as_sf()` and then call **ggplot2**. This is particularly\nuseful because you can hand off some heavy calculation to **geos** before\nbringing it back to **sf** for any additional functionality. Again, the \ndevelopers of these packages designed them to act as complements.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_sf(data = nc) +\n  geom_sf(data = nc_geos |> geos_centroid() |> st_as_sf(), \n          col = \"red\")\n```\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/nc_geos_ggplot-1.png){width=2400}\n:::\n:::\n\n\n\nOkay, back to the main post...\n\n### data.table + geos workflow\n\nFinally, we get to the _pièce de résistance_ of today's post. The fact that \n`as_geos_geometry()` creates a GEOS geometry object---rather than\npreserving all of the data frame attributes---is a good thing for our\n**data.table** workflow. Why? Well, because we can just include this\ngeometry object as a list column inside our data.table.[^3] \nIn turn, this means you can treat spatial operations as you would any other \noperation inside a data.table. You can aggregate by group, merge, compare, \nand generally **combine the power of data.table and geos** as you see fit.\n\n(The same is true for regular data frames and tibbles, but we'll get to that.)\n\nLet's prove that this idea works by creating a GEOS column in our data.table. \nI'll creatively call this column `geo`, but really you could call it anything\nyou want (including overwriting the existing `geometry` column).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_dt[, geo := as_geos_geometry(geometry)]\nnc_dt[1:3, c('NAME', 'CNTY_ID', 'geo')] ## Print a few rows/columns\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        NAME CNTY_ID                                              geo\n      <char>   <num>                                  <geos_geometry>\n1:      Ashe    1825 <MULTIPOLYGON [-81.741 36.234...-81.240 36.590]>\n2: Alleghany    1827 <MULTIPOLYGON [-81.348 36.365...-80.903 36.573]>\n3:     Surry    1828 <MULTIPOLYGON [-80.966 36.234...-80.435 36.565]>\n```\n\n\n:::\n:::\n\n\n\nGEOS column in hand, we can manipulate or plot it directly from within the \ndata.table. For example, we can recreate our previous centroid plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(nc_dt[, geo], col = \"gray90\")\nplot(nc_dt[, geos_centroid(geo)], pch = 21, col = 'red', bg = 'red', add = TRUE)\n```\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/dt_geos_centroid-1.png){width=2400}\n:::\n:::\n\n\n\nAnd here's how we could replicate our earlier \"hemisphere\" plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_dt[\n  ,\n  .(geo = geo |> geos_make_collection() |> geos_unary_union()),\n  by = .(region = ifelse(CNTY_ID<=1980, 'high', 'low'))\n][, geo] |>\n  plot()\n```\n\n::: {.cell-output-display}\n![](2022-01-14-fast-geospatial-datatable-geos_files/figure-html/geos_union-1.png){width=2400}\n:::\n:::\n\n\n\nThis time around the translation from the equivalent **sf** code isn't as \ndirect. We have one step (`st_union()`) vs. two \n(`geos_make_collection() |> geos_unary_union()`). The second `geo_unary_union()`\nstep is clear enough. But it's the first `geos_make_collection()`step that's\nkey for our aggregating task. We have to tell **geos** to treat everything\nwithin the same group (i.e. whatever is in `by = ...`) as a collective. This \nextra step becomes very natural after you've done it a few times and is a small\nprice to pay for the resulting performance boost.\n\nSpeaking of which, it's nearly time for some final benchmarks. The only extra\nthing I want to do first is, as promised, include a **tibble**/**dplyr**\nequivalent. The exact same concepts and benefits carry over here, for those of\nyou that prefer the tidyverse syntax and \nworkflow.[^4]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnc_tibble = tibble::as_tibble(nc) |> \n  mutate(geo = as_geos_geometry(geometry))\n```\n:::\n\n\n\n### Benchmarks\n\nFor this final set of benchmarks, I'm going to horserace the same grouped\naggregation that we've been using throughout.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark(\n  \n  sf_tidy = nc |>\n    group_by(region = ifelse(CNTY_ID<=1980, 'high', 'low')) |>\n    summarise(geometry = st_union(geometry)),\n  \n  sf_dt = nc_dt[\n    ,\n    .(geometry = st_union(geometry)),\n    by = .(region = ifelse(CNTY_ID<=1980, 'high', 'low'))\n  ],\n  \n  geos_tidy = nc_tibble |>  \n    group_by(region = ifelse(CNTY_ID<=1980, 'high', 'low')) |>\n    summarise(geo = geos_unary_union(geos_make_collection(geo))),\n  \n  geos_dt = nc_dt[\n    ,\n    .(geo = geos_unary_union(geos_make_collection(geo))),\n    by = .(region = ifelse(CNTY_ID<=1980, 'high', 'low'))\n  ],\n  \n  times = 2\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: milliseconds\n      expr      min       lq     mean   median       uq      max neval  cld\n   sf_tidy 63.96700 63.96700 64.97700 64.97700 65.98700 65.98700     2 a   \n     sf_dt 58.31117 58.31117 58.39415 58.39415 58.47713 58.47713     2  b  \n geos_tidy 14.03589 14.03589 14.54002 14.54002 15.04415 15.04415     2   c \n   geos_dt 10.98849 10.98849 11.08022 11.08022 11.17195 11.17195     2    d\n```\n\n\n:::\n:::\n\n\n\n**Result:** A 6x speed-up. Nice! While the toy dataset that we're using here is \ntoo small to make a meaningful difference in practice, those same \nperformance benefits will carry over to big geospatial tasks too. Being able to \nreduce your computation time by a factor of 10 really makes a difference once \nyou're talking minutes or hours.\n\n## Conclusion\n\nMy takeaways:\n\n- It's fine to treat **sf** objects as **data.tables** (or vice versa) if \nthat's your preferred workflow. Just remember to specify the geometry column.\n\n- For large (or small!) geospatial tasks, give the **geos** package a go. \nIt integrates very well with both **data.table** and the **tidyverse**, and the \nhigh-performance benefits carry over to both ecosystems.\n\nBy the way, there are more exciting high-performance geospatial developments on \nthe way in R (as well as other languages) like\n[**geoarrow**](https://github.com/paleolimbot/geoarrow). We're lucky to have \nthese tools at our disposal.\n\n### Postscript: planar vs spherical \n\n_Note: This section was added on 2021-01-16._\n\nAs Roger Bivand \n[points out](https://twitter.com/RogerBivand/status/1482691924561698817) on \nTwitter, I'm not truly comparing apples with apples in the above benchmarks. \n**geos** assumes planar (\"flat\") geometries, whereas **sf** does the more \ncomplicated task of calculating spherical (\"curved\") geometries. More on that\n[here](https://r-spatial.github.io/sf/articles/sf7.html) if you are interested.\nBelow I repeat these same benchmarks, but with **sf** switched to the same\nplanar backend. The upshot is that **geos** is still faster, but the gap narrows\nconsiderably. A reminder that we're also dealing with a very small dataset, so I\nrecommend benchmarking on your own datasets to avoid the influence of misleading\noverhead. But I stand by my comment that these differences persist at scale, \nbased on my own experiences and testing.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Turn off sf's spherical (\"S2\") backend\nsf_use_s2(FALSE)\n\n## Now redo our earlier benchmarks...\n\n## Centroid\nmicrobenchmark(\n  sf = nc$geometry |> st_centroid(),\n  geos = nc_geos |> geos_centroid(), \n  times = 2\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: microseconds\n expr      min       lq      mean    median       uq      max neval cld\n   sf 2290.378 2290.378 2631.5020 2631.5020 2972.626 2972.626     2  a \n geos  126.717  126.717  185.2345  185.2345  243.752  243.752     2   b\n```\n\n\n:::\n\n```{.r .cell-code}\n## Hemisphere aggregation\nmicrobenchmark(\n  \n  sf_tidy = nc |>\n    group_by(region = ifelse(CNTY_ID<=1980, 'high', 'low')) |>\n    summarise(geometry = st_union(geometry)),\n  \n  sf_dt = nc_dt[\n    ,\n    .(geometry = st_union(geometry)),\n    by = .(region = ifelse(CNTY_ID<=1980, 'high', 'low'))\n  ],\n  \n  geos_tidy = nc_tibble |>  \n    group_by(region = ifelse(CNTY_ID<=1980, 'high', 'low')) |>\n    summarise(geo = geos_unary_union(geos_make_collection(geo))),\n  \n  geos_dt = nc_dt[\n    ,\n    .(geo = geos_unary_union(geos_make_collection(geo))),\n    by = .(region = ifelse(CNTY_ID<=1980, 'high', 'low'))\n  ],\n  \n  times = 2\n  \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnit: milliseconds\n      expr      min       lq     mean   median       uq      max neval  cld\n   sf_tidy 19.61392 19.61392 19.70825 19.70825 19.80258 19.80258     2 a   \n     sf_dt 13.72748 13.72748 13.88728 13.88728 14.04707 14.04707     2  b  \n geos_tidy 14.98676 14.98676 15.19424 15.19424 15.40171 15.40171     2   c \n   geos_dt 11.03429 11.03429 11.14203 11.14203 11.24977 11.24977     2    d\n```\n\n\n:::\n:::\n\n\n\n[^1]: Use what you want, people.\n\n[^2]: None of the actual _functionality_ that I show here requires the dev version of **data.table**. But I recommend downloading it regardless, since v1.14.3 is set to introduce a bunch of other [killer features](https://rdatatable.gitlab.io/data.table/news/index.html#unreleased-data-table-v1-14-3-in-development-). I might write up a list of my favourites once the new version hits CRAN. In the meantime, if any DT devs are reading this, _please pretty please_ can we include these two PRs ([1](https://github.com/Rdatatable/data.table/pull/4163), [2](https://github.com/Rdatatable/data.table/pull/4883)) into the next release too.\n\n[^3]: Yes, yes. I know you can include a (list) column of data frames within a data.table. But just bear with me for the moment.\n\n[^4]: The important thing is that you _explicitly_ convert it to a tibble. Leaving it as an **sf** object won't yield the same speed benefits.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}