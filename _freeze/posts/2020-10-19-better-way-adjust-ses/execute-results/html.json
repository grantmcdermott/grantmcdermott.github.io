{
  "hash": "6543346301af144a4ef0b367a2400d0a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: A better way to adjust your standard errors\ndescription: On the fly, for the win.\ndate: '2020-10-19'\nslug: better-way-adjust-SEs\ntoc: true\ncategories: [R, Stata, econometrics]\n---\n\n\n\n\n\n## Motivation\n\nConsider the following scenario:\n\n*A researcher has to adjust the standard errors (SEs) for a regression model that she has already run. Maybe this is to appease a journal referee. Or, maybe it's because she is busy iterating through the early stages of a project. She's still getting to grips with her data and wants to understand how sensitive her results are to different modeling assumptions.*\n\nDoes that sound familiar? I believe it should, because something like that has happened to me on every single one of my empirical projects. I end up estimating multiple versions of the *same* underlying regression model &mdash; even putting them side-by-side in a regression table, where the only difference across columns is slight tweaks to the way that the SEs were calculated.\n\nConfronted by this task, I'm willing to bet that most people do the following:\n\n- Run their model under one SE specification (e.g. iid). \n- Re-run their model a second time under another SE specification (e.g. HC robust).\n- Re-run their model a third time under yet another SE specification (e.g. clustered).\n- Etc.\n\nWhile this is fine as far as it goes, I'm here to tell you that there's a better way. Rather than re-running your model multiple times, I'm going to advocate that you run your model only **once** and then adjust SEs on the backend as needed. This approach --- what I'll call \"on-the-fly\" SE adjustment --- is not only safer, it's much faster too.\n\nLet's see some examples.\n\n## Example 1: **sandwich**\n\nTo the best of my knowledge, on-the-fly SE adjustment was introduced to R by the [**sandwich**](http://sandwich.r-forge.r-project.org/) package ([@Achim Zeilles](https://twitter.com/AchimZeileis) et al.) This package has been around for well over a decade and is incredibly versatile, providing an object-orientated framework for recomputing variance-covariance (VCOV) matrix estimators &mdash; and thus SEs &mdash; for a wide array of model objects and classes. At the same time, **sandwich** just recently got its [own website](http://sandwich.r-forge.r-project.org/) to coincide with some cool new features. So it's worth exploring what that means for a modern empirical workflow. In the code that follows, I'm going to borrow liberally from the [introductory vignette](http://sandwich.r-forge.r-project.org/articles/sandwich.html#illustrations-1). But I'll also tack on some additional tips and tricks that I use in my own workflow. (**UPDATE (2020-08-23):** The vignette has now been [updated](http://sandwich.r-forge.r-project.org/news/#sandwich-3-0-1-unreleased) to include some of the suggestions from this post. Thanks Achim!)\n\nLet's start by running a simple linear regression on some sample data; namely, the \"PetersenCL\" dataset that comes bundled with the package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sandwich)\ndata('PetersenCL')\n\nm = lm(y ~ x, data = PetersenCL)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = PetersenCL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7611 -1.3680 -0.0166  1.3387  8.6779 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.02968    0.02836   1.047    0.295    \nx            1.03483    0.02858  36.204   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.005 on 4998 degrees of freedom\nMultiple R-squared:  0.2078,\tAdjusted R-squared:  0.2076 \nF-statistic:  1311 on 1 and 4998 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nOur simple model above assumes that the errors are [iid](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables). But we can adjust these SEs by calling one of the many alternate VCOV estimators provided by **sandwich**. For example, to get a robust, or heteroscedasticity-consistent (\"HC3\"), VCOV matrix we'd use:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvcovHC(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              (Intercept)             x\n(Intercept)  8.046458e-04 -1.155095e-05\nx           -1.155095e-05  8.072475e-04\n```\n\n\n:::\n:::\n\n\n\nTo actually substitute the robust VCOV into our original model --- so that we can print it in a nice regression table and perform statistical inference --- we pair **sandwich** with its companion package, **lmtest**. The workhorse function here is `lmtest::coeftest` and, as we can see, this yields an object that is similar to a standard model summary in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest)\n\ncoeftest(m, vcov = vcovHC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.029680   0.028366  1.0463   0.2955    \nx           1.034833   0.028412 36.4223   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\nTo recap: We ran our base `m` model just the once and then adjusted for robust SEs on the backend using **sandwich**/**coeftest**. \n\nNow, I'll admit that the benefits of this workflow aren't super clear from my simple example yet. Though, we did cut down on copying-and-pasting of duplicate code and this automatically helps to minimize user error. (Remember: [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)!) But we can easily scale things up to get a better sense of its power. For instance, we could imagine calling a whole host of alternate VCOVs to our base model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Calculate the VCOV (SEs) under a range of different assumptions\nvc = list(\n  \"Standard\"              = vcov(m),\n  \"Sandwich (basic)\"      = sandwich(m),\n  \"Clustered\"             = vcovCL(m, cluster = ~ firm),\n  \"Clustered (two-way)\"   = vcovCL(m, cluster = ~ firm + year),\n  \"HC3\"                   = vcovHC(m),\n  \"Andrews' kernel HAC\"   = kernHAC(m),\n  \"Newey-West\"            = NeweyWest(m),\n  \"Bootstrap\"             = vcovBS(m),\n  \"Bootstrap (clustered)\" = vcovBS(m, cluster = ~ firm)\n  )\n```\n:::\n\n\n\nYou could, of course, print the `vc` list to screen now if you so wanted. But I want to go one small step further by showing you how easy it is to create a regression table that encapsulates all of these different models. In the next code chunk, I'm going to create a list of models by passing `vc` to an `lapply()` call.[^1] I'm then going to generate a regression table using `msummary()` from the excellent [**modelsummary**](https://vincentarelbundock.github.io/modelsummary) package ([@Vincent Arel-Bundock](https://twitter.com/VincentAB)).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(modelsummary) ## For great-looking regression tables (among other things)\n\n## Adjust our model SEs on-the-fly\nlm_mods = lapply(vc, function(x) coeftest(m, vcov = x))\n\n## Print the regression table\nmsummary(lm_mods)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_joawtif4c38dtsa3lo47(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_joawtif4c38dtsa3lo47\");\n          var cell = table.rows[i]?.cells[j];  // Safe navigation to avoid errors\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_joawtif4c38dtsa3lo47');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_joawtif4c38dtsa3lo47(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_joawtif4c38dtsa3lo47\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: 0, j: 1 }, { i: 0, j: 6 }, { i: 0, j: 5 }, { i: 0, j: 3 }, { i: 0, j: 8 }, { i: 0, j: 7 }, { i: 0, j: 4 }, { i: 0, j: 2 }, { i: 0, j: 9 },  ], css_id: 'tinytable_css_whn77u55oi27sv3t3soo',}, \n          { positions: [ { i: 7, j: 0 },  ], css_id: 'tinytable_css_vnl0mlhvvgmqi9o7hotz',}, \n          { positions: [ { i: 0, j: 0 },  ], css_id: 'tinytable_css_r9evyubkzzu0jg8l17ty',}, \n          { positions: [ { i: 4, j: 0 },  ], css_id: 'tinytable_css_piuf8w4rf74t2b4vxr1c',}, \n          { positions: [ { i: 2, j: 1 }, { i: 5, j: 1 }, { i: 2, j: 2 }, { i: 1, j: 5 }, { i: 5, j: 2 }, { i: 1, j: 1 }, { i: 1, j: 6 }, { i: 3, j: 1 }, { i: 1, j: 3 }, { i: 2, j: 3 }, { i: 6, j: 1 }, { i: 6, j: 6 }, { i: 5, j: 3 }, { i: 1, j: 2 }, { i: 1, j: 7 }, { i: 3, j: 2 }, { i: 1, j: 4 }, { i: 2, j: 4 }, { i: 6, j: 2 }, { i: 6, j: 7 }, { i: 5, j: 4 }, { i: 6, j: 4 }, { i: 1, j: 8 }, { i: 3, j: 3 }, { i: 3, j: 8 }, { i: 2, j: 5 }, { i: 6, j: 3 }, { i: 6, j: 8 }, { i: 5, j: 5 }, { i: 6, j: 5 }, { i: 1, j: 9 }, { i: 3, j: 4 }, { i: 3, j: 9 }, { i: 2, j: 6 }, { i: 3, j: 6 }, { i: 6, j: 9 }, { i: 5, j: 6 }, { i: 2, j: 7 }, { i: 3, j: 7 }, { i: 3, j: 5 }, { i: 5, j: 7 }, { i: 2, j: 8 }, { i: 5, j: 9 }, { i: 5, j: 8 }, { i: 2, j: 9 },  ], css_id: 'tinytable_css_p4p4e4ph4oub3ay1mb3g',}, \n          { positions: [ { i: 4, j: 8 }, { i: 4, j: 9 }, { i: 4, j: 7 }, { i: 4, j: 5 }, { i: 4, j: 3 }, { i: 4, j: 2 }, { i: 4, j: 6 }, { i: 4, j: 4 }, { i: 4, j: 1 },  ], css_id: 'tinytable_css_ovjj2hyzx03ckt0qrcie',}, \n          { positions: [ { i: 1, j: 0 }, { i: 2, j: 0 }, { i: 3, j: 0 }, { i: 5, j: 0 }, { i: 6, j: 0 },  ], css_id: 'tinytable_css_mi5rtx8qr68eohods7rt',}, \n          { positions: [ { i: 7, j: 1 }, { i: 7, j: 8 }, { i: 7, j: 6 }, { i: 7, j: 3 }, { i: 7, j: 2 }, { i: 7, j: 7 }, { i: 7, j: 5 }, { i: 7, j: 4 }, { i: 7, j: 9 },  ], css_id: 'tinytable_css_1eb9sfjooe5r1g76yc50',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_joawtif4c38dtsa3lo47(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_whn77u55oi27sv3t3soo, .table th.tinytable_css_whn77u55oi27sv3t3soo { text-align: center; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_vnl0mlhvvgmqi9o7hotz, .table th.tinytable_css_vnl0mlhvvgmqi9o7hotz { text-align: left; border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_r9evyubkzzu0jg8l17ty, .table th.tinytable_css_r9evyubkzzu0jg8l17ty { text-align: left; border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n      .table td.tinytable_css_piuf8w4rf74t2b4vxr1c, .table th.tinytable_css_piuf8w4rf74t2b4vxr1c { text-align: left; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_p4p4e4ph4oub3ay1mb3g, .table th.tinytable_css_p4p4e4ph4oub3ay1mb3g { text-align: center; }\n      .table td.tinytable_css_ovjj2hyzx03ckt0qrcie, .table th.tinytable_css_ovjj2hyzx03ckt0qrcie { text-align: center; border-bottom: solid black 0.05em; }\n      .table td.tinytable_css_mi5rtx8qr68eohods7rt, .table th.tinytable_css_mi5rtx8qr68eohods7rt { text-align: left; }\n      .table td.tinytable_css_1eb9sfjooe5r1g76yc50, .table th.tinytable_css_1eb9sfjooe5r1g76yc50 { text-align: center; border-bottom: solid #d3d8dc 0.1em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_joawtif4c38dtsa3lo47\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\"> </th>\n                <th scope=\"col\">Standard</th>\n                <th scope=\"col\">Sandwich (basic)</th>\n                <th scope=\"col\">Clustered</th>\n                <th scope=\"col\">Clustered (two-way)</th>\n                <th scope=\"col\">HC3</th>\n                <th scope=\"col\">Andrews' kernel HAC</th>\n                <th scope=\"col\">Newey-West</th>\n                <th scope=\"col\">Bootstrap</th>\n                <th scope=\"col\">Bootstrap (clustered)</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td>(Intercept)</td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                  <td>0.030  </td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>(0.028)</td>\n                  <td>(0.028)</td>\n                  <td>(0.067)</td>\n                  <td>(0.065)</td>\n                  <td>(0.028)</td>\n                  <td>(0.044)</td>\n                  <td>(0.066)</td>\n                  <td>(0.029)</td>\n                  <td>(0.065)</td>\n                </tr>\n                <tr>\n                  <td>x          </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                  <td>1.035  </td>\n                </tr>\n                <tr>\n                  <td>           </td>\n                  <td>(0.029)</td>\n                  <td>(0.028)</td>\n                  <td>(0.051)</td>\n                  <td>(0.054)</td>\n                  <td>(0.028)</td>\n                  <td>(0.035)</td>\n                  <td>(0.048)</td>\n                  <td>(0.027)</td>\n                  <td>(0.048)</td>\n                </tr>\n                <tr>\n                  <td>Num.Obs.   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                  <td>5000   </td>\n                </tr>\n                <tr>\n                  <td>AIC        </td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                  <td>31141.2</td>\n                </tr>\n                <tr>\n                  <td>BIC        </td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                  <td>63714.1</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\nIf you're the type of person  --- like me --- that prefers visual representation, then printing a coefficient plot is equally easy with `modelsummary::modelplot()`. This creates a **ggplot2** object that can be further manipulated as needed. In the code chunk below, I'll demonstrate this fairly simply by flipping the plot orientation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nmodelplot(lm_mods, coef_omit = 'Interc') +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](2020-10-19-better-way-adjust-ses_files/figure-html/sandwich_coefplot-1.png){width=2400}\n:::\n:::\n\n\n\nAnd there you have it: An intuitive and helpful comparison across a host of specifications, even though we only \"ran\" the underlying model once. Simple!\n\n:::{.callout-note}\n**UPDATE (June 2021):** You can now automate all of the steps that I show above in newer versions of **modelsummary**, thanks to the incredibly flexible `vcov` argument. See the documentation [here](https://modelsummary.com/articles/modelsummary.html#vcov-robust-ses).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmsummary(m, vcov = vc)\nmodelplot(m, vcov = vc, coef_omit = 'Interc') +\n    coord_flip()\n# etc.\n```\n:::\n\n\n:::\n\n## Example 2: fixest\n\nWhile **sandwich** covers a wide range of model classes in R, it's important to know that a number of libraries provide their own specialised methods for on-the-fly SE adjustment. The one that I want to show you for this second example is the [**fixest**](https://github.com/lrberge/fixest) package ([@Laurent Bergé](https://twitter.com/lrberge)). \n\nIf you follow me on Twitter or have read my lecture notes, you already know that I am a huge fan of this package. It's very elegantly designed and provides an insanely fast way to estimate high-dimensional fixed effects models. More importantly for today's post, **fixest** offers automatic support for on-the-fly SE adjustment. We only need to run our model once and can then adjust the SEs on the backend via a call to `summary(..., vcov = 'vcov_type')`.[^2]\n\nTo demonstrate, I'm going to run some regressions on a subsample of\nthe well-known RITA air traffic data. I've already downloaded the dataset from [Revolution Analytics](https://packages.revolutionanalytics.com/datasets/) and prepped it for the narrow purposes of this blog post. (See the [data appendix](#data) below for code.) All told we're looking at 9 variables extending over approximately 1.8 million rows. So, not \"big\" data by any stretch of the imagination, but my regressions should take at least a few seconds to run on most computers.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'data.table'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:zoo':\n\n    yearmon, yearqtr\n```\n\n\n:::\n\n```{.r .cell-code}\nair = fread('~/air.csv')\nair\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          year month day_of_week tail_num origin_airport_id dest_airport_id\n         <int> <int>       <int>   <char>             <int>           <int>\n      1:  2012     1           3   N320AA             12478           12892\n      2:  2012     1           5   N327AA             12478           12892\n      3:  2012     1           4   N329AA             12892           12478\n      4:  2012     1           5   N336AA             12478           12892\n      5:  2012     1           7   N323AA             12478           12892\n     ---                                                                   \n1844063:  2011     1           1   N904FJ             14107           15376\n1844064:  2011     1           1   N7305V             14262           14107\n1844065:  2011     1           1   N922FJ             14683           11057\n1844066:  2011     1           1   N935LR             14698           14107\n1844067:  2011     1           1   N932LR             15376           14107\n         arr_delay dep_delay dep_tod\n             <int>     <int>  <char>\n      1:       -34         4     am2\n      2:         2        -2     am2\n      3:        -6        -5     am2\n      4:       -32        -9     am2\n      5:        -7        -6     am2\n     ---                            \n1844063:        -7        -1     pm2\n1844064:        -3         0     pm1\n1844065:       -10        -4     pm2\n1844066:        11         0     pm1\n1844067:        30        -5     am2\n```\n\n\n:::\n:::\n\n\n\nThe actual regression that I'm going to run on these data is somewhat uninspired: Namely, how does arrival delay depend on departure delay, conditional on the time of day?[^3] I'll throw in a bunch of fixed effects to make the computation a bit more interesting/intensive, but it's fairly standard stuff. Note that I am running a linear fixed effect model by calling `fixest::feols()`.\n\nBut, really, I don't want you to get sidetrack by the regression details. The main thing I want to focus your attention on is the fact that I'm only going run the base model *once*, i.e. for `mod1`. Then, I'm going to adjust the SE for two more models, `mod2` and `mod3`, on the fly via respective `summary()` calls.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fixest)\n\n## Start timer; we'll use for benchmarking later\npt = proc.time()\n\n## Run the model once and once only!\n## By default, fixest::feols will cluster the SEs by the first FE (here: month)\nmod1 = feols(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id, \n             data = air)\n\n## Adjust SEs on the fly: two-way cluster\nmod2 = summary(mod1, vcov = ~month + origin_airport_id)\n\n## Adjust SEs on the fly: three-way cluster. Note that we can even include\n## cluster vars (e.g. tail_num) that weren't present in the original regression\nmod3 = summary(mod1, vcov = ~month + origin_airport_id + tail_num)\n\n## Stop timer and save results\ntime_feols = (proc.time() - pt)[3]\n```\n:::\n\n\n\nBefore I get to benchmarking, how about a quick coefficient plot? I'll use `modelsummary::modelplot()` again, focusing only on the key \"time of day × departure delay\" interaction terms.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeols_mods = list(mod1, mod2, mod3)\n\nmodelplot(\n  feols_mods, \n  coef_map = c('dep_todam1:dep_delay' = 'Midnight × Dep. delay',\n               'dep_todam2:dep_delay' = 'Morning × Dep. delay',\n               'dep_todpm1:dep_delay' = 'Afternoon × Dep. delay',\n               'dep_todpm2:dep_delay' = 'Evening × Dep. delay')\n  ) +\n  labs(caption = 'Dependent variable: Arrival delay')\n```\n\n::: {.cell-output-display}\n![](2020-10-19-better-way-adjust-ses_files/figure-html/fixest_coefplot-1.png){width=2400}\n:::\n:::\n\n\n\n### Benchmarking\n\nGreat, it worked. But did it save time? To answer this question I've benchmarked against three other methods:\n\n- `feols()`, again from the **fixest** package, but this time with each of the three models run separately.\n- `felm()` from the **lfe** package.\n- `reghdfe` from the **reghdfe** package (Stata).\n\nYou can find the benchmarking code for these other methods in the [appendix](#benchmark-other). (Please let me know if you spot any errors.) In the interests of brevity, here are the results.\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2020-10-19-better-way-adjust-ses_files/figure-html/benchmark-1.png){width=2400}\n:::\n:::\n\n\n\nThere are several takeaways from this exercise. For example, `fixest::feols()` is the fastest method even if you are (inefficiently) re-running the models separately. But --- yet again and once more, dear friends --- the key thing that I want to emphasise is the *additional* time savings brought on by adjusting the SEs on the fly. Indeed, we can see that the on-the-fly `feols` approach only takes a third of the time (approximately) that it does to run the models separately. This means that **fixest** is recomputing the SEs for models 2 and 3 pretty much instantaneously. \n\nTo add one last thing about benchmarking, the absolute difference in model run times was not *that* huge for this particular exercise. There's maybe two minutes separating the fastest and slowest methods. (Then again, not trivial either...) But if you are like me and find yourself estimating models where each run takes many minutes or hours, or even days and weeks, then the time savings are literally exponential.\n\n## Conclusion\n\nThere comes a point in almost every empirical project where you have to estimate multiple versions of the same model. Which is to say, the only difference between these multiple versions is how the standard errors were calculated: robust, clustered, etc. Maybe you're trying to satisfy a referee request before publication. Or, maybe you're trying to understand how sensitive your results are to different modeling assumptions.\n\nThe goal of this blog post has been to show you that there is often a better approach than manually re-running multiple iterations of your model. Instead, I advocate that you run the model *once* and then adjust your standard errors on the backend, as needed. This \"on-the-fly\" approach will save you a ton of time if you are working with big datasets. Even if you aren't working with big data, you will minimize copying and pasting of duplicate code. All of which will help to make your code more readable and cut down on potential errors. \n\nWhat's not to like?\n\n**P.S.** There are a couple of other R libraries with support for on-the-fly SE adjustment, e.g. [clubSandwich](https://cran.r-project.org/web/packages/clubSandwich/index.html). Since I've used it as a counterfoil in the benchmark, I should add that `lfe::felm()` provides its own method for swapping out different SEs post-estimation; see [here](https://broom.tidymodels.org/reference/tidy.felm.html#examples). Similarly, I've focused on R because it's the software language that I use most often and --- as far as I am aware --- is the only one to provide methods for on-the-fly SE adjustment across a wide range of models. If anyone knows of equivalent methods or canned routines in other languages, please let me know in the comments.\n\n**P.P.S.** Look, I'm not saying it's necessarily \"wrong\" to specify your SEs in the model call. Particularly if you've already settled on a single VCOV to use. Then, by all means, use the convenience of  Stata's `, robust` syntax or the R equivalent `lm_robust()` (via the [**estimatr**](https://declaredesign.org/r/estimatr) package).\n\n[^1]: You can substitute with a regular *for* loop or `purrr::map()` if you prefer.\n\n[^2]: See the [package documentation](https://lrberge.github.io/fixest/articles/fixest_walkthrough.html#the-vcov-argument) for details about valid `vcov` arguments (e.g., \"iid\", \"hc1\", \"twoway\", \"threeway\", etc.)\n\n[^3]: Note that I'm going to use a `dep_tod / dep_delay` expansion on the RHS to get the [full marginal effect]({{ site.baseurl }}{% post_url 2019-12-16-interaction-effects %}) of the interaction terms. Don't worry too much about this if you haven't seen it before (click on the previous link if you want to learn more).\n\n## Appendices\n\n### Flight data download and prep {#data}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!file.exists(path.expand('~/air.csv'))) {\n  \n  ## Download and extract the data\n  URL = 'https://packages.revolutionanalytics.com/datasets/AirlineSubsetCsv.tar.gz'\n  dest_file = path.expand('~/AirlineSubsetCsv.tar.gz')\n  download.file(URL, dest_file, mode = \"wb\")\n  untar(dest_file, exdir = path.expand('~'))\n  \n  ## Bind together and do some data cleaning\n  library(data.table)\n  csvs = list.files(path.expand('~/AirlineSubsetCsv/'), full.names = TRUE)\n  air = rbindlist(lapply(csvs, fread))\n  names(air) = tolower(names(air))\n  int_vars = c('arr_delay', 'dep_delay', 'dep_time', 'arr_time')\n  air[, (int_vars) := lapply(.SD, as.integer), .SDcols = int_vars]\n  \n  ## Create a departure 'time of day' factor variable, dividing the day in four\n  ## quarters\n  air[, dep_tod := fcase(dep_time <= 600, 'am1',\n                         dep_time <= 1200, 'am2',\n                         dep_time <= 1800, 'pm1',\n                         dep_time > 1800, 'pm2')]\n  \n  ## Subset\n  air = air[!is.na(arr_delay), \n            .(year, month, day_of_week, tail_num, origin_airport_id, \n              dest_airport_id, arr_delay, dep_delay, dep_tod)]\n  \n  ## Write to disk\n  fwrite(air, path.expand('~/air.csv'))\n  \n  ## Clean up\n  file.remove(c(dest_file, csvs))\n  file.remove(path.expand('~/AirlineSubsetCsv/')) ## empty dir too\n}\n```\n:::\n\n\n\n### Benchmarking code for other methods {#benchmark-other}\n\n#### fixest (separate models)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(fixest) ## Already loaded\n\npt = proc.time()\nmod1a = feols(arr_delay ~ dep_tod / dep_delay | \n                month + year + day_of_week + origin_airport_id + dest_airport_id,    \n              data = air)\nmod2a = summary(\n  feols(arr_delay ~ dep_tod / dep_delay | \n          month + year + day_of_week + origin_airport_id + dest_airport_id,          \n        data = air), \n  cluster = c('month', 'origin_airport_id')\n  )\nmod3a = summary(\n  feols(arr_delay ~ dep_tod / dep_delay | \n          month + year + day_of_week + origin_airport_id + dest_airport_id,          \n        data = air), \n  cluster = c('month', 'origin_airport_id', 'tail_num')\n  )\ntime_feols_sep = (proc.time() - pt)[3]\n```\n:::\n\n\n\n#### lfe\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lfe)\n\npt = proc.time()\nest1 = felm(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id |\n             0 |\n             month, \n            data = air)\nest2 = felm(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id |\n             0 |\n             month + origin_airport_id, \n            data = air)\nest3 = felm(arr_delay ~ dep_tod / dep_delay | \n               month + year + day_of_week + origin_airport_id + dest_airport_id |\n             0 |\n             month + origin_airport_id + tail_num, \n            data = air)\ntime_felm = (proc.time() - pt)[3]\n```\n:::\n\n\n\n#### reghdfe\n\n\n\n::: {.cell}\n\n```{.stata .cell-code}\nclear\nclear matrix\ntimer clear\nset more off\n\ncd \"Z:\\home\\grant\"\n\nimport delimited air.csv\n\n// Encode strings as numeric factors (Stata struggles with the former)\nencode tail_num, generate(tail_num2)\nencode dep_tod, generate(dep_tod2)\n\n// Start timer and run regs\ntimer on 1\n\nqui reghdfe arr_delay i.dep_tod2 i.dep_tod2#c.dep_delay, ///\n  absorb(month year day_of_week origin_airport_id dest_airport_id) ///\n  cluster(month)\n\nqui reghdfe arr_delay i.dep_tod2 i.dep_tod2#c.dep_delay, ///\n  absorb(month year day_of_week origin_airport_id dest_airport_id) ///\n  cluster(month origin_airport_id)\n\nqui reghdfe arr_delay i.dep_tod2 i.dep_tod2#c.dep_delay, ///\n  absorb(month year day_of_week origin_airport_id dest_airport_id) ///\n  cluster(month origin_airport_id tail_num2)\n\ntimer off 1\n\n// Export time\ndrop _all\ngen elapsed = .\nset obs 1\nreplace elapsed = r(t1) if _n == 1\noutsheet using \"air-reghdfe.csv\", replace\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}