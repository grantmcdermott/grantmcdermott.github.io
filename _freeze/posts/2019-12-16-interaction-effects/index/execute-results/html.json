{
  "hash": "992e81cda45c0100156d283ea15ade55",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Marginal effects and interaction terms\"\ndescription: \"Quickly get the full marginal effect of interaction terms in R (and other software)\"\ndate: \"2019-12-16\"\n# slug: interaction-effects\ntoc: true\ncategories: [econometrics, R]\n---\n\n\n\n\n\n## The trick\n\nI recently [tweeted](https://twitter.com/grant_mcdermott/status/1202084676439085056?s=20) one of my favourite R tricks for getting the full marginal effect(s) of interaction terms. The short version is that, instead of writing your model as `lm(y ~ f1 * x2)`, you write it as `lm(y ~ f1 / x2)`. Here's an example using everyone's favourite `mtcars` dataset.\n\nFirst, partial marginal effects with the standard `f1 * x2` interaction syntax.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(mpg ~ factor(am) * wt, mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ factor(am) * wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     31.4161     3.0201  10.402 4.00e-11 ***\nfactor(am)1     14.8784     4.2640   3.489  0.00162 ** \nwt              -3.7859     0.7856  -4.819 4.55e-05 ***\nfactor(am)1:wt  -5.2984     1.4447  -3.667  0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833,\tAdjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n```\n\n\n:::\n:::\n\n\n\nSecond, full marginal effects with the trick `f1 / x2` interaction syntax.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(mpg ~ factor(am) / wt, mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ factor(am)/wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6004 -1.5446 -0.5325  0.9012  6.0909 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     31.4161     3.0201  10.402 4.00e-11 ***\nfactor(am)1     14.8784     4.2640   3.489  0.00162 ** \nfactor(am)0:wt  -3.7859     0.7856  -4.819 4.55e-05 ***\nfactor(am)1:wt  -9.0843     1.2124  -7.493 3.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.591 on 28 degrees of freedom\nMultiple R-squared:  0.833,\tAdjusted R-squared:  0.8151 \nF-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11\n```\n\n\n:::\n:::\n\n\n\nTo get the full marginal effect of `factor(am)1:wt` in the first case, I have to manually sum up the coefficients on the constituent parts (i.e. `factor(am)1=14.8784` + `factor(am)1:wt=-5.2984`). In the second case, I get the full marginal effect of **&minus;9.0843** immediately in the model summary. Not only that, but the correct standard errors, p-values, etc. are also automatically calculated for me. (If you don't remember, manually calculating SEs for multiplicative interaction terms is a [huge](http://mattgolder.com/wp-content/uploads/2015/05/standarderrors1.png) [pain](http://mattgolder.com/wp-content/uploads/2015/05/standarderrors2.png). And that's before we even get to complications like standard error clustering.)\n\nNote that the `lm(y ~ f1 / x2)` syntax is actually shorthand for the more verbose `lm(y ~ f1 + f1:x2)`. I'll get back to this point further below, but I wanted to flag the expanded syntax as important because it demonstrates why this trick \"works\". The key idea is to drop the continuous variable parent term (here: `x2`) from the regression. This forces all of the remaining child terms relative to the same base. It's also why this trick can easily be adapted to, say, Julia or Stata (see [here](https://twitter.com/paulgp/status/1202085605116665856)).\n\nSo far, so good. It's a great trick that has saved me a bunch of time (say nothing of likely user-error) that I recommend to everyone. Yet, I was prompted to write a separate blog post after being asked whether this trick a) works for higher-order interactions, and b) other non-linear models like logit? The answer in both cases is a happy \"Yes!\".\n\n## Dealing with higher-order interactions\n\nLet's consider a threeway interaction, since this will demonstrate the general principle for higher-order interactions. First, as a matter of convenience, I'll create a new dataset so that I don't have to keep specifying the factor variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars2 = transform(\n  mtcars,\n  vs = factor(vs),\n  am = factor(am)\n)\n```\n:::\n\n\n\nNow, we run a threeway interaction and view the (naive, partial) marginal effects.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 = lm(mpg ~ am * vs * wt, mtcars2)\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ am * vs * wt, data = mtcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3055 -1.7152 -0.7279  1.3504  5.3624 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  25.0594     4.0397   6.203 2.07e-06 ***\nam1          17.3041     7.7041   2.246   0.0342 *  \nvs1           6.4677    10.1440   0.638   0.5298    \nwt           -2.4389     0.9689  -2.517   0.0189 *  \nam1:vs1      -4.7049    12.9763  -0.363   0.7201    \nam1:wt       -5.4749     2.4667  -2.220   0.0362 *  \nvs1:wt       -0.9372     3.0560  -0.307   0.7617    \nam1:vs1:wt    1.0833     4.4419   0.244   0.8094    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.469 on 24 degrees of freedom\nMultiple R-squared:  0.8701,\tAdjusted R-squared:  0.8322 \nF-statistic: 22.96 on 7 and 24 DF,  p-value: 3.533e-09\n```\n\n\n:::\n:::\n\n\n\nSay we are interested in the full marginal effect of the threeway interaction `vs1:am1:wt`. Even summing the correct parent coefficients is a potentially error-prone process of thinking through the underlying math (which terms are excluded from the partial derivative, etc.) And don't even get me started on the standard errors...\n\nNow, it should be said that there _are_ several existing tools for obtaining this number that don't require us working through everything by hand. Here I'll use my favourite such tool &mdash; the [**margins**](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html) package &mdash; to save me the mental arithmetic.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(margins)\n\nmargins_summary(\n  fit1,\n  variables = \"wt\",\n  at = list(vs = 1, am = 1),\n)[1,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n factor     vs     am     AME     SE       z      p    lower   upper\n     wt 2.0000 2.0000 -7.7676 2.2903 -3.3916 0.0007 -12.2565 -3.2788\n```\n\n\n:::\n:::\n\n\n\nWe now at least see that the full (average) marginal effect is **&minus;7.7676**. Still, while this approach works well in the present example, we can also begin to see some downsides. It requires extra coding steps and comes with its own specialised syntax. Moreover, underneath the hood, **margins** relies on a [numerical delta method](https://cran.r-project.org/web/packages/margins/vignettes/TechnicalDetails.pdf) that can dramatically increase computation time and memory use for even moderately sized real-world problems. (Is your dataset bigger than 1 GB? [Good luck](https://github.com/leeper/margins/issues/130).) Another practical problem is that **margins** may not even support your model class. (See [here](https://github.com/leeper/margins/issues/101).)\n\n::: {.callout-tip}\n## marginaleffects >> margins\n\nIn the period since I originally wrote this post, the **margins** package has come to be superseded by\n[**marginaleffects**](https://marginaleffects.com/). The **marginaleffects** package is not only significantly faster, but also supports a much wider set of model classes and offers far richer functionality. So it's essentially superior to **margins** in every way. Despite these improvements, I would still argue that the specific advantanges of the `f1 / x2` trick carry over i.t.o. speed, convenience, etc. But for posterity here is the equivalent syntax for this newer package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(marginaleffects)\nslopes(\n  fit1,\n  variables = \"wt\",\n  newdata = datagrid(vs = 1, am = 1)\n)\n```\n:::\n\n\n\nOkay, back to the original post...\n:::\n\nSo, what about the alternative? Does our little syntax trick work here too? The good news is that, yes, it's just as simple as it was before.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 = lm(mpg ~ am / vs / wt, mtcars2)\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ am/vs/wt, data = mtcars2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3055 -1.7152 -0.7279  1.3504  5.3624 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  25.0594     4.0397   6.203 2.07e-06 ***\nam1          17.3041     7.7041   2.246  0.03417 *  \nam0:vs1       6.4677    10.1440   0.638  0.52978    \nam1:vs1       1.7629     8.0922   0.218  0.82939    \nam0:vs0:wt   -2.4389     0.9689  -2.517  0.01891 *  \nam1:vs0:wt   -7.9138     2.2685  -3.489  0.00190 ** \nam0:vs1:wt   -3.3761     2.8983  -1.165  0.25552    \nam1:vs1:wt   -7.7676     2.2903  -3.392  0.00241 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.469 on 24 degrees of freedom\nMultiple R-squared:  0.8701,\tAdjusted R-squared:  0.8322 \nF-statistic: 22.96 on 7 and 24 DF,  p-value: 3.533e-09\n```\n\n\n:::\n:::\n\n\n\nAgain, we get the full marginal effect of **&minus;7.7676** (and correct SE of 2.2903) directly in the model object. Much easier, isn't it?\n\nWhere this approach really shines is in combination with plotting. Say, after extracting the coefficients with `broom::tidy()`, or simply plotting them directly with `modelsummary::modelplot()`. Model results are usually much easier to interpret visually, but this is precisely where we want to depict full marginal effects to our reader. Here I'll use the **modelsummary** package to produce a nice coefficient plot of our threeway interaction terms. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(modelsummary)\nlibrary(ggplot2)    ## for some extra ggplot2 layers\nlibrary(hrbrthemes) ## theme(s) I like\n\n## Optional: A dictionary of \"nice\" coefficient names for our plot\ndict = c('am0:vs0:wt' = 'Manual\\nStraight',\n         'am0:vs1:wt' = 'Manual\\nV-shaped',\n         'am1:vs0:wt' = 'Automatic\\nStraight',\n         'am1:vs1:wt' = 'Automatic\\nV-shaped')\n\nmodelplot(fit2, coef_map = dict) +\n  geom_vline(xintercept = 0, col = \"orange\") +\n  labs(\n    x = \"Marginal effect (Δ in MPG : Δ in '000 lbs)\",\n    title = \" Marginal effect of vehicle weight on MPG\", \n    subtitle = \"Conditional on transmission type and engine shape\"\n    ) +\n  theme_ipsum() \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fit2_coefplot-1.png){width=2400}\n:::\n:::\n\n\n\nThe above plot immediately makes clear how automatic transmission exacerbates the impact of vehicle weight on MPG. We also see that the conditional impact of engine shape is more ambiguous. In contrast, I invite you to produce an equivalent plot using our earlier `fit1` object and see if you can easily make sense of it. I certainly can't.\n\n## Aside: Specifying (parent) terms as fixed effects\n\nOn the subject of speed, recall that the `lm(y ~ f1 / x2)` syntax is equivalent to the more verbose `lm(y ~ f1 + f1:x2)`. This verbose syntax provides a clue for greatly reducing computation time for large models; particularly those with factor variables that contain many levels. We simply need specify the parent factor terms as _fixed effects_ (using a specialised library like [**fixest**](https://lrberge.github.io/fixest)). Going back to our introductory twoway interaction example, you would thus write the model as follows. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fixest)\n\n# Note: i(f1, x1) is like f1:x2 but with some special fixest features \n\n# feols(mpg ~ am:wt | am, mtcars2)\nfeols(mpg ~ i(am, wt) | am, mtcars2) \n```\n:::\n\n\n\n(I'll let you confirm for yourself that running the above models gives the correct &minus;9.0843 figure from before.)\n\nIn case you're wondering, the verbose equivalent for the `f1 / f2 / x3` threeway interaction is `f1 + f2 + f1:f2 + f1:f2:x3`. So we can use the same FE approach for this more complicated case as follows.[^1]\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Option 1 using verbose base lm(). Not run.\n# summary(lm(mpg ~ am + vs + am:vs + am:vs:wt, mtcars2))\n\n## Option 2 using fixest::feols()\nfeols(mpg ~ am:vs:wt | am^vs, mtcars2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS estimation, Dep. Var.: mpg\nObservations: 32\nFixed-effects: am^vs: 4\nStandard-errors: Clustered (am^vs) \n           Estimate Std. Error       t value  Pr(>|t|)    \nam0:vs0:wt -2.43889  2.330e-16 -1.048054e+16 < 2.2e-16 ***\nam1:vs0:wt -7.91376  1.044e-15 -7.582149e+15 < 2.2e-16 ***\nam0:vs1:wt -3.37612  1.514e-15 -2.229254e+15 < 2.2e-16 ***\nam1:vs1:wt -7.76765  2.955e-15 -2.628411e+15 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 2.1381     Adj. R2: 0.832197\n               Within R2: 0.566525\n```\n\n\n:::\n:::\n\n\n\nThere's our desired **&minus;7.7676** coefficient again. This time, however, we also get the added bonus of clustered standard errors (which are switched on by default in `fixest::feols()` models).\n\n**Caveat:** The above example implicitly presumes that you don't care about doing inference on the parent term(s), since these are swept away by the underlying fixed-effect procedures. That is clearly not going to be desirable in every case. But, in practice, I often find that it is a perfectly acceptable trade-off for models that I am running. (For example, when I am trying to remove general calender artefacts like monthly effects.)\n\n## Other model classes\n\nThe last thing I want to demonstrate quickly is that our little trick carries over neatly to other model classes to. Say, that ~~old workhorse of non-linear stats~~ hot! new! machine! learning! classifier: logit models. Again, I'll let you run these to confirm for yourself:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Tired\nsummary(glm(am ~ vs * wt, family = binomial, mtcars2))\n## Wired\nsummary(glm(am ~ vs / wt, family = binomial, mtcars2))\n```\n:::\n\n\n\nOkay, I confess: That last code chunk was a trick to see who was staying awake during statistics class. I mean, it will correctly sum the coefficient values. But we all know that the raw coefficient values on generalised linear models like logit cannot be interpreted as marginal effects, regardless of whether there are interactions or not. Instead, we need to convert them via an appropriate link function. In R, the [**mfx**](https://cran.r-project.org/web/packages/mfx/index.html) package will do this for us automatically. My real point, then, is to say that we can combine the link function (via **mfx**) and our syntax trick (in the case of interaction terms). This makes a surprisingly complicated problem much easier to handle.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mfx, quietly = TRUE)\n\n## Broke\nlogitmfx(am ~ vs * wt, mtcars2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlogitmfx(formula = am ~ vs * wt, data = mtcars2)\n\nMarginal Effects:\n           dF/dx Std. Err.        z   P>|z|    \nvs1    -0.994682  0.045074 -22.0680 < 2e-16 ***\nwt     -1.268124  0.639812  -1.9820 0.04748 *  \nvs1:wt  0.494044  0.719052   0.6871 0.49203    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndF/dx is for discrete change for the following variables:\n\n[1] \"vs1\"\n```\n\n\n:::\n\n```{.r .cell-code}\n## Woke\nlogitmfx(am ~ vs / wt, mtcars2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlogitmfx(formula = am ~ vs/wt, data = mtcars2)\n\nMarginal Effects:\n           dF/dx Std. Err.        z   P>|z|    \nvs1    -0.994682  0.045074 -22.0680 < 2e-16 ***\nvs0:wt -1.268124  0.639812  -1.9820 0.04748 *  \nvs1:wt -0.774081  0.673267  -1.1497 0.25025    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndF/dx is for discrete change for the following variables:\n\n[1] \"vs1\"\n```\n\n\n:::\n:::\n\n\n\n## Conclusion\n\nWe don't always want the full marginal effect of an interaction term. Indeed, there are times where we are specifically interested in evaluating the partial marginal effect. (In a difference-in-differences model, for example.) But in many other cases, the full marginal effect of the interaction terms is _exactly_ what we want. The `lm(y ~ f1 / x2)` syntax trick (and its equivalents) is a really useful shortcut to remember in these cases.\n\n**PS.** In case, I didn't make it clear: This trick works best when your interaction contains at most one continuous variable. (This is the parent \"x\" term that gets left out in all of the above examples.) You can still use it when you have more than one continuous variable, but it will implicitly force one of them to zero. Factor variables, on the other hand, get forced relative to the same base (here: the intercept), which is what we want.\n\n**Update.** Subsequent to posting this, I was made aware of this nice [SO answer](https://stackoverflow.com/questions/32616762/defining-an-infix-operator-for-use-within-a-formula/32682826#32682826) by Heather Turner, which treads similar ground. I particularly like the definitional contrast between factors that are \"crossed\" versus those that are \"nested\".\n\n\n[^1]: For the `fixest::feols` case, we don't have to specify all of the parent terms in the fixed-effects slot --- i.e. we just need `| am^vs` --- because these fixed-effects terms are all swept out of the model simultaneously at estimation time.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}